# %% [markdown]
# # ***REGRESSION***

# %%
!pip install catboost
!pip install optuna

# %%
from sklearn.impute import SimpleImputer
import numpy as np
import pandas as pd
from catboost import CatBoostRegressor, Pool
from sklearn.metrics import mean_squared_error
import optuna
from sklearn.model_selection import train_test_split

# %%
train = pd.read_csv('/content/train (6).csv', index_col='Id')
train.head(3)

# %%
columns = list(train.columns)
categorical_features = []
for i in columns:
    if str(type(train[i].iloc[0])) == "<class 'str'>":
        categorical_features.append(i)

# %%
missing_values_table = train.isna().sum().reset_index()
missing_values_table.columns = ['Column', 'Missing Values']
missing_values_table['% of Total Values'] = (missing_values_table['Missing Values'] / len(train)) * 100

print(missing_values_table['% of Total Values'])

# %%
a = list(missing_values_table['% of Total Values'])
num = []
cat = []
drp = []
for i in range(len(columns)):
    if missing_values_table['% of Total Values'].iloc[i] > 0:
        if missing_values_table['% of Total Values'].iloc[i] > 40:
            drp.append(str(missing_values_table['Column'].iloc[i]))
        elif str(missing_values_table['Column'].iloc[i]) in categorical_features:
            print(missing_values_table['Column'].iloc[i], missing_values_table['% of Total Values'].iloc[i], "CATEGORICAL")
            cat.append(str(missing_values_table['Column'].iloc[i]))
        else:
            print(missing_values_table['Column'].iloc[i], missing_values_table['% of Total Values'].iloc[i])
            num.append(str(missing_values_table['Column'].iloc[i]))

print(drp)


# %%
train = train.drop(drp, axis = 1)

imputer = SimpleImputer(strategy='mean')
train[num] = imputer.fit_transform(train[num])

imputer_cat = SimpleImputer(strategy='most_frequent')
train[cat] = imputer_cat.fit_transform(train[cat])

# %%
y = train['SalePrice']
x = train.drop('SalePrice', axis =1)

train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.2)

# %%
columns = list(train_x.columns)
categorical_features = []
for i in columns:
    if str(type(train[i].iloc[0])) == "<class 'str'>":
        categorical_features.append(i)

# %%
def objective(trial):
    params = {
        "iterations": trial.suggest_int("iterations", 500, 1000),
        "depth": trial.suggest_int("depth", 3, 6),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 5, log=True),
        "loss_function": "RMSE",
        "eval_metric": "RMSE",
        "random_seed": 42,
        "verbose": 0
    }

    model = CatBoostRegressor(**params)
    model.fit(train_x, train_y, eval_set=(val_x, val_y), verbose=1000, cat_features=categorical_features)

    preds = model.predict(val_x)
    rmse = mean_squared_error(val_y, preds)
    rmse = np.sqrt(rmse)
    return rmse

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=20)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π

# üìå 6. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
best_params = study.best_params
best_model = CatBoostRegressor(**best_params)
best_model.fit(train_x, train_y, eval_set=(val_x, val_y), verbose=100, cat_features=categorical_features)

# üìå 7. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
final_preds = best_model.predict(val_x)
rmse = mean_squared_error(val_y, final_preds)
rmse = np.sqrt(rmse)
print(f"Final RMSE: {rmse}")

# %%
print(best_params)

# %%
test = pd.read_csv('/content/test (3).csv')
test.head(3)

# %%
test = test.drop(drp, axis = 1)

# %%
x = list(val_x.columns)
y = list(test.columns)
for i in y:
    if i not in x:
        print(i)

# %%
missing_values_table = test.isna().sum().reset_index()
missing_values_table.columns = ['Column', 'Missing Values']
missing_values_table['% of Total Values'] = (missing_values_table['Missing Values'] / len(test)) * 100

print(missing_values_table['% of Total Values'])

# %%
a = list(missing_values_table['% of Total Values'])
num = []
cat = []
drp = []
for i in range(len(columns)):
    if missing_values_table['% of Total Values'].iloc[i] > 0:
        if missing_values_table['% of Total Values'].iloc[i] > 40:
            drp.append(str(missing_values_table['Column'].iloc[i]))
        elif str(missing_values_table['Column'].iloc[i]) in categorical_features:
            print(missing_values_table['Column'].iloc[i], missing_values_table['% of Total Values'].iloc[i], "CATEGORICAL")
            cat.append(str(missing_values_table['Column'].iloc[i]))
        else:
            print(missing_values_table['Column'].iloc[i], missing_values_table['% of Total Values'].iloc[i])
            num.append(str(missing_values_table['Column'].iloc[i]))

# %%
imputer = SimpleImputer(strategy='mean')
test[num] = imputer.fit_transform(test[num])

imputer_cat = SimpleImputer(strategy='most_frequent')
test[cat] = imputer_cat.fit_transform(test[cat])

# %%
final_preds = best_model.predict(test)

# %%
final = pd.DataFrame({
    "Id": test['Id'],
    "SalePrice": final_preds
})
final.head(3)

# %%
final.to_csv('final_0.csv', index=False)

# %% [markdown]
# # Classification

# %%
!pip install optuna
!pip install catboost

# %%
from sklearn.impute import SimpleImputer
import numpy as np
import pandas as pd
from catboost import CatBoostClassifier, Pool
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# %%
train = pd.read_csv('/content/train (7).csv', index_col=False)
train = train.drop('PassengerId', axis=1)
test = pd.read_csv('/content/test (4).csv', index_col=False)

# %%
train.head(3)

# %%
missing_values_table = train.isna().sum().reset_index()
missing_values_table.columns = ['Column', 'Missing Values']
missing_values_table['% of Total Values'] = (missing_values_table['Missing Values'] / len(train)) * 100

print(missing_values_table['% of Total Values'])

# %%
columns = list(train.columns)
categorical_features = []
for i in columns:
    if str(type(train[i].iloc[0])) == "<class 'str'>":
        categorical_features.append(i)

# %%
a = list(missing_values_table['% of Total Values'])
num = []
cat = []
drp = []
for i in range(len(a)):
    if missing_values_table['% of Total Values'].iloc[i] > 0:
        if missing_values_table['% of Total Values'].iloc[i] > 40:
            drp.append(str(missing_values_table['Column'].iloc[i]))
        elif str(missing_values_table['Column'].iloc[i]) in categorical_features:
            print(missing_values_table['Column'].iloc[i], missing_values_table['% of Total Values'].iloc[i], "CATEGORICAL")
            cat.append(str(missing_values_table['Column'].iloc[i]))
        else:
            print(missing_values_table['Column'].iloc[i], missing_values_table['% of Total Values'].iloc[i])
            num.append(str(missing_values_table['Column'].iloc[i]))

print(drp)

# %%
train = train.drop(drp, axis = 1)

imputer = SimpleImputer(strategy='mean')
train[num] = imputer.fit_transform(train[num])

imputer_cat = SimpleImputer(strategy='most_frequent')
train[cat] = imputer_cat.fit_transform(train[cat])

# %%
y = train['Survived']
x = train.drop('Survived', axis =1)

train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.2)

# %%
columns = list(train_x.columns)
categorical_features = []
for i in columns:
    if str(type(train[i].iloc[0])) == "<class 'str'>":
        categorical_features.append(i)

# %%
def objective(trial):
    params = {
        "iterations": trial.suggest_int("iterations", 500, 2000),
        "depth": trial.suggest_int("depth", 4, 7),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 5, log=True),
        "loss_function": "Logloss",
        "eval_metric": "Accuracy",
        "random_seed": 42,
        "verbose": 0
    }

    model = CatBoostClassifier(**params)
    model.fit(train_x, train_y, eval_set=(val_x, val_y), verbose=1000, cat_features=categorical_features)

    preds = model.predict(val_x)
    accuracy = accuracy_score(val_y, preds)
    return accuracy

# üìå 5. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
study = optuna.create_study(direction="maximize")  # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ‚Äî –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º accuracy
study.optimize(objective, n_trials=20)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π

# üìå 6. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
best_params = study.best_params
best_model = CatBoostClassifier(**best_params)
best_model.fit(train_x, train_y, eval_set=(val_x, val_y), verbose=100, cat_features=categorical_features)

# üìå 7. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
final_preds = best_model.predict(val_x)
accuracy = accuracy_score(val_y, final_preds)
print(f"Final Accuracy: {accuracy:.4f}")


# %%
print(best_params)

# %%
test.head(3)

# %%
a = list(test.columns)
num = []
cat = []
for i in range(len(a)):
    if missing_values_table['% of Total Values'].iloc[i] > 0:
        if missing_values_table['% of Total Values'].iloc[i] > 40:
            continue
        elif str(missing_values_table['Column'].iloc[i]) in categorical_features:
            print(missing_values_table['Column'].iloc[i], missing_values_table['% of Total Values'].iloc[i], "CATEGORICAL")
            cat.append(str(missing_values_table['Column'].iloc[i]))
        else:
            print(missing_values_table['Column'].iloc[i], missing_values_table['% of Total Values'].iloc[i])
            num.append(str(missing_values_table['Column'].iloc[i]))


# %%
test = test.drop(drp, axis = 1)

imputer = SimpleImputer(strategy='mean')
test[num] = imputer.fit_transform(test[num])

imputer_cat = SimpleImputer(strategy='most_frequent')
test[cat] = imputer_cat.fit_transform(test[cat])

# %%
id = test['PassengerId']
test = test.drop('PassengerId', axis = 1)

# %%
final_preds = best_model.predict(test)

final = pd.DataFrame({
    "PassengerId": id,
    "Survived": final_preds
})
final.head(3)

# %%
final.to_csv('final_2.csv', index=False)

# %% [markdown]
# # Classification YOLO

# %%
import pandas as pd
import os
from ultralytics import YOLO

DATA_DIR = "path/to/dataset"
TEST_DIR = "path/to/test"
OUTPUT_CSV = "submission.csv"
# n - s - m - l - x
model = YOLO("yolov11x-cls.pt")

model.train(
    data=DATA_DIR,
    epochs=10,
    imgsz=640,
    batch=16,
    lr0=1e-3,
    momentum=0.9,
    augment=True
)

model_path = "runs/train/exp/weights/best.pt"
model = YOLO(model_path)

test_images = [f for f in os.listdir(TEST_DIR) if f.endswith((".jpg", ".png"))]

predictions = []

for img_name in test_images:
    img_path = os.path.join(TEST_DIR, img_name)
    results = model(img_path)

    predicted_class = results[0].probs.top1
    predictions.append((img_name, predicted_class))

df = pd.DataFrame(predictions, columns=["filename", "class"])
df.to_csv(OUTPUT_CSV, index=False)

print(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_CSV}")


# %% [markdown]
# # Detection YOLO

# %%
#–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑ csv –≤ yaml
import os
import pandas as pd
from PIL import Image

images_dir = "path/to/images"
csv_file = "path/to/annotations.csv"
output_dir = "path/to/yolo_format"

annotations = pd.read_csv(csv_file)

def normalize_coordinates(x1, y1, x2, y2, img_width, img_height):
    x_center = (x1 + x2) / 2 / img_width
    y_center = (y1 + y2) / 2 / img_height
    width = (x2 - x1) / img_width
    height = (y2 - y1) / img_height
    return x_center, y_center, width, height

for img_name in annotations['filename'].unique():
    img_path = os.path.join(images_dir, img_name)
    img = Image.open(img_path)
    img_width, img_height = img.size

    txt_file = os.path.join(output_dir, img_name.replace('.jpg', '.txt').replace('.png', '.txt'))

    with open(txt_file, 'w') as f:
        img_annotations = annotations[annotations['filename'] == img_name]

        for _, row in img_annotations.iterrows():
            class_id = row['class']
            x_center, y_center, width, height = normalize_coordinates(
                row['x1'], row['y1'], row['x2'], row['y2'], img_width, img_height
            )
            f.write(f"{class_id} {x_center} {y_center} {width} {height}\n")



# %%
# dataset.yaml
# train: path/to/dataset/images/train  # –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
# val: path/to/dataset/images/val      # –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
# test: path/to/dataset/images/test    # –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

# nc: 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
# names: ['car', 'person', 'dog']  # –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤


# %%
import os
import pandas as pd
from ultralytics import YOLO

# === 1. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===
DATA_YAML = "path/to/dataset.yaml"  # –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
TEST_DIR = "path/to/test"  # –ü–∞–ø–∫–∞ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
OUTPUT_CSV = "submission.csv"  # –§–∞–π–ª –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

# === 2. –û–ë–£–ß–ï–ù–ò–ï YOLOv11 ===
model = YOLO("yolov11n.pt")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º Nano-–≤–µ—Ä—Å–∏—é –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

model.train(
    data=DATA_YAML,  # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É .yaml —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π
    epochs=10,       # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    imgsz=640,       # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    batch=16,        # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    lr0=1e-3,        # –ù–∞—á–∞–ª—å–Ω—ã–π learning rate
    momentum=0.9,    # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    augment=True     # –í–∫–ª—é—á–∞–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
)

model_path = "runs/train/exp/weights/best.pt"
model = YOLO(model_path)

test_images = [f for f in os.listdir(TEST_DIR) if f.endswith((".jpg", ".png"))]

predictions = []

for img_name in test_images:
    img_path = os.path.join(TEST_DIR, img_name)
    results = model(img_path)

    for result in results:
        for box in result.boxes.data:
            x1, y1, x2, y2, conf, cls = box.tolist()
            predictions.append((img_name, int(cls), conf, x1, y1, x2, y2))

df = pd.DataFrame(predictions, columns=["filename", "class", "confidence", "x1", "y1", "x2", "y2"])
df.to_csv(OUTPUT_CSV, index=False)

# %% [markdown]
# # Segmentation YOLO

# %%
import pandas as pd
import os
from PIL import Image

csv_path = 'annotations.csv'
images_dir = 'path/to/images'
output_dir = 'yolo_annotations'

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
os.makedirs(output_dir, exist_ok=True)

data = pd.read_csv(csv_path)

def convert_to_yolo_format(image_width, image_height, x_min, y_min, x_max, y_max):
    x_center = (x_min + x_max) / 2 / image_width
    y_center = (y_min + y_max) / 2 / image_height
    width = (x_max - x_min) / image_width
    height = (y_max - y_min) / image_height
    return x_center, y_center, width, height

for image_name in os.listdir(images_dir):
    image_path = os.path.join(images_dir, image_name)

    with Image.open(image_path) as img:
        image_width, image_height = img.size

    image_annotations = data[data['image_name'] == image_name]

    annotation_file = os.path.join(output_dir, os.path.splitext(image_name)[0] + '.txt')
    with open(annotation_file, 'w') as f:
        for _, row in image_annotations.iterrows():
            class_id = row['class_id']
            x_min, y_min, x_max, y_max = row['x_min'], row['y_min'], row['x_max'], row['y_max']

            x_center, y_center, width, height = convert_to_yolo_format(image_width, image_height, x_min, y_min, x_max, y_max)

            f.write(f"{class_id} {x_center} {y_center} {width} {height}\n")


# %%
# train: path/to/train/images
# val: path/to/val/images
# test: path/to/test/images

# nc: 2  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
# names: ['class_0', 'class_1']  # –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤

# %%
from ultralytics import YOLO
import os
import pandas as pd

def mask_to_rle(mask):
    mask = mask.astype(np.uint8)

    rle = mask_util.encode(np.asfortranarray(mask))

    rle_string = str(rle['counts'], encoding='utf-8')
    return rle_string

data_path = 'path/to/data.yaml'

model = YOLO('yolo11n-seg.pt')

model.train(data=data_path, epochs=50, imgsz=640, batch_size=16, augment=True)

model.save('yolov11_segmentation_model.pt')

test_path = 'path/to/test/images'
results = model.predict(test_path, save=True)

predictions = []
for image_path, pred in zip(results.files, results.pred):
    file_name = os.path.basename(image_path)

    masks = pred.masks

    if masks is not None:
        for idx, mask in enumerate(masks):
            rle_string = mask_to_rle(mask)
            predictions.append([file_name, idx, rle_string])

df = pd.DataFrame(predictions, columns=['file_name', 'object_id', 'rle'])
df.to_csv('final.csv', index=False)


# %% [markdown]
# # Pose Detection

# %%
from ultralytics import YOLO
import os
import pandas as pd
import numpy as np
from PIL import Image

data_path = 'path/to/data.yaml'

model = YOLO('yolov11-pose.pt')

model.train(data=data_path, epochs=50, imgsz=640, batch_size=16)

test_path = 'path/to/test/images'
results = model.predict(test_path, save=True)

def extract_keypoints(pred):
    keypoints = []
    for i, person in enumerate(pred.keypoints):
        keypoints.append(list(person.xy))
    return keypoints

predictions = []
for image_path, pred in zip(results.files, results.pred):
    file_name = os.path.basename(image_path)

    keypoints = extract_keypoints(pred)

    if keypoints:
        for person_id, person_keypoints in enumerate(keypoints):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ç–æ—á–µ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞
            for idx, (x, y) in enumerate(person_keypoints):
                predictions.append([file_name, person_id, idx, x, y])  # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏

df = pd.DataFrame(predictions, columns=['file_name', 'person_id', 'keypoint_id', 'x', 'y'])
df.to_csv('final.csv', index=False)


# %% [markdown]
# # RecSys Surprise

# %%
!pip install scikit-surprise optuna

# %%
import random
import pandas as pd
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
from surprise import accuracy

# %%
import pandas as pd
from surprise import Dataset, Reader, SVD
from surprise.model_selection import cross_validate, GridSearchCV

# üìå –ó–∞–≥—Ä—É–∂–∞–µ–º MovieLens 100K
data = Dataset.load_builtin('ml-100k')

# üìå –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Grid Search
param_grid = {
    'n_factors': [50, 100, 150],  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
    'lr_all': [0.002, 0.005, 0.01],  # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    'reg_all': [0.02, 0.05, 0.1]  # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
}

# üìå –ó–∞–ø—É—Å–∫–∞–µ–º Grid Search
gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5, n_jobs=-1)
gs.fit(data)

# üìå –í—ã–≤–æ–¥–∏–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
print(f"üìä –õ—É—á—à–∏–π RMSE: {gs.best_score['rmse']:.4f}")
print(f"üîß –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {gs.best_params['rmse']}")

# üìå –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
best_model = SVD(**gs.best_params['rmse'])
cross_validate(best_model, data, cv=5, verbose=True)


# %%
import optuna
import pandas as pd
from surprise import Dataset, Reader, KNNBasic
from surprise.model_selection import cross_validate

# üìå –ó–∞–≥—Ä—É–∂–∞–µ–º MovieLens 100K
data = Dataset.load_builtin('ml-100k')

# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
def objective(trial):
    sim_options = {
        "name": trial.suggest_categorical("name", ["cosine", "pearson", "msd"]),
        "user_based": trial.suggest_categorical("user_based", [True, False])
    }

    model = KNNBasic(k=trial.suggest_int("k", 10, 50), sim_options=sim_options)

    cv_results = cross_validate(model, data, measures=["rmse"], cv=3, verbose=False)
    return cv_results["test_rmse"].mean()

# üìå –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å Optuna
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=30)

# üìå –í—ã–≤–æ–¥–∏–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
best_params = study.best_params
print(f"üìä –õ—É—á—à–∏–π RMSE: {study.best_value:.4f}")
print(f"üîß –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}")

# üìå –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
final_sim_options = {"name": best_params["name"], "user_based": best_params["user_based"]}
best_model = KNNBasic(k=best_params["k"], sim_options=final_sim_options)

cross_validate(best_model, data, measures=["rmse"], cv=5, verbose=True)


# %% [markdown]
#
#   \begin{array}{|c|c|c|c|c|c|c|c|}\hline\\ \\
#   \mathcal{} & Movielens 100k & RMSE & MAE & Time  \\ \hline\\
#   a & SVD & 0.934 & 0.737 & 0:00:06   \\ \hline\\ \\
#   b & SVD++ (cache_ratings=False) & 0.919 & 0.721 & 0:01:39\\ \hline\\ \\
#   c & SVD++ (cache_ratings=True) & 0.919 & 0.721 & 0:01:22  \\ \hline\\ \\
#   d & NMF & 0.963 & 0.758 & 0:00:06 \\ \hline\\ \\
#   e & Slope One & 0.946 & 0.743 & 0:00:09 \\ \hline\\ \\
#   f & k-NN & 0.98 & 0.774 & 0:00:08 \\ \hline\\ \\
#   g & Slope One & 0.946 & 0.743 & 0:00:09 \\ \hline\\ \\
#   h & Centered k-NN & 0.951 & 0.749 & 0:00:09 \\ \hline\\ \\
#   i & k-NN Baseline & 0.931 & 0.733 & 0:00:13 \\ \hline\\ \\
#   j & Co-Clustering & 0.963 & 0.753 & 0:00:06 \\ \hline\\ \\
#   k & Random & 1.518 & 1.219 & 0:00:01 \\ \hline\\ \\
#   l & Baseline & 0.944 & 0.748 & 0:00:02 \\ \hline
#   \end{array}
#
#
#

# %% [markdown]
# # RecSys Rectools

# %%
!pip install rectools
!pip install rectools[all]
!pip install rectools[torch]

# %%
'''–ü–æ —Ñ–∞–∫—Ç—É –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±—ã—á–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞, 
–≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –æ—Ç—Ä–∞–∂–∞–µ—Ç –æ–¥–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ: –≤ –ø–µ—Ä–≤–æ–º —Å—Ç–æ–ª–±—Ü–µ id —é–∑–µ—Ä–∞, –≤–æ –≤—Ç–æ—Ä–æ–º ‚Äî –∞–π—Ç–µ–º–∞,
 –∞ –≤ —Ç—Ä–µ—Ç—å–µ–º ‚Äî —Å–∫–æ—Ä –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫—É–ø–∏–ª/ –Ω–µ –∫—É–ø–∏–ª). 
 –ï—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –∏—Ö —Ç–æ–∂–µ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å. '''

# %%
import numpy as np
import pandas as pd

from implicit.nearest_neighbours import TFIDFRecommender

from rectools import Columns
from rectools.dataset import Dataset
from rectools.metrics import (
    Precision,
    NDCG,
    AvgRecPopularity,
    Intersection,
    HitRate,
    SufficientReco,
    DebiasConfig,
    IntraListDiversity,
    Serendipity,
    calc_metrics,
)
from rectools.metrics.distances import PairwiseHammingDistanceCalculator
from rectools.models import *

# %%
ratings = pd.read_csv(
    "ratings.dat",
    sep="::",
    engine="python",  # Because of 2-chars separators
    header=None,
    names=[Columns.User, Columns.Item, Columns.Weight, Columns.Datetime],
)
print(ratings.shape)
ratings.head()

# %%
ratings["datetime"] = pd.to_datetime(ratings["datetime"] * 10 ** 9)
ratings["datetime"].min(), ratings["datetime"].max()

# %%
movies = pd.read_csv(
    "movies.dat",
    sep="::",
    engine="python",  # Because of 2-chars separators
    header=None,
    names=[Columns.Item, "title", "genres"],
    encoding_errors="ignore",
)
print(movies.shape)
split_dt = pd.Timestamp("2003-02-01")
df_train = ratings.loc[ratings["datetime"] < split_dt]
df_test = ratings.loc[ratings["datetime"] >= split_dt]
dataset = Dataset.construct(df_train)

# %%
from rectools.models import LightFMWrapperModel
from lightfm import LightFM

model = LightFMWrapperModel(
        # –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏ —É–∫–∞–∑—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä no_components
        # —ç—Ç–æ —Ä–∞–∑–º–µ–∑–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã—É—á–∏—Ç –º–æ–¥–µ–ª—å
        model=LightFM(no_components = 30)
        )

model.fit(dataset)
recos = model.recommend(
    users=ratings[Columns.User].unique(),
    dataset=dataset,
    k=5,
    filter_viewed=True,
)

serendipity = Serendipity(k=10)
precision = Precision(k=10, r_precision=True)  # r_precision means division by min(k, n_user_test_items)
ndcg = NDCG(k=10, log_base=3)

movies["genre"] = movies["genres"].str.split("|")
genre_exploded = movies[["item_id", "genre"]].set_index("item_id").explode("genre")
genre_dummies = pd.get_dummies(genre_exploded, prefix="", prefix_sep="").groupby("item_id").sum()

precision_value = precision.calc(reco=recos, interactions=df_test)
print(f"precision: {precision_value}")

catalog = df_train[Columns.Item].unique()

serendipity_value = serendipity.calc(
    reco=recos,
    interactions=df_test,
    prev_interactions=df_train,
    catalog=catalog
)
print("Serendipity: ", serendipity_value)

print("NDCG: ", ndcg.calc(reco=recos, interactions=df_test))

distance_calculator = PairwiseHammingDistanceCalculator(genre_dummies)
ild = IntraListDiversity(k=10, distance_calculator=distance_calculator)
print("ILD: ", ild.calc(reco=recos))

# %%
model = SASRecModel(
    session_max_len=20,
    loss="softmax",
    n_factors=64,
    n_blocks=1,
    n_heads=4,
    dropout_rate=0.2,
    lr=0.001,
    batch_size=128,
    epochs=6,
    verbose=1,
    deterministic=True,
)

model.fit(dataset)
recos = model.recommend(
    users=ratings[Columns.User].unique(),
    dataset=dataset,
    k=5,
    filter_viewed=True,
)

serendipity = Serendipity(k=10)
precision = Precision(k=10, r_precision=True)  # r_precision means division by min(k, n_user_test_items)
ndcg = NDCG(k=10, log_base=3)

movies["genre"] = movies["genres"].str.split("|")
genre_exploded = movies[["item_id", "genre"]].set_index("item_id").explode("genre")
genre_dummies = pd.get_dummies(genre_exploded, prefix="", prefix_sep="").groupby("item_id").sum()

precision_value = precision.calc(reco=recos, interactions=df_test)
print(f"precision: {precision_value}")

catalog = df_train[Columns.Item].unique()

serendipity_value = serendipity.calc(
    reco=recos,
    interactions=df_test,
    prev_interactions=df_train,
    catalog=catalog
)
print("Serendipity: ", serendipity_value)

print("NDCG: ", ndcg.calc(reco=recos, interactions=df_test))

distance_calculator = PairwiseHammingDistanceCalculator(genre_dummies)
ild = IntraListDiversity(k=10, distance_calculator=distance_calculator)
print("ILD: ", ild.calc(reco=recos))

# %%
model = BERT4RecModel(
    mask_prob=0.15,  # specify probability of masking tokens
    deterministic=True,
)

model.fit(dataset)
recos = model.recommend(
    users=ratings[Columns.User].unique(),
    dataset=dataset,
    k=5,
    filter_viewed=True,
)

serendipity = Serendipity(k=10)
precision = Precision(k=10, r_precision=True)  # r_precision means division by min(k, n_user_test_items)
ndcg = NDCG(k=10, log_base=3)

movies["genre"] = movies["genres"].str.split("|")
genre_exploded = movies[["item_id", "genre"]].set_index("item_id").explode("genre")
genre_dummies = pd.get_dummies(genre_exploded, prefix="", prefix_sep="").groupby("item_id").sum()

precision_value = precision.calc(reco=recos, interactions=df_test)
print(f"precision: {precision_value}")

catalog = df_train[Columns.Item].unique()

serendipity_value = serendipity.calc(
    reco=recos,
    interactions=df_test,
    prev_interactions=df_train,
    catalog=catalog
)
print("Serendipity: ", serendipity_value)

print("NDCG: ", ndcg.calc(reco=recos, interactions=df_test))

distance_calculator = PairwiseHammingDistanceCalculator(genre_dummies)
ild = IntraListDiversity(k=10, distance_calculator=distance_calculator)
print("ILD: ", ild.calc(reco=recos))

# %% [markdown]
#
#   \begin{array}{|c|c|c|c|c|c|c|c|}\hline\\ \\
#   \mathcal{} & Movielens 1m & Precision & Serendipity & NDCG & ILD  \\ \hline\\
#   a & SASRec (6 epochs) & 0.04781 & 4.5023e-05 & 0.03513 & 3.2877   \\ \hline\\ \\
#   b & BERT4Rec & 0.02630 & 1.1553e-05 & 0.03233 & 3.0311 \\ \hline\\ \\
#   c & implicit ALS Wrapper & 0.0515 & 5.4056e-05 & 0.05165 & 2.8392 \\ \hline\\ \\
#   d & implicit BPR-MF Wrapper & 0.02997 & 3.19868e-06 & 0.03615 & 3.86506 \\ \hline\\ \\
#   e & implicit ItemKNN Wrapper & 0.0456 & 3.0093e-05 & 0.04615 & 3.1726 \\ \hline\\ \\
#   f & LightFM Wrapper & 0.05858 & 4.7578e-06 & 0.0604 & 3.6395 \\ \hline\\ \\
#   g & EASE & 0.0367 & 3.0522e-05 & 0.03431 & 2.8200 \\ \hline\\ \\
#   h & PureSVD & 0.05952 & 2.5205e-05 & 0.05248 & 3.0020 \\ \hline\\ \\
#   j & Popular & 0.0330 & 4.1636e-06 & 0.04160 & 3.4595 \\ \hline\\ \\
#   l & Random & 0.0131 & 1.6940e-05 & 0.00487 & 2.6273 \\ \hline
#   \end{array}

# %% [markdown]
# # –î–µ—Ç–µ–∫—Ü–∏—è –±–æ—Ç–æ–≤

# %%
import numpy as np
import pandas as pd

from implicit.nearest_neighbours import TFIDFRecommender

from rectools import Columns
from rectools.dataset import Dataset
from rectools.metrics import (
    Precision,
    NDCG,
    IntraListDiversity,
    Serendipity,
    calc_metrics,
)
from rectools.metrics.distances import PairwiseHammingDistanceCalculator
from rectools.models import LightFMWrapperModel
from lightfm import LightFM

# 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
ratings = pd.read_csv(
    "ratings.dat",
    sep="::",
    engine="python",
    header=None,
    names=[Columns.User, Columns.Item, Columns.Weight, Columns.Datetime],
)

ratings["datetime"] = pd.to_datetime(ratings["datetime"] * 10 ** 9)

movies = pd.read_csv(
    "movies.dat",
    sep="::",
    engine="python",
    header=None,
    names=[Columns.Item, "title", "genres"],
    encoding_errors="ignore",
)

# 2Ô∏è‚É£ –î–ï–¢–ï–ö–¶–ò–Ø –ë–û–¢–û–í

# 2.1 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
user_activity = ratings.groupby(Columns.User).agg(
    num_ratings=(Columns.Item, "count"),  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫
    unique_movies=(Columns.Item, "nunique"),  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å–º—ã
    rating_std=(Columns.Weight, "std"),  # –†–∞–∑–±—Ä–æ—Å –æ—Ü–µ–Ω–æ–∫
    first_rating_time=("datetime", "min"),
    last_rating_time=("datetime", "max")
)

# 2.2 –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è (–æ—Ü–µ–Ω–∫–∏ –≤ –¥–µ–Ω—å)
user_activity["rating_speed"] = user_activity["num_ratings"] / (
    (user_activity["last_rating_time"] - user_activity["first_rating_time"]).dt.days + 1
)

# 2.3 –£–¥–∞–ª–µ–Ω–∏–µ –±–æ—Ç–æ–≤ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º:
thresholds = {
    "num_ratings": user_activity["num_ratings"].quantile(0.99),  # > 99-–≥–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è
    "unique_movies": user_activity["unique_movies"].quantile(0.01),  # < 1-–≥–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è
    "rating_std": 0.1,  # –ù–µ—Ç —Ä–∞–∑–±—Ä–æ—Å–∞ –≤ –æ—Ü–µ–Ω–∫–∞—Ö
    "rating_speed": user_activity["rating_speed"].quantile(0.99),  # > 99-–≥–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è
}

bots = user_activity[
    (user_activity["num_ratings"] > thresholds["num_ratings"]) |
    (user_activity["unique_movies"] < thresholds["unique_movies"]) |
    (user_activity["rating_std"] < thresholds["rating_std"]) |
    (user_activity["rating_speed"] > thresholds["rating_speed"])
]

# –£–¥–∞–ª—è–µ–º –±–æ—Ç–æ–≤
ratings_cleaned = ratings[~ratings[Columns.User].isin(bots.index)]
print(f"–£–¥–∞–ª–µ–Ω–æ {len(bots)} –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.")

# 3Ô∏è‚É£ –î–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
split_dt = pd.Timestamp("2003-02-01")
df_train = ratings_cleaned.loc[ratings_cleaned["datetime"] < split_dt]
df_test = ratings_cleaned.loc[ratings_cleaned["datetime"] >= split_dt]

# 4Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ LightFM
dataset = Dataset.construct(df_train)

model = LightFMWrapperModel(
    model=LightFM(no_components=30)
)
model.fit(dataset)

# 5Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
recos = model.recommend(
    users=df_test[Columns.User].unique(),
    dataset=dataset,
    k=5,
    filter_viewed=True,
)

# 6Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫
serendipity = Serendipity(k=10)
precision = Precision(k=10, r_precision=True)
ndcg = NDCG(k=10, log_base=3)

movies["genre"] = movies["genres"].str.split("|")
genre_exploded = movies[[Columns.Item, "genre"]].set_index(Columns.Item).explode("genre")
genre_dummies = pd.get_dummies(genre_exploded, prefix="", prefix_sep="").groupby(Columns.Item).sum()

precision_value = precision.calc(reco=recos, interactions=df_test)
print(f"precision: {precision_value}")

catalog = df_train[Columns.Item].unique()
serendipity_value = serendipity.calc(
    reco=recos,
    interactions=df_test,
    prev_interactions=df_train,
    catalog=catalog
)
print("Serendipity: ", serendipity_value)

print("NDCG: ", ndcg.calc(reco=recos, interactions=df_test))

distance_calculator = PairwiseHammingDistanceCalculator(genre_dummies)
ild = IntraListDiversity(k=10, distance_calculator=distance_calculator)
print("ILD: ", ild.calc(reco=recos))


# %% [markdown]
#

# %% [markdown]
# # NLP NIR Natasha

# %%
!pip install natasha

# %%
import pandas as pd
from natasha import (
    Doc,
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Natasha
segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)
syntax_parser = NewsSyntaxParser(emb)
ner_tagger = NewsNERTagger(emb)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
def extract_entities(sentence, sentence_id):
    doc = Doc(sentence)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)
    doc.tag_ner(ner_tagger)

    entities = []
    for span in doc.spans:
        entities.append({
            'sentence_id': sentence_id,
            'entity': span.text,
            'entity_type': span.type
        })
    return entities

# –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
sentence = "–ë—É—Ä—è—Ç–∏—è –∏ –ó–∞–±–∞–π–∫–∞–ª—å—Å–∫–∏–π –∫—Ä–∞–π –ø–µ—Ä–µ–¥–∞–Ω—ã –∏–∑ –°–∏–±–∏—Ä—Å–∫–æ–≥–æ —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–≥–∞ (–°–§–û) –≤ —Å–æ—Å—Ç–∞–≤ –î–∞–ª—å–Ω–µ–≤–æ—Å—Ç–æ—á–Ω–æ–≥–æ (–î–§–û). –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —É–∫–∞–∑ –ø–æ–¥–ø–∏—Å–∞–ª –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω, –¥–æ–∫—É–º–µ–Ω—Ç –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ—Ä—Ç–∞–ª–µ –ø—Ä–∞–≤–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠—Ç–∏–º –∂–µ —É–∫–∞–∑–æ–º –≥–ª–∞–≤–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ –ø–æ—Ä—É—á–∏–ª —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—é —Å–≤–æ–µ–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ —É—Ç–≤–µ—Ä–¥–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —à—Ç–∞—Ç–Ω—É—é —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –∞–ø–ø–∞—Ä–∞—Ç–æ–≤ –ø–æ–ª–Ω–æ–º–æ—á–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –≤ —ç—Ç–∏—Ö –¥–≤—É—Ö –æ–∫—Ä—É–≥–∞—Ö. –ü–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ë—É—Ä—è—Ç–∏–∏ –∏ –ó–∞–±–∞–π–∫–∞–ª—å—è –≤ —Å–æ—Å—Ç–∞–≤–µ –°–§–û –æ—Å—Ç–∞–ª–∏—Å—å –¥–µ—Å—è—Ç—å —Ä–µ–≥–∏–æ–Ω–æ–≤: –ê–ª—Ç–∞–π, –ê–ª—Ç–∞–π—Å–∫–∏–π –∫—Ä–∞–π, –ò—Ä–∫—É—Ç—Å–∫–∞—è, –ö–µ–º–µ—Ä–æ–≤—Å–∫–∞—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è, –û–º—Å–∫–∞—è –∏ –¢–æ–º—Å–∫–∞—è –æ–±–ª–∞—Å—Ç–∏, –ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫–∏–π –∫—Ä–∞–π, –¢—É–≤–∞ –∏ –•–∞–∫–∞—Å–∏—è. –î–µ–π—Å—Ç–≤—É—é—â–∏–º –ø–æ–ª–ø—Ä–µ–¥–æ–º –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –≤ —ç—Ç–æ–º –æ–∫—Ä—É–≥–µ —è–≤–ª—è–µ—Ç—Å—è –±—ã–≤—à–∏–π –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—è, —ç–∫—Å-–∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å –∫–æ–º–∞–Ω–¥—É—é—â–µ–≥–æ –ß–µ—Ä–Ω–æ–º–æ—Ä—Å–∫–∏–º —Ñ–ª–æ—Ç–æ–º –†–æ—Å—Å–∏–∏ –°–µ—Ä–≥–µ–π –ú–µ–Ω—è–π–ª–æ. –í —Å–æ—Å—Ç–∞–≤–µ –î–§–û –æ—Ç–Ω—ã–Ω–µ 11 —Å—É–±—ä–µ–∫—Ç–æ–≤. –ü–æ–º–∏–º–æ –ë—É—Ä—è—Ç–∏–∏ –∏ –ó–∞–±–∞–π–∫–∞–ª—å—è, —ç—Ç–æ –ö–∞–º—á–∞—Ç—Å–∫–∏–π, –ü—Ä–∏–º–æ—Ä—Å–∫–∏–π –∏ –•–∞–±–∞—Ä–æ–≤—Å–∫–∏–π –∫—Ä–∞—è, –ê–º—É—Ä—Å–∫–∞—è, –ï–≤—Ä–µ–π—Å–∫–∞—è –∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è, –ú–∞–≥–∞–¥–∞–Ω—Å–∫–∞—è –∏ –°–∞—Ö–∞–ª–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç–∏, –∞ —Ç–∞–∫–∂–µ –Ø–∫—É—Ç–∏—è –∏ –ß—É–∫–æ—Ç–∫–∞. –î–∞–ª—å–Ω–µ–≤–æ—Å—Ç–æ—á–Ω–æ–µ –ø–æ–ª–ø—Ä–µ–¥—Å—Ç–≤–æ –≤–æ–∑–≥–ª–∞–≤–ª—è–µ—Ç –Æ—Ä–∏–π –¢—Ä—É—Ç–Ω–µ–≤, —Å–æ–≤–º–µ—â–∞—é—â–∏–π —ç—Ç—É –¥–æ–ª–∂–Ω–æ—Å—Ç—å —Å –ø–æ—Å—Ç–æ–º –≤–∏—Ü–µ-–ø—Ä–µ–º—å–µ—Ä–∞ –≤ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–µ –†–æ—Å—Å–∏–∏. –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –æ–∫—Ä—É–≥–∞ –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ –º–∞–µ 2000 –≥–æ–¥–∞ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —É–∫–∞–∑–æ–º –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –ü—É—Ç–∏–Ω–∞."
sentence_id = 1  # ID –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
entities = extract_entities(sentence, sentence_id)

# –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
df = pd.DataFrame(entities)

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
df.head(20)

# %% [markdown]
# # –û–±—Ä–∞–±–æ—Ç–∫–∞ + Word2Vec + TF-IDF

# %%
!pip install pymorphy3

# %%
categories = {
    "–ñ–∏–∑–Ω—å —á–µ–ª–æ–≤–µ–∫–∞": {
        "description": "–ó–∞–¥–∞–Ω–∏—è –æ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –∂–∏–∑–Ω–∏, –¥–æ–Ω–æ—Ä—Å—Ç–≤–µ, –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è—Ö",
        "examples": [
            "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –¥–æ–Ω–æ—Ä—Å–∫–æ–π –∞–∫—Ü–∏–∏",
            "–ø–µ—Ä–µ–ª–∏–≤–∞–Ω–∏–µ –∫—Ä–æ–≤–∏",
            "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ",
            "–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π",
            "–ø–µ—Ä–≤–∞—è –ø–æ–º–æ—â—å"
        ]
    },
    "–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–æ —á–µ–ª–æ–≤–µ–∫–∞": {
        "description": "–ó–∞–¥–∞–Ω–∏—è –æ–± —É–≤–∞–∂–µ–Ω–∏–∏ –∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º –∏ –ª—é–¥—è–º —Ä–∞–∑–Ω—ã—Ö —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç—É—Å–æ–≤",
        "examples": [
            "–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å –æ —É–≤–∞–∂–µ–Ω–∏–∏",
            "—Å–æ—Ü–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å",
            "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç—Ç–∏–∫–∞",
            "—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π",
            "—Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—å",
            ""
        ]
    },
    "–ü—Ä–∞–≤–∞ –∏ —Å–≤–æ–±–æ–¥—ã —á–µ–ª–æ–≤–µ–∫–∞": {
        "description": "–ò–∑—É—á–µ–Ω–∏–µ –∏ –∑–∞—â–∏—Ç–∞ –ø—Ä–∞–≤ —á–µ–ª–æ–≤–µ–∫–∞",
        "examples": [
            "—Ç–µ—Å—Ç –æ –ø—Ä–∞–≤–∞—Ö —á–µ–ª–æ–≤–µ–∫–∞",
            "–∫–æ–Ω–≤–µ–Ω—Ü–∏—è –æ –ø—Ä–∞–≤–∞—Ö —Ä–µ–±–µ–Ω–∫–∞",
            "–ø—Ä–∞–≤–æ–≤–∞—è –≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å",
            "–∑–∞—â–∏—Ç–∞ —Å–≤–æ–±–æ–¥",
            "–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–µ –ø—Ä–∞–≤–∞"
        ]
    },
    "–ü–∞—Ç—Ä–∏–æ—Ç–∏–∑–º": {
        "description": "–ò—Å—Ç–æ—Ä–∏—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞, –ø—Ä–µ–¥–∫–∏, –ø–∞—Ç—Ä–∏–æ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏",
        "examples": [
            "–†–æ—Å—Å–∏—è",
            "–†–æ—Å—Å–∏–π—Å–∫–∞—è —Ñ–µ–¥–µ—Ä–∞—Ü–∏—è",
            "–°–°–°–†"
            "–ø–æ—Å–µ—â–µ–Ω–∏–µ –≤–æ–µ–Ω–Ω–æ–≥–æ –º—É–∑–µ—è",
            "–≤–æ–ª–æ–Ω—Ç–µ—Ä—Å—Ç–≤–æ 9 –º–∞—è",
            "–∏—Å—Ç–æ—Ä–∏—è –≤–µ–ª–∏–∫–æ–π –æ—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π",
            "–ø–∞—Ç—Ä–∏–æ—Ç–∏—á–µ—Å–∫–∏–π —Ñ–ª–µ—à–º–æ–±",
            "–≥–µ—Ä–æ–∏ —Ä–æ—Å—Å–∏–∏"
        ]
    },
    "–ì—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å": {
        "description": "–ü—Ä–æ—Ü–≤–µ—Ç–∞–Ω–∏–µ –æ–±—â–µ—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ –ª–∏—á–Ω–æ–µ —É—á–∞—Å—Ç–∏–µ",
        "examples": [
            "—Å—É–±–±–æ—Ç–Ω–∏–∫",
            "—ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∞–∫—Ü–∏—è",
            "–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–µ –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—ã",
            "–±–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≥–æ—Ä–æ–¥–∞",
            "–æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å"
        ]
    },
    "–°–ª—É–∂–µ–Ω–∏–µ –û—Ç–µ—á–µ—Å—Ç–≤—É –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –µ–≥–æ —Å—É–¥—å–±—É": {
        "description": "–ò—Å—Ç–æ—Ä–∏—è –æ—Ç–µ—á–µ—Å—Ç–≤–∞ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≥—Ä–∞–∂–¥–∞–Ω",
        "examples": [
            "–≤–æ–µ–Ω–Ω–æ-—Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ –∏–≥—Ä—ã",
            "—É—Ä–æ–∫–∏ –º—É–∂–µ—Å—Ç–≤–∞",
            "–∑–¥–æ—Ä–æ–≤—å–µ –Ω–∞—Ü–∏–∏",
            "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è",
            "–≤–∞—Ö—Ç–∞ –ø–∞–º—è—Ç–∏"
        ]
    },
    "–í—ã—Å–æ–∫–∏–µ –Ω—Ä–∞–≤—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–¥–µ–∞–ª—ã": {
        "description": "–ö—É–ª—å—Ç—É—Ä–∞, –∏–¥–µ–∏ –∏ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ, —Ñ–æ—Ä–º–∏—Ä—É—é—â–∏–µ –º–æ—Ä–∞–ª—å",
        "examples": [
            "–æ–±—Å—É–∂–¥–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã",
            "–Ω—Ä–∞–≤—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∏–ª–µ–º–º—ã",
            "—ç—Ç–∏—á–µ—Å–∫–∏–µ –Ω–æ—Ä–º—ã",
            "–º–æ—Ä–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä",
            "–¥—É—Ö–æ–≤–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏",
            "–∏—Å—Ç–æ—Ä–∏—è",
            "—Å—Ç—Ä–∞–Ω—ã"
        ]
    },
    "–ö—Ä–µ–ø–∫–∞—è —Å–µ–º—å—è": {
        "description": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è —Å–µ–º–µ–π–Ω–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è",
        "examples": [
            "—Å–µ–º–µ–π–Ω—ã–π –∫–≤–µ—Å—Ç",
            "–≥–µ–Ω–µ–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –¥—Ä–µ–≤–æ",
            "—Å–æ–≤–º–µ—Å—Ç–Ω—ã–π –ø–∏–∫–Ω–∏–∫",
            "—Å–µ–º–µ–π–Ω—ã–µ —Ç—Ä–∞–¥–∏—Ü–∏–∏",
            "—Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –¥–µ–Ω—å"
        ]
    },
    "–°–æ–∑–∏–¥–∞—Ç–µ–ª—å–Ω—ã–π —Ç—Ä—É–¥": {
        "description": "–û–±—É—á–µ–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –Ω–∞–≤—ã–∫–∞–º –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –ø–æ–º–æ—â—å",
        "examples": [
            "–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å –ø–æ —Ä–µ–º–µ—Å–ª–∞–º",
            "–ø–æ–º–æ—â—å –ø–æ–∂–∏–ª—ã–º",
            "—Ç—Ä—É–¥–æ–≤–æ–π –¥–µ—Å–∞–Ω—Ç",
            "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±—ã",
            "—Å–æ—Ü–∏–∞–ª—å–Ω—ã–π —Ç—Ä—É–¥"
        ]
    },
    "–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥—É—Ö–æ–≤–Ω–æ–≥–æ –Ω–∞–¥ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–º": {
        "description": "–ü–æ–º–æ—â—å –Ω—É–∂–¥–∞—é—â–∏–º—Å—è –≤–º–µ—Å—Ç–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã—Ö –±–ª–∞–≥",
        "examples": [
            "–±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ü–∏—è",
            "–ø–æ–º–æ—â—å –ø—Ä–∏—é—Ç—É",
            "–¥—É—Ö–æ–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏",
            "–≤–æ–ª–æ–Ω—Ç–µ—Ä—Å—Ç–≤–æ –≤ —Ö—Ä–∞–º–µ",
            "–Ω–µ–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏"
        ]
    },
    "–ì—É–º–∞–Ω–∏–∑–º": {
        "description": "–ü–æ–º–æ—â—å –ª—é–¥—è–º –∏ –¥–æ–±—Ä—ã–µ –¥–µ–ª–∞",
        "examples": [
            "–≤–æ–ª–æ–Ω—Ç–µ—Ä—Å—Ç–≤–æ –≤ –ø—Ä–∏—é—Ç–µ",
            "–ø–æ–º–æ—â—å –±–µ–∑–¥–æ–º–Ω—ã–º",
            "–¥–æ–±—Ä—ã–µ –ø–∏—Å—å–º–∞",
            "–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∏–Ω–≤–∞–ª–∏–¥–æ–≤",
            "—á–µ–ª–æ–≤–µ–∫–æ–ª—é–±–∏–µ"
        ]
    },
    "–ú–∏–ª–æ—Å–µ—Ä–¥–∏–µ": {
        "description": "–ü–æ–º–æ—â—å –æ–∫—Ä—É–∂–∞—é—â–∏–º –∏ –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ –±–æ–ª–µ–∑–Ω—è—Ö",
        "examples": [
            "—Å–±–æ—Ä –≤–µ—â–µ–π –Ω—É–∂–¥–∞—é—â–∏–º—Å—è",
            "–ø–æ–º–æ—â—å –ø–æ–∂–∏–ª—ã–º",
            "–¥–æ–Ω–æ—Ä—Å—Ç–≤–æ –æ—Ä–≥–∞–Ω–æ–≤",
            "–ø–∞–ª–ª–∏–∞—Ç–∏–≤–Ω–∞—è –ø–æ–º–æ—â—å",
            "—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞"
        ]
    },
    "–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å": {
        "description": "–û–±—Å—É–∂–¥–µ–Ω–∏–µ –∏ —É–∫—Ä–µ–ø–ª–µ–Ω–∏–µ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏",
        "examples": [
            "–¥–µ–±–∞—Ç—ã –æ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ",
            "–ø—Ä–∞–≤–æ–≤—ã–µ –∫–≤–µ—Å—Ç—ã",
            "—á–µ—Å—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
            "–±–æ—Ä—å–±–∞ —Å –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏–µ–π",
            "—Ä–∞–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏"
        ]
    },
    "–ö–æ–ª–ª–µ–∫—Ç–∏–≤–∏–∑–º": {
        "description": "–ö–æ–º–∞–Ω–¥–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –≤ –≥—Ä—É–ø–ø–µ",
        "examples": [
            "–≥—Ä—É–ø–ø–æ–≤–æ–π —Ç—É—Ä–ø–æ—Ö–æ–¥",
            "–∫–æ–º–∞–Ω–¥–Ω—ã–π –∫–≤–∏–∑",
            "—Å–æ–≤–º–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç",
            "—Ä–∞–±–æ—Ç–∞ –≤ –∫–æ–º–∞–Ω–¥–µ",
            "–∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ"
        ]
    },
    "–í–∑–∞–∏–º–æ–ø–æ–º–æ—â—å –∏ –≤–∑–∞–∏–º–æ—É–≤–∞–∂–µ–Ω–∏–µ": {
        "description": "–í–æ–ª–æ–Ω—Ç–µ—Ä—Å—Ç–≤–æ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –æ–∫—Ä—É–∂–∞—é—â–∏—Ö",
        "examples": [
            "–ø–æ–º–æ—â—å –æ–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∞–º",
            "—Ç—å—é—Ç–æ—Ä—Å—Ç–≤–æ",
            "—à–µ—Ñ—Å–∫–∞—è –ø–æ–º–æ—â—å",
            "—Å–æ—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ",
            "–≤–∑–∞–∏–º–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞"
        ]
    },
    "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –∏ –ø—Ä–µ–µ–º—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–∫–æ–ª–µ–Ω–∏–π": {
        "description": "–ò—Å—Ç–æ—Ä–∏—è —Å–µ–º—å–∏ –∏ —Å—Ç—Ä–∞–Ω—ã, —Å–≤—è–∑—å –ø–æ–∫–æ–ª–µ–Ω–∏–π",
        "examples": [
            "–∏–Ω—Ç–µ—Ä–≤—å—é —Å –≤–µ—Ç–µ—Ä–∞–Ω–∞–º–∏",
            "—Å–µ–º–µ–π–Ω–∞—è –ª–µ—Ç–æ–ø–∏—Å—å",
            "–º—É–∑–µ–π –∏—Å—Ç–æ—Ä–∏–∏",
            "—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å–æ–±—ã—Ç–∏–π",
            "—É—Å—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è"
        ]
    },
    "–ï–¥–∏–Ω—Å—Ç–≤–æ –Ω–∞—Ä–æ–¥–æ–≤ –†–æ—Å—Å–∏–∏": {
        "description": "–ö—É–ª—å—Ç—É—Ä–Ω–æ–µ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–µ –∏ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–¥–∏—Ü–∏–∏",
        "examples": [
            "—Ñ–µ—Å—Ç–∏–≤–∞–ª—å –∫—É–ª—å—Ç—É—Ä",
            "—ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –º—É–∑–µ–π",
            "–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–º–µ—Å–ª–∞",
            "–º–µ–∂–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –¥–∏–∞–ª–æ–≥",
            "—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –æ–±—ã—á–∞–∏"
        ]
    }
}

# %%
import pandas as pd
import numpy as np
import re
from string import punctuation
import pymorphy3
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

morph = pymorphy3.MorphAnalyzer()
russian_stopwords = stopwords.words('russian')

def preprocess(text):
    text = text.lower()
    text = re.sub(f'[{punctuation}¬ª¬´‚Äì‚Ä¶‚Ññ¬©‚Ñ¢‚Ä¢¬∞]', '', text)
    words = re.findall(r'\b[a-z–∞-—è—ë]+\b', text)
    lemmas = []
    for word in words:
        if word not in russian_stopwords and len(word) > 2:
            lemma = morph.parse(word)[0].normal_form
            lemmas.append(lemma)
    return ' '.join(lemmas)

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
category_docs = []
for cat, data_s in categories.items():
    doc = f"{cat} {data_s['description']} {' '.join(data_s['examples'])}"
    category_docs.append(doc)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TF-IDF
vectorizer = TfidfVectorizer(tokenizer=preprocess)
tfidf_matrix = vectorizer.fit_transform(category_docs)

def classify_text(text):
    processed_text = preprocess(text)

    text_vector = vectorizer.transform([processed_text])
    print(text_vector)
    similarities = cosine_similarity(text_vector, tfidf_matrix)
    print(similarities)
    best_match_idx = similarities.argmax()
    best_category = list(categories.keys())[best_match_idx]
    accuracy = similarities[0][best_match_idx]


    return best_category, accuracy

# %%
input_text = "–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ —Ç—É—Ä–ø–æ—Ö–æ–¥–∞ —Å –æ–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∞–º–∏ –∏ —Ä–∞–±–æ—Ç–∞ –≤ –∫–æ–º–∞–Ω–¥–µ"
result = classify_text(input_text)
result

# %% [markdown]
# # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

# %%
import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â—É—é —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")

# üìå –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤
queries = [
    "–ö–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å –±–æ—Ä—â?",
    "–†–µ—Ü–µ–ø—Ç –≤–∫—É—Å–Ω–æ–≥–æ –±–æ—Ä—â–∞",
    "–ì–¥–µ –Ω–∞–π—Ç–∏ —Ä–µ—Ü–µ–ø—Ç –ª–∞–∑–∞–Ω—å–∏?",
    "–ö–∞–∫ —Å–≤–∞—Ä–∏—Ç—å —Å—É–ø?"
]

# üî• –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
query_embeddings = model.encode(queries, convert_to_tensor=True)

# üî• –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
similarity_matrix = cosine_similarity(query_embeddings.cpu().numpy())

# üìå –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
df = pd.DataFrame(similarity_matrix, index=queries, columns=queries)
print(df)


# %%
df.head()

# %% [markdown]
# # rubert (semi-final version; sentiment analysys/text classification)

# %%
import pandas as pd
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

# %%
train=pd.read_csv(r'C:\VisualCode\T1\tr.csv', engine='python', encoding='utf-8', on_bad_lines="skip")
test=pd.read_csv(r'C:\VisualCode\T1\ts.csv', engine='python', encoding='utf-8', on_bad_lines = "skip")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–µ—Ç–∫–∏
X_train, X_val, y_train, y_val = train_test_split(train['review'], train['sentiment'], test_size=0.2, random_state=42)

# %%
class ReviewsDataset(Dataset):
    def __init__(self, reviews, labels=None, tokenizer=None, max_len=128):
        self.reviews = reviews
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.reviews)

    def __getitem__(self, idx):
        review = self.reviews.iloc[idx]
        inputs = self.tokenizer.encode_plus(
            review,
            add_special_tokens=True,
            max_length=self.max_len,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        return item


# %%
tokenizer = BertTokenizer.from_pretrained("DeepPavlov/rubert-base-cased")
model = BertForSequenceClassification.from_pretrained(
    "DeepPavlov/rubert-base-cased",
    num_labels=3  # –£ –Ω–∞—Å —Ç—Ä–∏ –∫–ª–∞—Å—Å–∞: 0, 1, 2
)

# %%
train_dataset = ReviewsDataset(X_train, y_train, tokenizer)
val_dataset = ReviewsDataset(X_val, y_val, tokenizer)
test_dataset = ReviewsDataset(test['review'], tokenizer=tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)
test_loader = DataLoader(test_dataset, batch_size=16)

# %%
from transformers import AdamW

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

# –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
def train_epoch(model, data_loader, optimizer, device):
    model.train()
    total_loss = 0
    for batch in data_loader:
        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}
        labels = batch['labels'].to(device)

        outputs = model(**inputs, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    return total_loss / len(data_loader)

# %%
import time

epochs = 3  # –ú–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞

for epoch in range(epochs):
    start_time = time.time()

    train_loss = train_epoch(model, train_loader, optimizer, device)

    end_time = time.time()
    epoch_duration = end_time - start_time

    hours = int(epoch_duration // 3600)
    minutes = int((epoch_duration % 3600) // 60)
    seconds = int(epoch_duration % 60)

    torch.save(model.state_dict(), f"model_epoch_{epoch+1}.pth")
    print(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{3} –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    print(f"–ü–æ—Ç–µ—Ä—è: {train_loss:.4f}, –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {epoch_duration:.2f} —Å–µ–∫—É–Ω–¥.")


# %%
def predict(model, data_loader, device):
    model.eval()
    predictions = []
    with torch.no_grad():
        for batch in data_loader:
            inputs = {key: val.to(device) for key, val in batch.items()}
            outputs = model(**inputs)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            predictions.extend(preds)
    return predictions

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
test['sentiment'] = predict(model, test_loader, device)


# %%
ans = test[['index', 'sentiment']]
ans.head(3)
ans.to_csv('poputka_0.csv', index = False)

# %% [markdown]
# # RuBert + Distilbert (for classification and sentiment analysys)

# %%
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from datasets import load_dataset
from transformers import Trainer, TrainingArguments
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
train_dataset = load_dataset('csv', data_files='train.csv')['train']
test_dataset = load_dataset('csv', data_files='test.csv')['test']

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokenizer = RobertaTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')

def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
model = RobertaForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=3)  # –Ω–∞–ø—Ä–∏–º–µ—Ä, 3 –∫–ª–∞—Å—Å–∞

# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    evaluation_strategy="epoch",
    logging_dir='./logs',
)

# –°–æ–∑–¥–∞–µ–º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
trainer.train()

# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
predictions = trainer.predict(test_dataset)
pred_labels = torch.argmax(predictions.predictions, axis=1)
print(pred_labels)


# %%
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from transformers import Trainer, TrainingArguments
from datasets import load_dataset
import torch

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
train_dataset = load_dataset('csv', data_files='train.csv')['train']
test_dataset = load_dataset('csv', data_files='test.csv')['test']

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)  # 3 –∫–ª–∞—Å—Å–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞

# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    evaluation_strategy="epoch",
    logging_dir='./logs',
)

# –°–æ–∑–¥–∞–µ–º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
trainer.train()

# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
predictions = trainer.predict(test_dataset)
pred_labels = torch.argmax(predictions.predictions, axis=1)
print(pred_labels)
