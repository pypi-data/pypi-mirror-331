Metadata-Version: 2.4
Name: sik-llms
Version: 0.1.6
Summary: placeholder
License-File: LICENSE
Requires-Python: >=3.12
Requires-Dist: anthropic>=0.47.0
Requires-Dist: dotenv>=0.9.9
Requires-Dist: nest-asyncio>=1.6.0
Requires-Dist: openai>=1.64.0
Requires-Dist: pydantic>=2.10.6
Requires-Dist: tiktoken>=0.9.0
Description-Content-Type: text/markdown

# sik-llms

Easy llm interface. Sync and Async support.

```
from sik_llms import create_client, user_message, ChatChunkResponse

model = create_client(
    model_name='gpt-4o-mini',  # or e.g. 'claude-3-7-sonnet-latest'
    temperature=0.1,
)
message = user_message("What is the capital of France?")

# sync
response = model(messages=[message])

# async streaming
responses = []
summary = None
async for response in model.run_async(messages=[message]):
    if isinstance(response, ChatChunkResponse):
        print(response.content, end="")
        responses.append(response)
    else:
        summary = response

print(summary)
```

## Installation

`uv install sik-llms` or `pip install sik-llms`
