Index: deepsearcher/agent/chain_of_rag.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/deepsearcher/agent/chain_of_rag.py b/deepsearcher/agent/chain_of_rag.py
new file mode 100644
--- /dev/null	(date 1740386386119)
+++ b/deepsearcher/agent/chain_of_rag.py	(date 1740386386119)
@@ -0,0 +1,170 @@
+from typing import Tuple, List
+
+from deepsearcher.embedding.base import BaseEmbedding
+from deepsearcher.llm.base import BaseLLM
+from deepsearcher.vector_db import RetrievalResult
+from deepsearcher.vector_db.base import BaseVectorDB, deduplicate_results
+
+FOLLOWUP_QUERY_PROMPT = """You are using a search tool to answer the main query by iteratively searching the database. Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up questions only as the search tool may not understand complex questions.
+
+## Previous intermediate queries and answers
+{intermediate_context}
+
+## Main query to answer
+{query}
+
+Respond with a simple follow-up question that will help answer the main query, do not explain yourself or output anything else.
+"""
+
+INTERMEDIATE_ANSWER_PROMPT = """Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond “No relevant information found” if the documents do not contain useful information.
+
+## Documents
+{retrieved_documents}
+
+## Query
+{sub_query}
+
+Respond with a concise answer only, do not explain yourself or output anything else.
+"""
+
+FINAL_ANSWER_PROMPT = """Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate.
+
+## Documents
+{retrieved_documents}
+
+## Intermediate queries and answers
+{intermediate_context}
+
+## Main query
+{query}
+
+Respond with an appropriate answer only, do not explain yourself or output anything else.
+"""
+
+REFLECTION_PROMPT = """Given the following intermediate queries and answers, judge whether you have enough information to answer the main query. If you believe you have enough information, respond with “Yes”, otherwise respond with “No”.
+
+## Intermediate queries and answers
+{intermediate_context}
+
+## Main query
+{query}
+
+Respond with “Yes” or “No” only, do not explain yourself or output anything else.
+"""
+
+GET_SUPPORTED_DOCS_PROMPT = """Given the following documents, select the ones that are support the Q-A pair.
+
+## Documents
+{retrieved_documents}
+
+## Q-A Pair
+### Question
+{query}
+### Answer
+{answer}
+
+Respond with a python list of indices of the selected documents.
+"""
+
+
+class ChainOfRAG:  # todo subclass base agent
+    def __init__(
+        self,
+        llm: BaseLLM,
+        embedding_model: BaseEmbedding,
+        vectordb: BaseVectorDB,
+        **kwargs,
+    ):
+        self.llm = llm
+        self.embedding_model = embedding_model
+        self.vectordb = vectordb
+        self.intermediate_context = []
+
+    def retrieve(
+        self, original_query: str, max_iter: int = 4
+    ) -> Tuple[List[RetrievalResult], List[str], int]:
+        all_retrieved_results = []
+        for iter in range(max_iter):
+            chat_response = self.llm.chat(
+                [
+                    {
+                        "role": "user",
+                        "content": FOLLOWUP_QUERY_PROMPT.format(
+                            query=original_query,
+                            intermediate_context="\n".join(self.intermediate_context),
+                        ),
+                    }
+                ]
+            )
+            followup_query = chat_response.content
+
+            query_vector = self.embedding_model.embed_query(followup_query)
+            retrieved_results = self.vectordb.search_data(
+                collection=None, vector=query_vector
+            )
+
+            chat_response = self.llm.chat(
+                [
+                    {
+                        "role": "user",
+                        "content": INTERMEDIATE_ANSWER_PROMPT.format(
+                            retrieved_documents=self.format_retrieved_results(retrieved_results),
+                            sub_query=followup_query,
+                        ),
+                    }
+                ]
+            )
+            intermediate_answer = chat_response.content
+
+            if "No relevant information found" not in intermediate_answer:
+                chat_response = self.llm.chat(
+                    [
+                        {
+                            "role": "user",
+                            "content": GET_SUPPORTED_DOCS_PROMPT.format(
+                            retrieved_documents=self.format_retrieved_results(retrieved_results),
+                            query=followup_query,
+                            answer=intermediate_answer,
+                            ),
+                        }
+                    ]
+                )
+                supported_doc_indices = self.llm.literal_eval(chat_response.content)
+                # get retrieved_results from retrived_results using supported_doc_indices
+                supported_retrieved_results = [retrieved_results[i] for i in supported_doc_indices]
+                all_retrieved_results.extend(supported_retrieved_results)
+
+            self.intermediate_context.append(
+                f"Intermediate query{len(self.intermediate_context) + 1}: {followup_query}\nIntermediate answer{len(self.intermediate_context) + 1}: {intermediate_answer}"
+            )
+        all_retrieved_results = deduplicate_results(all_retrieved_results)
+        return all_retrieved_results, [], 0#todo
+
+
+
+    def query(
+        self, original_query: str, max_iter: int = 3
+    ) -> Tuple[str, List[RetrievalResult], int]:
+        all_retrieved_results, _, _ = self.retrieve(original_query, max_iter)
+        chat_response = self.llm.chat(
+            [
+                {
+                    "role": "user",
+                    "content": FINAL_ANSWER_PROMPT.format(
+                        retrieved_documents=self.format_retrieved_results(all_retrieved_results),
+                        intermediate_context="\n".join(self.intermediate_context),
+                        query=original_query,
+                    ),
+                }
+            ]
+        )
+        final_answer = chat_response.content
+        return final_answer, all_retrieved_results, 0 #todo
+
+    def format_retrieved_results(self, retrieved_results: List[RetrievalResult]) -> str:
+        formatted_documents = []
+        for i, result in enumerate(retrieved_results):
+            formatted_documents.append(
+                f"<Document {i}>\n{result.text}\n<\Document {i}>"
+            )
+        return "\n".join(formatted_documents)
Index: deepsearcher/online_query.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Tuple, List\n\nfrom deepsearcher.agent import (\n    generate_sub_queries,\n    generate_gap_queries,\n    generate_final_answer,\n)\nfrom deepsearcher.agent.search_vdb import search_chunks_from_vectordb\nfrom deepsearcher.vector_db.base import deduplicate_results, RetrievalResult\n\n# from deepsearcher.configuration import vector_db, embedding_model, llm\nfrom deepsearcher import configuration\nfrom deepsearcher.tools import log\nimport asyncio\n\n\n# Add a wrapper function to support synchronous calls\ndef query(\n    original_query: str, max_iter: int = 3\n) -> Tuple[str, List[RetrievalResult], int]:\n    return asyncio.run(async_query(original_query, max_iter))\n\nasync def async_query(\n    original_query: str, max_iter: int = 3\n) -> Tuple[str, List[RetrievalResult], int]:\n    retrieval_res, all_sub_queries, retrieve_conseumed_token = await async_retrieve(\n        original_query, max_iter\n    )\n    ### GENERATE FINAL ANSWER ###\n    log.color_print(\"<think> Generating final answer... </think>\\n\")\n    final_answer, final_consumed_token = generate_final_answer(\n        original_query, all_sub_queries, retrieval_res\n    )\n    log.color_print(\"\\n==== FINAL ANSWER====\\n\")\n    log.color_print(final_answer)\n    return final_answer, retrieval_res, retrieve_conseumed_token + final_consumed_token\n\n\ndef retrieve(\n    original_query: str, max_iter: int = 3\n) -> Tuple[List[RetrievalResult], List[str], int]:\n    return asyncio.run(async_retrieve(original_query, max_iter))\n\n\nasync def async_retrieve(\n    original_query: str, max_iter: int = 3\n) -> Tuple[List[RetrievalResult], List[str], int]:\n    log.color_print(f\"<query> {original_query} </query>\\n\")\n    all_search_res = []\n    all_sub_queries = []\n    total_tokens = 0\n\n    ### SUB QUERIES ###\n    sub_queries, used_token = generate_sub_queries(original_query)\n    total_tokens += used_token\n    if not sub_queries:\n        log.color_print(\"No sub queries were generated by the LLM. Exiting.\")\n        return [], [], total_tokens\n    else:\n        log.color_print(\n            f\"<think> Break down the original query into new sub queries: {sub_queries}</think>\\n\"\n        )\n    all_sub_queries.extend(sub_queries)\n    sub_gap_queries = sub_queries\n\n    for iter in range(max_iter):\n        log.color_print(f\">> Iteration: {iter + 1}\\n\")\n        search_res_from_vectordb = []\n        search_res_from_internet = []  # TODO\n\n        # Create all search tasks\n        search_tasks = [\n            search_chunks_from_vectordb(query, sub_gap_queries)\n            for query in sub_gap_queries\n        ]\n        # Execute all tasks in parallel and wait for results\n        search_results = await asyncio.gather(*search_tasks)\n        # Merge all results\n        for result in search_results:\n            search_res, consumed_token = result\n            total_tokens += consumed_token\n            search_res_from_vectordb.extend(search_res)\n\n        search_res_from_vectordb = deduplicate_results(search_res_from_vectordb)\n        # search_res_from_internet = deduplicate_results(search_res_from_internet)\n        all_search_res.extend(search_res_from_vectordb + search_res_from_internet)\n\n        ### REFLECTION & GET GAP QUERIES ###\n        log.color_print(\"<think> Reflecting on the search results... </think>\\n\")\n        sub_gap_queries, consumed_token = generate_gap_queries(\n            original_query, all_sub_queries, all_search_res\n        )\n        total_tokens += consumed_token\n        if not sub_gap_queries:\n            log.color_print(\n                \"<think> No new search queries were generated. Exiting. </think>\\n\"\n            )\n            break\n        else:\n            log.color_print(\n                f\"<think> New search queries for next iteration: {sub_gap_queries} </think>\\n\"\n            )\n            all_sub_queries.extend(sub_gap_queries)\n\n    all_search_res = deduplicate_results(all_search_res)\n    return all_search_res, all_sub_queries, total_tokens\n\n\ndef naive_retrieve(\n    query: str, collection: str = None, top_k=10\n) -> List[RetrievalResult]:\n    vector_db = configuration.vector_db\n    embedding_model = configuration.embedding_model\n\n    if not collection:\n        retrieval_res = []\n        collections = [\n            col_info.collection_name for col_info in vector_db.list_collections()\n        ]\n        for collection in collections:\n            retrieval_res_col = vector_db.search_data(\n                collection=collection,\n                vector=embedding_model.embed_query(query),\n                top_k=top_k // len(collections),\n            )\n            retrieval_res.extend(retrieval_res_col)\n        retrieval_res = deduplicate_results(retrieval_res)\n    else:\n        retrieval_res = vector_db.search_data(\n            collection=collection,\n            vector=embedding_model.embed_query(query),\n            top_k=top_k,\n        )\n    return retrieval_res\n\n\ndef naive_rag_query(\n    query: str, collection: str = None, top_k=10\n) -> Tuple[str, List[RetrievalResult]]:\n    llm = configuration.llm\n    retrieval_res = naive_retrieve(query, collection, top_k)\n\n    chunk_texts = []\n    for chunk in retrieval_res:\n        if \"wider_text\" in chunk.metadata:\n            chunk_texts.append(chunk.metadata[\"wider_text\"])\n        else:\n            chunk_texts.append(chunk.text)\n    mini_chunk_str = \"\"\n    for i, chunk in enumerate(chunk_texts):\n        mini_chunk_str += f\"\"\"<chunk_{i}>\\n{chunk}\\n</chunk_{i}>\\n\"\"\"\n\n    summary_prompt = f\"\"\"You are a AI content analysis expert, good at summarizing content. Please summarize a specific and detailed answer or report based on the previous queries and the retrieved document chunks.\n\n    Original Query: {query}\n    Related Chunks: \n    {mini_chunk_str}\n    \"\"\"\n    char_response = llm.chat([{\"role\": \"user\", \"content\": summary_prompt}])\n    final_answer = char_response.content\n    log.color_print(\"\\n==== FINAL ANSWER====\\n\")\n    log.color_print(final_answer)\n    return final_answer, retrieval_res\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/deepsearcher/online_query.py b/deepsearcher/online_query.py
--- a/deepsearcher/online_query.py	(revision 3a6c15f48e78fe541abad32b20a9366cbfb3b331)
+++ b/deepsearcher/online_query.py	(date 1740374588058)
@@ -5,6 +5,7 @@
     generate_gap_queries,
     generate_final_answer,
 )
+from deepsearcher.agent.chain_of_rag import ChainOfRAG
 from deepsearcher.agent.search_vdb import search_chunks_from_vectordb
 from deepsearcher.vector_db.base import deduplicate_results, RetrievalResult
 
@@ -39,7 +40,13 @@
 def retrieve(
     original_query: str, max_iter: int = 3
 ) -> Tuple[List[RetrievalResult], List[str], int]:
-    return asyncio.run(async_retrieve(original_query, max_iter))
+    # return asyncio.run(async_retrieve(original_query, max_iter))#todo
+    agent = ChainOfRAG(
+        llm=configuration.llm,
+        embedding_model=configuration.embedding_model,
+        vectordb=configuration.vector_db,
+    )
+    return agent.retrieve(original_query, max_iter=4)
 
 
 async def async_retrieve(
Index: examples/evaluation.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Some test dataset and evaluation method are ref from https://github.com/OSU-NLP-Group/HippoRAG/tree/main/data , many thanks\n\n################################################################################\n# Note: This evaluation script will cost a lot of LLM token usage, please make sure you have enough token budget.\n################################################################################\n\n\nimport json\nimport os\n\nfrom tqdm import tqdm\n\nfrom deepsearcher.configuration import Configuration, init_config\nfrom deepsearcher.offline_loading import load_from_local_files\nfrom deepsearcher.online_query import query, naive_rag_query, naive_retrieve, retrieve\n\ndataset_name = \"2wikimultihopqa\"  # a multi-hop QA dataset for comprehensive evaluation of reasoning steps\n\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\ncorpus_file = os.path.join(current_dir, f\"data/{dataset_name}_corpus.json\")\n\nconfig = Configuration()\n\nconfig.set_provider_config(\"file_loader\", \"JsonFileLoader\", {\"text_key\": \"text\"})\n## Replace with your provider settings\nconfig.set_provider_config(\"vector_db\", \"Milvus\", {\"uri\": ...})\nconfig.set_provider_config(\"llm\", \"OpenAI\", {\"model\": \"gpt-4o-mini\"})\n# config.set_provider_config(\"llm\", \"AzureOpenAI\", {\n#     \"model\": \"zilliz-gpt-4o-mini\",\n#     \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT_BAK\"),\n#     \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY_BAK\"),\n#     \"api_version\": \"2023-05-15\"\n# })\nconfig.set_provider_config(\n    \"embedding\", \"OpenAIEmbedding\", {\"model_name\": \"text-embedding-ada-002\"}\n)\ninit_config(config=config)\n\n# set chunk size to a large number to avoid chunking, because the dataset was chunked already.\nload_from_local_files(\n    corpus_file, force_new_collection=True, chunk_size=999999, chunk_overlap=0\n)\n\n\ndata_with_gt_file_path = os.path.join(current_dir, f\"data/{dataset_name}.json\")\ndata_with_gt = json.load(open(data_with_gt_file_path, \"r\"))\n\nk_list = [1, 2, 5, 10, 15, 20, 30, 40, 50, 80, 100]\ntotal_recall = {k: 0 for k in k_list}\n\n\n# There are 1000 samples in total,\n# for cost efficiency, we only evaluate the first 300 samples by default.\n# You can change the value of pre_num to evaluate more samples.\nPRE_NUM = 300\n\nif not PRE_NUM:\n    PRE_NUM = len(data_with_gt)\n\nfor sample_idx, sample in tqdm(\n    enumerate(data_with_gt), total=min(PRE_NUM, len(data_with_gt)), desc=\"Evaluation\"\n):  # for each sample\n    question = sample[\"question\"]\n\n    retry_num = 3\n    for i in range(retry_num):\n        try:\n            retrieved_results, _, _ = retrieve(question)\n            break\n        except SyntaxError as e:\n            print(\"Parse LLM's output failed, retry again...\")\n\n    # naive_retrieved_results = naive_retrieve(question)\n\n    retrieved_titles = [\n        retrieved_result.metadata[\"title\"] for retrieved_result in retrieved_results\n    ]\n    # naive_retrieved_titles = [retrieved_result.metadata[\"title\"] for retrieved_result in naive_retrieved_results]\n\n    # retrieved_titles = naive_retrieved_titles#todo\n\n    if dataset_name in [\"hotpotqa\", \"hotpotqa_train\"]:\n        gold_passages = [item for item in sample[\"supporting_facts\"]]\n        gold_items = set([item[0] for item in gold_passages])\n        retrieved_items = retrieved_titles\n    elif dataset_name in [\"2wikimultihopqa\"]:\n        gold_passages = [item for item in sample[\"supporting_facts\"]]\n        gold_items = set([item[0] for item in gold_passages])\n        retrieved_items = retrieved_titles\n    # elif dataset_name in ['musique']:\n    #     gold_passages = [item for item in sample['paragraphs'] if item['is_supporting']]\n    #     gold_items = set(\n    #         [item['title'] + '\\n' + item['paragraph_text'] for item in gold_passages])\n    #     retrieved_items = retrieved_passages\n    # else:\n    #     gold_passages = [item for item in sample['paragraphs'] if item['is_supporting']]\n    #     gold_items = set(\n    #         [item['title'] + '\\n' + item['text'] for item in gold_passages])\n    #     retrieved_items = retrieved_passages\n\n    # calculate metrics\n    recall = dict()\n    print(f\"idx: {sample_idx + 1} \", end=\"\")\n    for k in k_list:\n        recall[k] = round(\n            sum(1 for t in gold_items if t in retrieved_items[:k]) / len(gold_items), 4\n        )\n        total_recall[k] += recall[k]\n        print(f\"R@{k}: {total_recall[k] / (sample_idx + 1):.4f} \", end=\"\")\n    print()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/examples/evaluation.py b/examples/evaluation.py
--- a/examples/evaluation.py	(revision 3a6c15f48e78fe541abad32b20a9366cbfb3b331)
+++ b/examples/evaluation.py	(date 1740449900277)
@@ -3,83 +3,70 @@
 ################################################################################
 # Note: This evaluation script will cost a lot of LLM token usage, please make sure you have enough token budget.
 ################################################################################
-
-
+import ast
 import json
 import os
+import time
+import pandas as pd
+from typing import List, Tuple
 
 from tqdm import tqdm
 
 from deepsearcher.configuration import Configuration, init_config
 from deepsearcher.offline_loading import load_from_local_files
 from deepsearcher.online_query import query, naive_rag_query, naive_retrieve, retrieve
+from deepsearcher.vector_db import RetrievalResult
 
 dataset_name = "2wikimultihopqa"  # a multi-hop QA dataset for comprehensive evaluation of reasoning steps
 
 current_dir = os.path.dirname(os.path.abspath(__file__))
 
 corpus_file = os.path.join(current_dir, f"data/{dataset_name}_corpus.json")
+log_dir = os.path.join(current_dir, "..", "eval_logs")
 
-config = Configuration()
+config = Configuration(config_path=os.path.join(current_dir, "..", "config_zc_test.yaml")) #todo: replace with your own config file
 
-config.set_provider_config("file_loader", "JsonFileLoader", {"text_key": "text"})
-## Replace with your provider settings
-config.set_provider_config("vector_db", "Milvus", {"uri": ...})
-config.set_provider_config("llm", "OpenAI", {"model": "gpt-4o-mini"})
+# config.set_provider_config("llm", "OpenAI", {"model": "o1-mini"})
 # config.set_provider_config("llm", "AzureOpenAI", {
 #     "model": "zilliz-gpt-4o-mini",
 #     "azure_endpoint": os.getenv("AZURE_OPENAI_ENDPOINT_BAK"),
 #     "api_key": os.getenv("AZURE_OPENAI_API_KEY_BAK"),
 #     "api_version": "2023-05-15"
 # })
-config.set_provider_config(
-    "embedding", "OpenAIEmbedding", {"model_name": "text-embedding-ada-002"}
-)
+
 init_config(config=config)
 
-# set chunk size to a large number to avoid chunking, because the dataset was chunked already.
-load_from_local_files(
-    corpus_file, force_new_collection=True, chunk_size=999999, chunk_overlap=0
-)
-
-
-data_with_gt_file_path = os.path.join(current_dir, f"data/{dataset_name}.json")
-data_with_gt = json.load(open(data_with_gt_file_path, "r"))
-
-k_list = [1, 2, 5, 10, 15, 20, 30, 40, 50, 80, 100]
-total_recall = {k: 0 for k in k_list}
+k_list = [2, 5]
 
-
-# There are 1000 samples in total,
-# for cost efficiency, we only evaluate the first 300 samples by default.
-# You can change the value of pre_num to evaluate more samples.
-PRE_NUM = 300
-
-if not PRE_NUM:
-    PRE_NUM = len(data_with_gt)
-
-for sample_idx, sample in tqdm(
-    enumerate(data_with_gt), total=min(PRE_NUM, len(data_with_gt)), desc="Evaluation"
-):  # for each sample
-    question = sample["question"]
-
-    retry_num = 3
+def deepsearch_retrieve_titles(question: str, retry_num: int = 4, wait_time_list: List[int] = [4, 8, 16, 32, 64]) -> Tuple[List[str], bool]:
+    retrieved_results = []
     for i in range(retry_num):
         try:
             retrieved_results, _, _ = retrieve(question)
             break
-        except SyntaxError as e:
+        # except SyntaxError as e:
+        except Exception as e:
             print("Parse LLM's output failed, retry again...")
-
-    # naive_retrieved_results = naive_retrieve(question)
-
-    retrieved_titles = [
-        retrieved_result.metadata["title"] for retrieved_result in retrieved_results
-    ]
-    # naive_retrieved_titles = [retrieved_result.metadata["title"] for retrieved_result in naive_retrieved_results]
+            time.sleep(wait_time_list[i])
+    if retrieved_results:
+        retrieved_titles = [
+            retrieved_result.metadata["title"] for retrieved_result in retrieved_results
+        ]
+        fail = False
+    else:
+        print("Pipeline error, no retrieved results.")
+        retrieved_titles = []
+        fail = True
+    return retrieved_titles, fail
+
 
-    # retrieved_titles = naive_retrieved_titles#todo
+def naive_retrieve_titles(question: str) -> List[str]:
+    retrieved_results = naive_retrieve(question)
+    retrieved_titles = [retrieved_result.metadata["title"] for retrieved_result in retrieved_results]
+    return retrieved_titles
 
+
+def calcu_f1(sample, retrieved_titles, dataset_name) -> dict:
     if dataset_name in ["hotpotqa", "hotpotqa_train"]:
         gold_passages = [item for item in sample["supporting_facts"]]
         gold_items = set([item[0] for item in gold_passages])
@@ -88,6 +75,8 @@
         gold_passages = [item for item in sample["supporting_facts"]]
         gold_items = set([item[0] for item in gold_passages])
         retrieved_items = retrieved_titles
+    else:
+        raise NotImplementedError
     # elif dataset_name in ['musique']:
     #     gold_passages = [item for item in sample['paragraphs'] if item['is_supporting']]
     #     gold_items = set(
@@ -101,11 +90,112 @@
 
     # calculate metrics
     recall = dict()
-    print(f"idx: {sample_idx + 1} ", end="")
     for k in k_list:
         recall[k] = round(
             sum(1 for t in gold_items if t in retrieved_items[:k]) / len(gold_items), 4
         )
-        total_recall[k] += recall[k]
-        print(f"R@{k}: {total_recall[k] / (sample_idx + 1):.4f} ", end="")
-    print()
+    return recall
+
+def get_average_recall(total_recall: dict, sample_idx: int) -> dict:
+
+    average_recall = dict()
+    for k in k_list:
+        average_recall[k] = round(total_recall[k] / (sample_idx + 1), 4)
+    return average_recall
+
+
+def print_recall_line(recall: dict, pre_str = "", post_str = "\n"):
+    print(pre_str, end="")
+    for k in k_list:
+        print(f"R@{k}: {recall[k]:.4f} ", end="")
+    print(post_str, end="")
+
+# set chunk size to a large number to avoid chunking, because the dataset was chunked already.
+# load_from_local_files(
+#     corpus_file, force_new_collection=True, chunk_size=999999, chunk_overlap=0
+# )
+
+
+data_with_gt_file_path = os.path.join(current_dir, f"data/{dataset_name}.json")
+data_with_gt = json.load(open(data_with_gt_file_path, "r"))
+
+
+
+
+# There are 1000 samples in total,
+# for cost efficiency, we only evaluate the first 300 samples by default.
+# You can change the value of pre_num to evaluate more samples.
+PRE_NUM = 10
+
+if not PRE_NUM:
+    PRE_NUM = len(data_with_gt)
+
+pipeline_error_num = 0
+total_recall = {k: 0 for k in k_list}
+total_recall_naive = {k: 0 for k in k_list}
+end_ind = min(PRE_NUM, len(data_with_gt))
+
+
+# final_log_json_file = os.path.join(log_dir, "deepsearcher-openai_ada-o1_mini(2).json")
+# final_log_json_file = os.path.join(log_dir, "deepsearcher-openai_ada-gpt_4o_mini(2).json")
+# final_log_json_file = os.path.join(log_dir, "deepsearcher-openai_ada-gpt_4o_mini-corag.json")
+# final_log_json_file = os.path.join(log_dir, "deepsearcher-openai_ada-gpt_4o_mini-corag(2).json")
+# final_log_json_file = os.path.join(log_dir, "deepsearcher-openai_ada-o1_mini-corag.json")
+# final_log_json_file = os.path.join(log_dir, "deepsearcher-openai_ada-o1_mini-corag(2).json")
+eval_output_dir = os.path.join(log_dir, "eval_output")
+os.makedirs(eval_output_dir, exist_ok=True)
+
+csv_file_path = os.path.join(eval_output_dir, 'csv_output.csv')
+
+start_ind = 0
+existing_df = pd.DataFrame()
+if os.path.exists(csv_file_path):
+    existing_df = pd.read_csv(csv_file_path)
+    start_ind = len(existing_df)
+    print(f"Loading from {csv_file_path}, start_ind = {start_ind}")
+
+
+
+for sample_idx, sample in enumerate(data_with_gt[start_ind:end_ind]):
+    global_idx = sample_idx + start_ind
+    question = sample["question"]
+
+    # fail = False
+    retrieved_titles, fail = deepsearch_retrieve_titles(question)
+    retrieved_titles_naive = naive_retrieve_titles(question)
+
+    if fail:
+        pipeline_error_num += 1
+        print(f"Pipeline error, no retrieved results. Current pipeline_error_num = {pipeline_error_num}")
+
+    print(f"idx: {global_idx}: ")
+    recall = calcu_f1(sample, retrieved_titles, dataset_name)
+    recall_naive = calcu_f1(sample, retrieved_titles_naive, dataset_name)
+    # print("  Current recall: ")
+    # print_recall_line(recall, pre_str="    deepsearcher recall: ")
+    # print_recall_line(recall_naive, pre_str="           naive recall: ")
+    # print("  Average recall: ")
+    # print_recall_line(average_recall, pre_str="    deepsearcher recall: ")
+    # print_recall_line(average_recall_naive, pre_str="           naive recall: ")
+    current_result =[{
+        "idx": global_idx,
+        "question": question,
+        "recall": recall,
+        "recall_naive": recall_naive,
+        "gold_titles": [item[0] for item in sample["supporting_facts"]],
+        "retrieved_titles": retrieved_titles,
+        "retrieved_titles_naive": retrieved_titles_naive,
+    }]
+    current_df = pd.DataFrame(current_result)
+    existing_df = pd.concat([existing_df, current_df], ignore_index=True)
+    existing_df.to_csv(csv_file_path, index=False)
+    average_recall = dict()
+    average_recall_naive = dict()
+    for k in k_list:
+        average_recall[k] = sum([ast.literal_eval(d).get(k) if isinstance(d, str) else d.get(k) for d in existing_df['recall']]) / len(existing_df)
+        average_recall_naive[k] = sum([ast.literal_eval(d).get(k) if isinstance(d, str) else d.get(k) for d in existing_df['recall_naive']]) / len(existing_df)
+    print(average_recall)
+    print(average_recall_naive)
+
+
+print("Finish results to save.")
