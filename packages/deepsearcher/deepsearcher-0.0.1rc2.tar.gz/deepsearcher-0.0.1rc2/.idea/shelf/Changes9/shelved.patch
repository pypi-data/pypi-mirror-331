Index: deepsearcher/online_query.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import asyncio\nfrom typing import List, Tuple\n\n# from deepsearcher.configuration import vector_db, embedding_model, llm\nfrom deepsearcher import configuration\nfrom deepsearcher.agent import (\n    generate_final_answer,\n    generate_gap_queries,\n    generate_sub_queries,\n)\nfrom deepsearcher.agent.search_vdb import search_chunks_from_vectordb\nfrom deepsearcher.tools import log\nfrom deepsearcher.vector_db.base import RetrievalResult, deduplicate_results\n\n\n# Add a wrapper function to support synchronous calls\ndef query(original_query: str, max_iter: int = 3) -> Tuple[str, List[RetrievalResult], int]:\n    return asyncio.run(async_query(original_query, max_iter))\n\n\nasync def async_query(\n    original_query: str, max_iter: int = 3\n) -> Tuple[str, List[RetrievalResult], int]:\n    retrieval_res, all_sub_queries, retrieve_conseumed_token = await async_retrieve(\n        original_query, max_iter\n    )\n    ### GENERATE FINAL ANSWER ###\n    log.color_print(\"<think> Generating final answer... </think>\\n\")\n    final_answer, final_consumed_token = generate_final_answer(\n        original_query, all_sub_queries, retrieval_res\n    )\n    log.color_print(\"\\n==== FINAL ANSWER====\\n\")\n    log.color_print(final_answer)\n    return final_answer, retrieval_res, retrieve_conseumed_token + final_consumed_token\n\n\ndef retrieve(\n    original_query: str, max_iter: int = 3\n) -> Tuple[List[RetrievalResult], List[str], int]:\n    return asyncio.run(async_retrieve(original_query, max_iter))\n\n\nasync def async_retrieve(\n    original_query: str, max_iter: int = 3\n) -> Tuple[List[RetrievalResult], List[str], int]:\n    log.color_print(f\"<query> {original_query} </query>\\n\")\n    all_search_res = []\n    all_sub_queries = []\n    total_tokens = 0\n\n    ### SUB QUERIES ###\n    sub_queries, used_token = generate_sub_queries(original_query)\n    total_tokens += used_token\n    if not sub_queries:\n        log.color_print(\"No sub queries were generated by the LLM. Exiting.\")\n        return [], [], total_tokens\n    else:\n        log.color_print(\n            f\"<think> Break down the original query into new sub queries: {sub_queries}</think>\\n\"\n        )\n    all_sub_queries.extend(sub_queries)\n    sub_gap_queries = sub_queries\n\n    for iter in range(max_iter):\n        log.color_print(f\">> Iteration: {iter + 1}\\n\")\n        search_res_from_vectordb = []\n        search_res_from_internet = []  # TODO\n\n        # Create all search tasks\n        search_tasks = [\n            search_chunks_from_vectordb(query, sub_gap_queries) for query in sub_gap_queries\n        ]\n        # Execute all tasks in parallel and wait for results\n        search_results = await asyncio.gather(*search_tasks)\n        # Merge all results\n        for result in search_results:\n            search_res, consumed_token = result\n            total_tokens += consumed_token\n            search_res_from_vectordb.extend(search_res)\n\n        search_res_from_vectordb = deduplicate_results(search_res_from_vectordb)\n        # search_res_from_internet = deduplicate_results(search_res_from_internet)\n        all_search_res.extend(search_res_from_vectordb + search_res_from_internet)\n\n        ### REFLECTION & GET GAP QUERIES ###\n        log.color_print(\"<think> Reflecting on the search results... </think>\\n\")\n        sub_gap_queries, consumed_token = generate_gap_queries(\n            original_query, all_sub_queries, all_search_res\n        )\n        total_tokens += consumed_token\n        if not sub_gap_queries:\n            log.color_print(\"<think> No new search queries were generated. Exiting. </think>\\n\")\n            break\n        else:\n            log.color_print(\n                f\"<think> New search queries for next iteration: {sub_gap_queries} </think>\\n\"\n            )\n            all_sub_queries.extend(sub_gap_queries)\n\n    all_search_res = deduplicate_results(all_search_res)\n    return all_search_res, all_sub_queries, total_tokens\n\n\ndef naive_retrieve(query: str, collection: str = None, top_k=10) -> List[RetrievalResult]:\n    vector_db = configuration.vector_db\n    embedding_model = configuration.embedding_model\n\n    if not collection:\n        retrieval_res = []\n        collections = [col_info.collection_name for col_info in vector_db.list_collections()]\n        for collection in collections:\n            retrieval_res_col = vector_db.search_data(\n                collection=collection,\n                vector=embedding_model.embed_query(query),\n                top_k=top_k // len(collections),\n            )\n            retrieval_res.extend(retrieval_res_col)\n        retrieval_res = deduplicate_results(retrieval_res)\n    else:\n        retrieval_res = vector_db.search_data(\n            collection=collection,\n            vector=embedding_model.embed_query(query),\n            top_k=top_k,\n        )\n    return retrieval_res\n\n\ndef naive_rag_query(\n    query: str, collection: str = None, top_k=10\n) -> Tuple[str, List[RetrievalResult]]:\n    llm = configuration.llm\n    retrieval_res = naive_retrieve(query, collection, top_k)\n\n    chunk_texts = []\n    for chunk in retrieval_res:\n        if \"wider_text\" in chunk.metadata:\n            chunk_texts.append(chunk.metadata[\"wider_text\"])\n        else:\n            chunk_texts.append(chunk.text)\n    mini_chunk_str = \"\"\n    for i, chunk in enumerate(chunk_texts):\n        mini_chunk_str += f\"\"\"<chunk_{i}>\\n{chunk}\\n</chunk_{i}>\\n\"\"\"\n\n    summary_prompt = f\"\"\"You are a AI content analysis expert, good at summarizing content. Please summarize a specific and detailed answer or report based on the previous queries and the retrieved document chunks.\n\n    Original Query: {query}\n    Related Chunks: \n    {mini_chunk_str}\n    \"\"\"\n    char_response = llm.chat([{\"role\": \"user\", \"content\": summary_prompt}])\n    final_answer = char_response.content\n    log.color_print(\"\\n==== FINAL ANSWER====\\n\")\n    log.color_print(final_answer)\n    return final_answer, retrieval_res\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/deepsearcher/online_query.py b/deepsearcher/online_query.py
--- a/deepsearcher/online_query.py	(revision 8f6c15515b06735daf03c5385be2b8fd0e40a889)
+++ b/deepsearcher/online_query.py	(date 1740476675204)
@@ -8,6 +8,7 @@
     generate_gap_queries,
     generate_sub_queries,
 )
+from deepsearcher.agent.chain_of_rag import ChainOfRAG
 from deepsearcher.agent.search_vdb import search_chunks_from_vectordb
 from deepsearcher.tools import log
 from deepsearcher.vector_db.base import RetrievalResult, deduplicate_results
@@ -37,7 +38,13 @@
 def retrieve(
     original_query: str, max_iter: int = 3
 ) -> Tuple[List[RetrievalResult], List[str], int]:
-    return asyncio.run(async_retrieve(original_query, max_iter))
+    # return asyncio.run(async_retrieve(original_query, max_iter))#todo
+    agent = ChainOfRAG(
+        llm=configuration.llm,
+        embedding_model=configuration.embedding_model,
+        vectordb=configuration.vector_db,
+    )
+    return agent.retrieve(original_query, max_iter=4)
 
 
 async def async_retrieve(
