Index: deepsearcher/online_query.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import asyncio\nfrom typing import List, Tuple\n\n# from deepsearcher.configuration import vector_db, embedding_model, llm\nfrom deepsearcher import configuration\nfrom deepsearcher.agent import (\n    generate_final_answer,\n    generate_gap_queries,\n    generate_sub_queries,\n)\nfrom deepsearcher.agent.search_vdb import search_chunks_from_vectordb\nfrom deepsearcher.tools import log\nfrom deepsearcher.vector_db.base import RetrievalResult, deduplicate_results\n\n\n# Add a wrapper function to support synchronous calls\ndef query(original_query: str, max_iter: int = 3) -> Tuple[str, List[RetrievalResult], int]:\n    return asyncio.run(async_query(original_query, max_iter))\n\n\nasync def async_query(\n    original_query: str, max_iter: int = 3\n) -> Tuple[str, List[RetrievalResult], int]:\n    retrieval_res, all_sub_queries, retrieve_conseumed_token = await async_retrieve(\n        original_query, max_iter\n    )\n    ### GENERATE FINAL ANSWER ###\n    log.color_print(\"<think> Generating final answer... </think>\\n\")\n    final_answer, final_consumed_token = generate_final_answer(\n        original_query, all_sub_queries, retrieval_res\n    )\n    log.color_print(\"\\n==== FINAL ANSWER====\\n\")\n    log.color_print(final_answer)\n    return final_answer, retrieval_res, retrieve_conseumed_token + final_consumed_token\n\n\ndef retrieve(\n    original_query: str, max_iter: int = 3\n) -> Tuple[List[RetrievalResult], List[str], int]:\n    return asyncio.run(async_retrieve(original_query, max_iter))\n\n\nasync def async_retrieve(\n    original_query: str, max_iter: int = 3\n) -> Tuple[List[RetrievalResult], List[str], int]:\n    log.color_print(f\"<query> {original_query} </query>\\n\")\n    all_search_res = []\n    all_sub_queries = []\n    total_tokens = 0\n\n    ### SUB QUERIES ###\n    sub_queries, used_token = generate_sub_queries(original_query)\n    total_tokens += used_token\n    if not sub_queries:\n        log.color_print(\"No sub queries were generated by the LLM. Exiting.\")\n        return [], [], total_tokens\n    else:\n        log.color_print(\n            f\"<think> Break down the original query into new sub queries: {sub_queries}</think>\\n\"\n        )\n    all_sub_queries.extend(sub_queries)\n    sub_gap_queries = sub_queries\n\n    for iter in range(max_iter):\n        log.color_print(f\">> Iteration: {iter + 1}\\n\")\n        search_res_from_vectordb = []\n        search_res_from_internet = []  # TODO\n\n        # Create all search tasks\n        search_tasks = [\n            search_chunks_from_vectordb(query, sub_gap_queries) for query in sub_gap_queries\n        ]\n        # Execute all tasks in parallel and wait for results\n        search_results = await asyncio.gather(*search_tasks)\n        # Merge all results\n        for result in search_results:\n            search_res, consumed_token = result\n            total_tokens += consumed_token\n            search_res_from_vectordb.extend(search_res)\n\n        search_res_from_vectordb = deduplicate_results(search_res_from_vectordb)\n        # search_res_from_internet = deduplicate_results(search_res_from_internet)\n        all_search_res.extend(search_res_from_vectordb + search_res_from_internet)\n\n        ### REFLECTION & GET GAP QUERIES ###\n        log.color_print(\"<think> Reflecting on the search results... </think>\\n\")\n        sub_gap_queries, consumed_token = generate_gap_queries(\n            original_query, all_sub_queries, all_search_res\n        )\n        total_tokens += consumed_token\n        if not sub_gap_queries:\n            log.color_print(\"<think> No new search queries were generated. Exiting. </think>\\n\")\n            break\n        else:\n            log.color_print(\n                f\"<think> New search queries for next iteration: {sub_gap_queries} </think>\\n\"\n            )\n            all_sub_queries.extend(sub_gap_queries)\n\n    all_search_res = deduplicate_results(all_search_res)\n    return all_search_res, all_sub_queries, total_tokens\n\n\ndef naive_retrieve(query: str, collection: str = None, top_k=10) -> List[RetrievalResult]:\n    vector_db = configuration.vector_db\n    embedding_model = configuration.embedding_model\n\n    if not collection:\n        retrieval_res = []\n        collections = [col_info.collection_name for col_info in vector_db.list_collections()]\n        for collection in collections:\n            retrieval_res_col = vector_db.search_data(\n                collection=collection,\n                vector=embedding_model.embed_query(query),\n                top_k=top_k // len(collections),\n            )\n            retrieval_res.extend(retrieval_res_col)\n        retrieval_res = deduplicate_results(retrieval_res)\n    else:\n        retrieval_res = vector_db.search_data(\n            collection=collection,\n            vector=embedding_model.embed_query(query),\n            top_k=top_k,\n        )\n    return retrieval_res\n\n\ndef naive_rag_query(\n    query: str, collection: str = None, top_k=10\n) -> Tuple[str, List[RetrievalResult]]:\n    llm = configuration.llm\n    retrieval_res = naive_retrieve(query, collection, top_k)\n\n    chunk_texts = []\n    for chunk in retrieval_res:\n        if \"wider_text\" in chunk.metadata:\n            chunk_texts.append(chunk.metadata[\"wider_text\"])\n        else:\n            chunk_texts.append(chunk.text)\n    mini_chunk_str = \"\"\n    for i, chunk in enumerate(chunk_texts):\n        mini_chunk_str += f\"\"\"<chunk_{i}>\\n{chunk}\\n</chunk_{i}>\\n\"\"\"\n\n    summary_prompt = f\"\"\"You are a AI content analysis expert, good at summarizing content. Please summarize a specific and detailed answer or report based on the previous queries and the retrieved document chunks.\n\n    Original Query: {query}\n    Related Chunks: \n    {mini_chunk_str}\n    \"\"\"\n    char_response = llm.chat([{\"role\": \"user\", \"content\": summary_prompt}])\n    final_answer = char_response.content\n    log.color_print(\"\\n==== FINAL ANSWER====\\n\")\n    log.color_print(final_answer)\n    return final_answer, retrieval_res\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/deepsearcher/online_query.py b/deepsearcher/online_query.py
--- a/deepsearcher/online_query.py	(revision b1755739e4b5848a6f26bbfac672e0c7caaa99a2)
+++ b/deepsearcher/online_query.py	(date 1740454941483)
@@ -8,6 +8,7 @@
     generate_gap_queries,
     generate_sub_queries,
 )
+from deepsearcher.agent.chain_of_rag import ChainOfRAG
 from deepsearcher.agent.search_vdb import search_chunks_from_vectordb
 from deepsearcher.tools import log
 from deepsearcher.vector_db.base import RetrievalResult, deduplicate_results
@@ -37,7 +38,13 @@
 def retrieve(
     original_query: str, max_iter: int = 3
 ) -> Tuple[List[RetrievalResult], List[str], int]:
-    return asyncio.run(async_retrieve(original_query, max_iter))
+    # return asyncio.run(async_retrieve(original_query, max_iter))#todo
+    agent = ChainOfRAG(
+        llm=configuration.llm,
+        embedding_model=configuration.embedding_model,
+        vectordb=configuration.vector_db,
+    )
+    return agent.retrieve(original_query, max_iter=4)
 
 
 async def async_retrieve(
Index: examples/evaluation.py
===================================================================
diff --git a/examples/evaluation.py b/examples/evaluation.py
deleted file mode 100644
--- a/examples/evaluation.py	(revision b1755739e4b5848a6f26bbfac672e0c7caaa99a2)
+++ /dev/null	(revision b1755739e4b5848a6f26bbfac672e0c7caaa99a2)
@@ -1,111 +0,0 @@
-# Some test dataset and evaluation method are ref from https://github.com/OSU-NLP-Group/HippoRAG/tree/main/data , many thanks
-
-################################################################################
-# Note: This evaluation script will cost a lot of LLM token usage, please make sure you have enough token budget.
-################################################################################
-
-
-import json
-import os
-
-from tqdm import tqdm
-
-from deepsearcher.configuration import Configuration, init_config
-from deepsearcher.offline_loading import load_from_local_files
-from deepsearcher.online_query import query, naive_rag_query, naive_retrieve, retrieve
-
-dataset_name = "2wikimultihopqa"  # a multi-hop QA dataset for comprehensive evaluation of reasoning steps
-
-current_dir = os.path.dirname(os.path.abspath(__file__))
-
-corpus_file = os.path.join(current_dir, f"data/{dataset_name}_corpus.json")
-
-config = Configuration()
-
-config.set_provider_config("file_loader", "JsonFileLoader", {"text_key": "text"})
-## Replace with your provider settings
-config.set_provider_config("vector_db", "Milvus", {"uri": ...})
-config.set_provider_config("llm", "OpenAI", {"model": "gpt-4o-mini"})
-# config.set_provider_config("llm", "AzureOpenAI", {
-#     "model": "zilliz-gpt-4o-mini",
-#     "azure_endpoint": os.getenv("AZURE_OPENAI_ENDPOINT_BAK"),
-#     "api_key": os.getenv("AZURE_OPENAI_API_KEY_BAK"),
-#     "api_version": "2023-05-15"
-# })
-config.set_provider_config(
-    "embedding", "OpenAIEmbedding", {"model_name": "text-embedding-ada-002"}
-)
-init_config(config=config)
-
-# set chunk size to a large number to avoid chunking, because the dataset was chunked already.
-load_from_local_files(
-    corpus_file, force_new_collection=True, chunk_size=999999, chunk_overlap=0
-)
-
-
-data_with_gt_file_path = os.path.join(current_dir, f"data/{dataset_name}.json")
-data_with_gt = json.load(open(data_with_gt_file_path, "r"))
-
-k_list = [1, 2, 5, 10, 15, 20, 30, 40, 50, 80, 100]
-total_recall = {k: 0 for k in k_list}
-
-
-# There are 1000 samples in total,
-# for cost efficiency, we only evaluate the first 300 samples by default.
-# You can change the value of pre_num to evaluate more samples.
-PRE_NUM = 300
-
-if not PRE_NUM:
-    PRE_NUM = len(data_with_gt)
-
-for sample_idx, sample in tqdm(
-    enumerate(data_with_gt), total=min(PRE_NUM, len(data_with_gt)), desc="Evaluation"
-):  # for each sample
-    question = sample["question"]
-
-    retry_num = 3
-    for i in range(retry_num):
-        try:
-            retrieved_results, _, _ = retrieve(question)
-            break
-        except SyntaxError as e:
-            print("Parse LLM's output failed, retry again...")
-
-    # naive_retrieved_results = naive_retrieve(question)
-
-    retrieved_titles = [
-        retrieved_result.metadata["title"] for retrieved_result in retrieved_results
-    ]
-    # naive_retrieved_titles = [retrieved_result.metadata["title"] for retrieved_result in naive_retrieved_results]
-
-    # retrieved_titles = naive_retrieved_titles#todo
-
-    if dataset_name in ["hotpotqa", "hotpotqa_train"]:
-        gold_passages = [item for item in sample["supporting_facts"]]
-        gold_items = set([item[0] for item in gold_passages])
-        retrieved_items = retrieved_titles
-    elif dataset_name in ["2wikimultihopqa"]:
-        gold_passages = [item for item in sample["supporting_facts"]]
-        gold_items = set([item[0] for item in gold_passages])
-        retrieved_items = retrieved_titles
-    # elif dataset_name in ['musique']:
-    #     gold_passages = [item for item in sample['paragraphs'] if item['is_supporting']]
-    #     gold_items = set(
-    #         [item['title'] + '\n' + item['paragraph_text'] for item in gold_passages])
-    #     retrieved_items = retrieved_passages
-    # else:
-    #     gold_passages = [item for item in sample['paragraphs'] if item['is_supporting']]
-    #     gold_items = set(
-    #         [item['title'] + '\n' + item['text'] for item in gold_passages])
-    #     retrieved_items = retrieved_passages
-
-    # calculate metrics
-    recall = dict()
-    print(f"idx: {sample_idx + 1} ", end="")
-    for k in k_list:
-        recall[k] = round(
-            sum(1 for t in gold_items if t in retrieved_items[:k]) / len(gold_items), 4
-        )
-        total_recall[k] += recall[k]
-        print(f"R@{k}: {total_recall[k] / (sample_idx + 1):.4f} ", end="")
-    print()
Index: deepsearcher/agent/chain_of_rag.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/deepsearcher/agent/chain_of_rag.py b/deepsearcher/agent/chain_of_rag.py
new file mode 100644
--- /dev/null	(date 1740470573753)
+++ b/deepsearcher/agent/chain_of_rag.py	(date 1740470573753)
@@ -0,0 +1,166 @@
+from typing import List, Tuple
+
+from deepsearcher.embedding.base import BaseEmbedding
+from deepsearcher.llm.base import BaseLLM
+from deepsearcher.vector_db import RetrievalResult
+from deepsearcher.vector_db.base import BaseVectorDB, deduplicate_results
+
+FOLLOWUP_QUERY_PROMPT = """You are using a search tool to answer the main query by iteratively searching the database. Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up questions only as the search tool may not understand complex questions.
+
+## Previous intermediate queries and answers
+{intermediate_context}
+
+## Main query to answer
+{query}
+
+Respond with a simple follow-up question that will help answer the main query, do not explain yourself or output anything else.
+"""
+
+INTERMEDIATE_ANSWER_PROMPT = """Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond “No relevant information found” if the documents do not contain useful information.
+
+## Documents
+{retrieved_documents}
+
+## Query
+{sub_query}
+
+Respond with a concise answer only, do not explain yourself or output anything else.
+"""
+
+FINAL_ANSWER_PROMPT = """Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate.
+
+## Documents
+{retrieved_documents}
+
+## Intermediate queries and answers
+{intermediate_context}
+
+## Main query
+{query}
+
+Respond with an appropriate answer only, do not explain yourself or output anything else.
+"""
+
+REFLECTION_PROMPT = """Given the following intermediate queries and answers, judge whether you have enough information to answer the main query. If you believe you have enough information, respond with “Yes”, otherwise respond with “No”.
+
+## Intermediate queries and answers
+{intermediate_context}
+
+## Main query
+{query}
+
+Respond with “Yes” or “No” only, do not explain yourself or output anything else.
+"""
+
+GET_SUPPORTED_DOCS_PROMPT = """Given the following documents, select the ones that are support the Q-A pair.
+
+## Documents
+{retrieved_documents}
+
+## Q-A Pair
+### Question
+{query}
+### Answer
+{answer}
+
+Respond with a python list of indices of the selected documents.
+"""
+
+
+class ChainOfRAG:  # todo subclass base agent
+    def __init__(
+        self,
+        llm: BaseLLM,
+        embedding_model: BaseEmbedding,
+        vectordb: BaseVectorDB,
+        **kwargs,
+    ):
+        self.llm = llm
+        self.embedding_model = embedding_model
+        self.vectordb = vectordb
+        self.intermediate_context = []
+
+    def retrieve(
+        self, original_query: str, max_iter: int = 4
+    ) -> Tuple[List[RetrievalResult], List[str], int]:
+        all_retrieved_results = []
+        for iter in range(max_iter):
+            chat_response = self.llm.chat(
+                [
+                    {
+                        "role": "user",
+                        "content": FOLLOWUP_QUERY_PROMPT.format(
+                            query=original_query,
+                            intermediate_context="\n".join(self.intermediate_context),
+                        ),
+                    }
+                ]
+            )
+            followup_query = chat_response.content
+
+            query_vector = self.embedding_model.embed_query(followup_query)
+            retrieved_results = self.vectordb.search_data(collection=None, vector=query_vector)
+
+            chat_response = self.llm.chat(
+                [
+                    {
+                        "role": "user",
+                        "content": INTERMEDIATE_ANSWER_PROMPT.format(
+                            retrieved_documents=self.format_retrieved_results(retrieved_results),
+                            sub_query=followup_query,
+                        ),
+                    }
+                ]
+            )
+            intermediate_answer = chat_response.content
+
+            if "No relevant information found" not in intermediate_answer:
+                chat_response = self.llm.chat(
+                    [
+                        {
+                            "role": "user",
+                            "content": GET_SUPPORTED_DOCS_PROMPT.format(
+                                retrieved_documents=self.format_retrieved_results(
+                                    retrieved_results
+                                ),
+                                query=followup_query,
+                                answer=intermediate_answer,
+                            ),
+                        }
+                    ]
+                )
+                supported_doc_indices = self.llm.literal_eval(chat_response.content)
+                # get retrieved_results from retrived_results using supported_doc_indices
+                supported_retrieved_results = [retrieved_results[i] for i in supported_doc_indices]
+                all_retrieved_results.extend(supported_retrieved_results)
+
+            self.intermediate_context.append(
+                f"Intermediate query{len(self.intermediate_context) + 1}: {followup_query}\nIntermediate answer{len(self.intermediate_context) + 1}: {intermediate_answer}"
+            )
+        all_retrieved_results = deduplicate_results(all_retrieved_results)
+        return all_retrieved_results, [], 0  # todo
+
+    def query(
+        self, original_query: str, max_iter: int = 3
+    ) -> Tuple[str, List[RetrievalResult], int]:
+        all_retrieved_results, _, _ = self.retrieve(original_query, max_iter)
+        chat_response = self.llm.chat(
+            [
+                {
+                    "role": "user",
+                    "content": FINAL_ANSWER_PROMPT.format(
+                        retrieved_documents=self.format_retrieved_results(all_retrieved_results),
+                        intermediate_context="\n".join(self.intermediate_context),
+                        query=original_query,
+                    ),
+                }
+            ]
+        )
+        final_answer = chat_response.content
+        return final_answer, all_retrieved_results, 0  # todo
+
+    def format_retrieved_results(self, retrieved_results: List[RetrievalResult]) -> str:
+        formatted_documents = []
+        for i, result in enumerate(retrieved_results):
+            formatted_documents.append(f"<Document {i}>\n{result.text}\n<\Document {i}>")
+        return "\n".join(formatted_documents)
Index: evaluation/eval_config.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/evaluation/eval_config.yaml b/evaluation/eval_config.yaml
new file mode 100644
--- /dev/null	(date 1740463858135)
+++ b/evaluation/eval_config.yaml	(date 1740463858135)
@@ -0,0 +1,97 @@
+provide_settings:
+  llm:
+    provider: "OpenAI"
+    config:
+      model: "gpt-4o-mini"  # "gpt-o1-mini"
+#      api_key: "sk-xxxx"  # Uncomment to override the `OPENAI_API_KEY` set in the environment variable
+#      base_url: ""
+
+#    provider: "DeepSeek"
+#    config:
+#      model: "deepseek-chat"  # "deepseek-reasoner"
+##      api_key: "sk-xxxx"  # Uncomment to override the `DEEPSEEK_API_KEY` set in the environment variable
+##      base_url: ""
+
+#    provider: "SiliconFlow"
+#    config:
+#      model: "deepseek-ai/DeepSeek-V3"
+##      api_key: "xxxx"  # Uncomment to override the `SILICONFLOW_API_KEY` set in the environment variable
+##      base_url: ""
+
+#    provider: "TogetherAI"
+#    config:
+#      model: "deepseek-ai/DeepSeek-V3"
+##      api_key: "xxxx"  # Uncomment to override the `TOGETHER_API_KEY` set in the environment variable
+
+#    provider: "AzureOpenAI"
+#    config:
+#      model: ""
+#      api_version: ""
+##      azure_endpoint: "xxxx"  # Uncomment to override the `AZURE_OPENAI_ENDPOINT` set in the environment variable
+##      api_key: "xxxx"  # Uncomment to override the `AZURE_OPENAI_KEY` set in the environment variable
+
+  embedding:
+    provider: "OpenAIEmbedding"
+    config:
+      model: "text-embedding-ada-002"
+#      api_key: ""  # Uncomment to override the `OPENAI_API_KEY` set in the environment variable
+
+
+#    provider: "MilvusEmbedding"
+#    config:
+#      model: "default"
+
+#    provider: "VoyageEmbedding"
+#    config:
+#      model: "voyage-3"
+##      api_key: ""  # Uncomment to override the `VOYAGE_API_KEY` set in the environment variable
+
+#    provider: "BedrockEmbedding"
+#    config:
+#      model: "amazon.titan-embed-text-v2:0"
+##      aws_access_key_id: ""  # Uncomment to override the `AWS_ACCESS_KEY_ID` set in the environment variable
+##      aws_secret_access_key: ""  # Uncomment to override the `AWS_SECRET_ACCESS_KEY` set in the environment variable
+    
+#    provider: "SiliconflowEmbedding"
+#    config:
+#      model: "BAAI/bge-m3"
+# .    api_key: ""   # Uncomment to override the `SILICONFLOW_API_KEY` set in the environment variable   
+
+  file_loader:
+#    provider: "PDFLoader"
+#    config: {}
+
+    provider: "JsonFileLoader"
+    config:
+      text_key: "text"
+
+#    provider: "TextLoader"
+#    config: {}
+
+#    provider: "UnstructuredLoader"
+#    config: {}
+
+  web_crawler:
+    provider: "FireCrawlCrawler"
+    config: {}
+
+#    provider: "Crawl4AICrawler"
+#    config: {}
+
+#    provider: "JinaCrawler"
+#    config: {}
+
+  vector_db:
+    provider: "Milvus"
+    config:
+      default_collection: "deepsearcher"
+      uri: "./milvus.db"
+      token: "root:Milvus"
+      db: "default"
+
+query_settings:
+  max_iter: 3
+
+load_settings:
+  chunk_size: 1500
+  chunk_overlap: 100
Index: evaluation/evaluate.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/evaluation/evaluate.py b/evaluation/evaluate.py
new file mode 100644
--- /dev/null	(date 1740473098306)
+++ b/evaluation/evaluate.py	(date 1740473098306)
@@ -0,0 +1,226 @@
+# Some test dataset and evaluation method are ref from https://github.com/OSU-NLP-Group/HippoRAG/tree/main/data , many thanks
+
+################################################################################
+# Note: This evaluation script will cost a lot of LLM token usage, please make sure you have enough token budget.
+################################################################################
+import argparse
+import ast
+import json
+import logging
+import os
+import time
+import warnings
+from typing import List, Tuple
+
+import pandas as pd
+
+from deepsearcher.configuration import Configuration, init_config
+from deepsearcher.offline_loading import load_from_local_files
+from deepsearcher.online_query import naive_retrieve, retrieve
+
+httpx_logger = logging.getLogger("httpx")  # disable openai's logger output
+httpx_logger.setLevel(logging.WARNING)
+
+
+warnings.simplefilter(action="ignore", category=FutureWarning)  # disable warning output
+
+
+current_dir = os.path.dirname(os.path.abspath(__file__))
+
+k_list = [2, 5]
+
+
+def _deepsearch_retrieve_titles(
+    question: str, retry_num: int = 4, base_wait_time: int = 4
+) -> Tuple[List[str], bool]:
+    retrieved_results = []
+    for i in range(retry_num):
+        try:
+            retrieved_results, _, _ = retrieve(question)
+            break
+        except Exception:
+            wait_time = base_wait_time * (2**i)
+            print(f"Parse LLM's output failed, retry again after {wait_time} seconds...")
+            time.sleep(wait_time)
+    if retrieved_results:
+        retrieved_titles = [
+            retrieved_result.metadata["title"] for retrieved_result in retrieved_results
+        ]
+        fail = False
+    else:
+        print("Pipeline error, no retrieved results.")
+        retrieved_titles = []
+        fail = True
+    return retrieved_titles, fail
+
+
+def _naive_retrieve_titles(question: str) -> List[str]:
+    retrieved_results = naive_retrieve(question)
+    retrieved_titles = [
+        retrieved_result.metadata["title"] for retrieved_result in retrieved_results
+    ]
+    return retrieved_titles
+
+
+def _calcu_recall(sample, retrieved_titles, dataset) -> dict:
+    if dataset in ["2wikimultihopqa"]:
+        gold_passages = [item for item in sample["supporting_facts"]]
+        gold_items = set([item[0] for item in gold_passages])
+        retrieved_items = retrieved_titles
+    else:
+        raise NotImplementedError
+
+    recall = dict()
+    for k in k_list:
+        recall[k] = round(
+            sum(1 for t in gold_items if t in retrieved_items[:k]) / len(gold_items), 4
+        )
+    return recall
+
+
+def _print_recall_line(recall: dict, pre_str="", post_str="\n"):
+    print(pre_str, end="")
+    for k in k_list:
+        print(f"R@{k}: {recall[k]:.3f} ", end="")
+    print(post_str, end="")
+
+
+def evaluate(
+    dataset: str,
+    output_root: str,
+    pre_num: int = 10,
+    skip_load=False,
+    flag: str = "result",
+):
+    corpus_file = os.path.join(current_dir, f"../examples/data/{dataset}_corpus.json")
+    if not skip_load:
+        # set chunk size to a large number to avoid chunking, because the dataset was chunked already.
+        load_from_local_files(
+            corpus_file, force_new_collection=True, chunk_size=999999, chunk_overlap=0
+        )
+
+    eval_output_subdir = os.path.join(output_root, flag)
+    os.makedirs(eval_output_subdir, exist_ok=True)
+    csv_file_path = os.path.join(eval_output_subdir, "details.csv")
+
+    data_with_gt_file_path = os.path.join(current_dir, f"../examples/data/{dataset}.json")
+    data_with_gt = json.load(open(data_with_gt_file_path, "r"))
+
+    if not pre_num:
+        pre_num = len(data_with_gt)
+
+    pipeline_error_num = 0
+    end_ind = min(pre_num, len(data_with_gt))
+
+    start_ind = 0
+    existing_df = pd.DataFrame()
+    if os.path.exists(csv_file_path):
+        existing_df = pd.read_csv(csv_file_path)
+        start_ind = len(existing_df)
+        print(f"Loading results from {csv_file_path}, start_index = {start_ind}")
+
+    for sample_idx, sample in enumerate(data_with_gt[start_ind:end_ind]):
+        global_idx = sample_idx + start_ind
+        question = sample["question"]
+
+        retrieved_titles, fail = _deepsearch_retrieve_titles(question)
+        retrieved_titles_naive = _naive_retrieve_titles(question)
+
+        if fail:
+            pipeline_error_num += 1
+            print(
+                f"Pipeline error, no retrieved results. Current pipeline_error_num = {pipeline_error_num}"
+            )
+
+        print(f"idx: {global_idx}: ")
+        recall = _calcu_recall(sample, retrieved_titles, dataset)
+        recall_naive = _calcu_recall(sample, retrieved_titles_naive, dataset)
+        current_result = [
+            {
+                "idx": global_idx,
+                "question": question,
+                "recall": recall,
+                "recall_naive": recall_naive,
+                "gold_titles": [item[0] for item in sample["supporting_facts"]],
+                "retrieved_titles": retrieved_titles,
+                "retrieved_titles_naive": retrieved_titles_naive,
+            }
+        ]
+        current_df = pd.DataFrame(current_result)
+        existing_df = pd.concat([existing_df, current_df], ignore_index=True)
+        existing_df.to_csv(csv_file_path, index=False)
+        average_recall = dict()
+        average_recall_naive = dict()
+        for k in k_list:
+            average_recall[k] = sum(
+                [
+                    ast.literal_eval(d).get(k) if isinstance(d, str) else d.get(k)
+                    for d in existing_df["recall"]
+                ]
+            ) / len(existing_df)
+            average_recall_naive[k] = sum(
+                [
+                    ast.literal_eval(d).get(k) if isinstance(d, str) else d.get(k)
+                    for d in existing_df["recall_naive"]
+                ]
+            ) / len(existing_df)
+        _print_recall_line(average_recall, pre_str="Average recall of DeepSearcher: ")
+        _print_recall_line(average_recall_naive, pre_str="Average recall of naive RAG   : ")
+        print("")
+    print("Finish results to save.")
+
+
+def main_eval():
+    parser = argparse.ArgumentParser(prog="evaluation", description="Deep Searcher evaluation.")
+    parser.add_argument(
+        "--dataset",
+        type=str,
+        default="2wikimultihopqa",
+        help="Dataset name, default is `2wikimultihopqa`. More datasets will be supported in the future.",
+    )
+    parser.add_argument(
+        "--config_yaml",
+        type=str,
+        default="./eval_config.yaml",
+        help="Configuration yaml file path, default is `./eval_config.yaml`",
+    )
+    parser.add_argument(
+        "--pre_num",
+        type=int,
+        default=30,
+        help="Number of samples to evaluate, default is 30",
+    )
+    parser.add_argument(
+        "--output_dir",
+        type=str,
+        default="./eval_output",
+        help="Output root directory, default is `./eval_output`",
+    )
+    parser.add_argument(
+        "--skip_load",
+        action="store_true",
+        help="Whether to skip loading the dataset. Default it don't skip loading. If you want to skip loading, please set this flag.",
+    )
+    parser.add_argument(
+        "--flag",
+        type=str,
+        default="result",
+        help="Flag for evaluation, default is `result`",
+    )
+
+    args = parser.parse_args()
+
+    config = Configuration(config_path=args.config_yaml)
+    init_config(config=config)
+
+    evaluate(
+        dataset=args.dataset,
+        output_root=args.output_dir,
+        pre_num=args.pre_num,
+        skip_load=args.skip_load,
+        flag=args.flag,
+    )
+
+
+if __name__ == "__main__":
+    main_eval()
Index: evaluation/README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/evaluation/README.md b/evaluation/README.md
new file mode 100644
--- /dev/null	(date 1740473653938)
+++ b/evaluation/README.md	(date 1740473653938)
@@ -0,0 +1,29 @@
+# Evaluation of DeepSearcher
+## Introduction
+DeepSearcher is very good at answering complex queries. In this evaluation introduction, we provide some scripts to evaluate the performance of DeepSearcher vs. naive RAG.
+
+The evaluation is based on the Recall metric:
+
+> Recall@K: The percentage of relevant documents that are retrieved among the top K documents returned by the search engine.
+
+Currently, we support the multi-hop question answering dataset of [2WikiMultiHopQA](https://paperswithcode.com/dataset/2wikimultihopqa). More dataset will be added in the future.
+
+## Evaluation Script
+The main evaluation script is `evaluate.py`. 
+
+Your can provide a config file, say `eval_config.yaml`, to specify the LLM, embedding model, and other provider and parameters.
+```shell
+python evaluate.py \
+--dataset 2wikimultihopqa \
+--config_yaml ./eval_config.yaml \
+--pre_num 5 \
+--output_dir ./eval_output
+```
+`pre_num` is the number of samples to evaluate, the more samples, the more accurate the results will be, but it will consume more time and your LLM api token usage.
+
+After you have loaded the dataset into vectorDB in the first run, if you want to skip loading dataset again, you can set the flag `--skip_load` in the command line.
+
+For more arguments details, you can run
+```shell
+python evaluate.py --help
+```
