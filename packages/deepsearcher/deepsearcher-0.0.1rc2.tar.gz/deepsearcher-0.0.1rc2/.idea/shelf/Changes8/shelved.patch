Index: deepsearcher/agent/chain_of_rag.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/deepsearcher/agent/chain_of_rag.py b/deepsearcher/agent/chain_of_rag.py
new file mode 100644
--- /dev/null	(date 1740476675211)
+++ b/deepsearcher/agent/chain_of_rag.py	(date 1740476675211)
@@ -0,0 +1,166 @@
+from typing import List, Tuple
+
+from deepsearcher.embedding.base import BaseEmbedding
+from deepsearcher.llm.base import BaseLLM
+from deepsearcher.vector_db import RetrievalResult
+from deepsearcher.vector_db.base import BaseVectorDB, deduplicate_results
+
+FOLLOWUP_QUERY_PROMPT = """You are using a search tool to answer the main query by iteratively searching the database. Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up questions only as the search tool may not understand complex questions.
+
+## Previous intermediate queries and answers
+{intermediate_context}
+
+## Main query to answer
+{query}
+
+Respond with a simple follow-up question that will help answer the main query, do not explain yourself or output anything else.
+"""
+
+INTERMEDIATE_ANSWER_PROMPT = """Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond “No relevant information found” if the documents do not contain useful information.
+
+## Documents
+{retrieved_documents}
+
+## Query
+{sub_query}
+
+Respond with a concise answer only, do not explain yourself or output anything else.
+"""
+
+FINAL_ANSWER_PROMPT = """Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate.
+
+## Documents
+{retrieved_documents}
+
+## Intermediate queries and answers
+{intermediate_context}
+
+## Main query
+{query}
+
+Respond with an appropriate answer only, do not explain yourself or output anything else.
+"""
+
+REFLECTION_PROMPT = """Given the following intermediate queries and answers, judge whether you have enough information to answer the main query. If you believe you have enough information, respond with “Yes”, otherwise respond with “No”.
+
+## Intermediate queries and answers
+{intermediate_context}
+
+## Main query
+{query}
+
+Respond with “Yes” or “No” only, do not explain yourself or output anything else.
+"""
+
+GET_SUPPORTED_DOCS_PROMPT = """Given the following documents, select the ones that are support the Q-A pair.
+
+## Documents
+{retrieved_documents}
+
+## Q-A Pair
+### Question
+{query}
+### Answer
+{answer}
+
+Respond with a python list of indices of the selected documents.
+"""
+
+
+class ChainOfRAG:  # todo subclass base agent
+    def __init__(
+        self,
+        llm: BaseLLM,
+        embedding_model: BaseEmbedding,
+        vectordb: BaseVectorDB,
+        **kwargs,
+    ):
+        self.llm = llm
+        self.embedding_model = embedding_model
+        self.vectordb = vectordb
+        self.intermediate_context = []
+
+    def retrieve(
+        self, original_query: str, max_iter: int = 4
+    ) -> Tuple[List[RetrievalResult], List[str], int]:
+        all_retrieved_results = []
+        for iter in range(max_iter):
+            chat_response = self.llm.chat(
+                [
+                    {
+                        "role": "user",
+                        "content": FOLLOWUP_QUERY_PROMPT.format(
+                            query=original_query,
+                            intermediate_context="\n".join(self.intermediate_context),
+                        ),
+                    }
+                ]
+            )
+            followup_query = chat_response.content
+
+            query_vector = self.embedding_model.embed_query(followup_query)
+            retrieved_results = self.vectordb.search_data(collection=None, vector=query_vector)
+
+            chat_response = self.llm.chat(
+                [
+                    {
+                        "role": "user",
+                        "content": INTERMEDIATE_ANSWER_PROMPT.format(
+                            retrieved_documents=self._format_retrieved_results(retrieved_results),
+                            sub_query=followup_query,
+                        ),
+                    }
+                ]
+            )
+            intermediate_answer = chat_response.content
+
+            if "No relevant information found" not in intermediate_answer:
+                chat_response = self.llm.chat(
+                    [
+                        {
+                            "role": "user",
+                            "content": GET_SUPPORTED_DOCS_PROMPT.format(
+                                retrieved_documents=self._format_retrieved_results(
+                                    retrieved_results
+                                ),
+                                query=followup_query,
+                                answer=intermediate_answer,
+                            ),
+                        }
+                    ]
+                )
+                supported_doc_indices = self.llm.literal_eval(chat_response.content)
+                # get retrieved_results from retrived_results using supported_doc_indices
+                supported_retrieved_results = [retrieved_results[i] for i in supported_doc_indices]
+                all_retrieved_results.extend(supported_retrieved_results)
+
+            self.intermediate_context.append(
+                f"Intermediate query{len(self.intermediate_context) + 1}: {followup_query}\nIntermediate answer{len(self.intermediate_context) + 1}: {intermediate_answer}"
+            )
+        all_retrieved_results = deduplicate_results(all_retrieved_results)
+        return all_retrieved_results, [], 0  # todo
+
+    def query(
+        self, original_query: str, max_iter: int = 3
+    ) -> Tuple[str, List[RetrievalResult], int]:
+        all_retrieved_results, _, _ = self.retrieve(original_query, max_iter)
+        chat_response = self.llm.chat(
+            [
+                {
+                    "role": "user",
+                    "content": FINAL_ANSWER_PROMPT.format(
+                        retrieved_documents=self._format_retrieved_results(all_retrieved_results),
+                        intermediate_context="\n".join(self.intermediate_context),
+                        query=original_query,
+                    ),
+                }
+            ]
+        )
+        final_answer = chat_response.content
+        return final_answer, all_retrieved_results, 0  # todo
+
+    def _format_retrieved_results(self, retrieved_results: List[RetrievalResult]) -> str:
+        formatted_documents = []
+        for i, result in enumerate(retrieved_results):
+            formatted_documents.append(f"<Document {i}>\n{result.text}\n<\Document {i}>")
+        return "\n".join(formatted_documents)
