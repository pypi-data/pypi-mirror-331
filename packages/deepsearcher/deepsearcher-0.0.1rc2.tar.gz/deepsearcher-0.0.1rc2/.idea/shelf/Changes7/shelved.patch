Index: deepsearcher/online_query.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import asyncio\nfrom typing import List, Tuple\n\n# from deepsearcher.configuration import vector_db, embedding_model, llm\nfrom deepsearcher import configuration\nfrom deepsearcher.agent import (\n    generate_final_answer,\n    generate_gap_queries,\n    generate_sub_queries,\n)\nfrom deepsearcher.agent.search_vdb import search_chunks_from_vectordb\nfrom deepsearcher.tools import log\nfrom deepsearcher.vector_db.base import RetrievalResult, deduplicate_results\n\n\n# Add a wrapper function to support synchronous calls\ndef query(original_query: str, max_iter: int = 3) -> Tuple[str, List[RetrievalResult], int]:\n    return asyncio.run(async_query(original_query, max_iter))\n\n\nasync def async_query(\n    original_query: str, max_iter: int = 3\n) -> Tuple[str, List[RetrievalResult], int]:\n    retrieval_res, all_sub_queries, retrieve_conseumed_token = await async_retrieve(\n        original_query, max_iter\n    )\n    ### GENERATE FINAL ANSWER ###\n    log.color_print(\"<think> Generating final answer... </think>\\n\")\n    final_answer, final_consumed_token = generate_final_answer(\n        original_query, all_sub_queries, retrieval_res\n    )\n    log.color_print(\"\\n==== FINAL ANSWER====\\n\")\n    log.color_print(final_answer)\n    return final_answer, retrieval_res, retrieve_conseumed_token + final_consumed_token\n\n\ndef retrieve(\n    original_query: str, max_iter: int = 3\n) -> Tuple[List[RetrievalResult], List[str], int]:\n    return asyncio.run(async_retrieve(original_query, max_iter))\n\n\nasync def async_retrieve(\n    original_query: str, max_iter: int = 3\n) -> Tuple[List[RetrievalResult], List[str], int]:\n    log.color_print(f\"<query> {original_query} </query>\\n\")\n    all_search_res = []\n    all_sub_queries = []\n    total_tokens = 0\n\n    ### SUB QUERIES ###\n    sub_queries, used_token = generate_sub_queries(original_query)\n    total_tokens += used_token\n    if not sub_queries:\n        log.color_print(\"No sub queries were generated by the LLM. Exiting.\")\n        return [], [], total_tokens\n    else:\n        log.color_print(\n            f\"<think> Break down the original query into new sub queries: {sub_queries}</think>\\n\"\n        )\n    all_sub_queries.extend(sub_queries)\n    sub_gap_queries = sub_queries\n\n    for iter in range(max_iter):\n        log.color_print(f\">> Iteration: {iter + 1}\\n\")\n        search_res_from_vectordb = []\n        search_res_from_internet = []  # TODO\n\n        # Create all search tasks\n        search_tasks = [\n            search_chunks_from_vectordb(query, sub_gap_queries) for query in sub_gap_queries\n        ]\n        # Execute all tasks in parallel and wait for results\n        search_results = await asyncio.gather(*search_tasks)\n        # Merge all results\n        for result in search_results:\n            search_res, consumed_token = result\n            total_tokens += consumed_token\n            search_res_from_vectordb.extend(search_res)\n\n        search_res_from_vectordb = deduplicate_results(search_res_from_vectordb)\n        # search_res_from_internet = deduplicate_results(search_res_from_internet)\n        all_search_res.extend(search_res_from_vectordb + search_res_from_internet)\n\n        ### REFLECTION & GET GAP QUERIES ###\n        log.color_print(\"<think> Reflecting on the search results... </think>\\n\")\n        sub_gap_queries, consumed_token = generate_gap_queries(\n            original_query, all_sub_queries, all_search_res\n        )\n        total_tokens += consumed_token\n        if not sub_gap_queries:\n            log.color_print(\"<think> No new search queries were generated. Exiting. </think>\\n\")\n            break\n        else:\n            log.color_print(\n                f\"<think> New search queries for next iteration: {sub_gap_queries} </think>\\n\"\n            )\n            all_sub_queries.extend(sub_gap_queries)\n\n    all_search_res = deduplicate_results(all_search_res)\n    return all_search_res, all_sub_queries, total_tokens\n\n\ndef naive_retrieve(query: str, collection: str = None, top_k=10) -> List[RetrievalResult]:\n    vector_db = configuration.vector_db\n    embedding_model = configuration.embedding_model\n\n    if not collection:\n        retrieval_res = []\n        collections = [col_info.collection_name for col_info in vector_db.list_collections()]\n        for collection in collections:\n            retrieval_res_col = vector_db.search_data(\n                collection=collection,\n                vector=embedding_model.embed_query(query),\n                top_k=top_k // len(collections),\n            )\n            retrieval_res.extend(retrieval_res_col)\n        retrieval_res = deduplicate_results(retrieval_res)\n    else:\n        retrieval_res = vector_db.search_data(\n            collection=collection,\n            vector=embedding_model.embed_query(query),\n            top_k=top_k,\n        )\n    return retrieval_res\n\n\ndef naive_rag_query(\n    query: str, collection: str = None, top_k=10\n) -> Tuple[str, List[RetrievalResult]]:\n    llm = configuration.llm\n    retrieval_res = naive_retrieve(query, collection, top_k)\n\n    chunk_texts = []\n    for chunk in retrieval_res:\n        if \"wider_text\" in chunk.metadata:\n            chunk_texts.append(chunk.metadata[\"wider_text\"])\n        else:\n            chunk_texts.append(chunk.text)\n    mini_chunk_str = \"\"\n    for i, chunk in enumerate(chunk_texts):\n        mini_chunk_str += f\"\"\"<chunk_{i}>\\n{chunk}\\n</chunk_{i}>\\n\"\"\"\n\n    summary_prompt = f\"\"\"You are a AI content analysis expert, good at summarizing content. Please summarize a specific and detailed answer or report based on the previous queries and the retrieved document chunks.\n\n    Original Query: {query}\n    Related Chunks: \n    {mini_chunk_str}\n    \"\"\"\n    char_response = llm.chat([{\"role\": \"user\", \"content\": summary_prompt}])\n    final_answer = char_response.content\n    log.color_print(\"\\n==== FINAL ANSWER====\\n\")\n    log.color_print(final_answer)\n    return final_answer, retrieval_res\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/deepsearcher/online_query.py b/deepsearcher/online_query.py
--- a/deepsearcher/online_query.py	(revision 7c0b1bfa6f8e7c05f1e1f0595ab71271668683e3)
+++ b/deepsearcher/online_query.py	(date 1740473685781)
@@ -8,6 +8,7 @@
     generate_gap_queries,
     generate_sub_queries,
 )
+from deepsearcher.agent.chain_of_rag import ChainOfRAG
 from deepsearcher.agent.search_vdb import search_chunks_from_vectordb
 from deepsearcher.tools import log
 from deepsearcher.vector_db.base import RetrievalResult, deduplicate_results
@@ -37,7 +38,13 @@
 def retrieve(
     original_query: str, max_iter: int = 3
 ) -> Tuple[List[RetrievalResult], List[str], int]:
-    return asyncio.run(async_retrieve(original_query, max_iter))
+    # return asyncio.run(async_retrieve(original_query, max_iter))#todo
+    agent = ChainOfRAG(
+        llm=configuration.llm,
+        embedding_model=configuration.embedding_model,
+        vectordb=configuration.vector_db,
+    )
+    return agent.retrieve(original_query, max_iter=4)
 
 
 async def async_retrieve(
Index: evaluation/evaluate.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Some test dataset and evaluation method are ref from https://github.com/OSU-NLP-Group/HippoRAG/tree/main/data , many thanks\n\n################################################################################\n# Note: This evaluation script will cost a lot of LLM token usage, please make sure you have enough token budget.\n################################################################################\nimport argparse\nimport ast\nimport json\nimport logging\nimport os\nimport time\nimport warnings\nfrom typing import List, Tuple\n\nimport pandas as pd\n\nfrom deepsearcher.configuration import Configuration, init_config\nfrom deepsearcher.offline_loading import load_from_local_files\nfrom deepsearcher.online_query import naive_retrieve, retrieve\n\nhttpx_logger = logging.getLogger(\"httpx\")  # disable openai's logger output\nhttpx_logger.setLevel(logging.WARNING)\n\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)  # disable warning output\n\n\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\nk_list = [2, 5]\n\n\ndef _deepsearch_retrieve_titles(\n    question: str, retry_num: int = 4, base_wait_time: int = 4\n) -> Tuple[List[str], bool]:\n    retrieved_results = []\n    for i in range(retry_num):\n        try:\n            retrieved_results, _, _ = retrieve(question)\n            break\n        except Exception:\n            wait_time = base_wait_time * (2**i)\n            print(f\"Parse LLM's output failed, retry again after {wait_time} seconds...\")\n            time.sleep(wait_time)\n    if retrieved_results:\n        retrieved_titles = [\n            retrieved_result.metadata[\"title\"] for retrieved_result in retrieved_results\n        ]\n        fail = False\n    else:\n        print(\"Pipeline error, no retrieved results.\")\n        retrieved_titles = []\n        fail = True\n    return retrieved_titles, fail\n\n\ndef _naive_retrieve_titles(question: str) -> List[str]:\n    retrieved_results = naive_retrieve(question)\n    retrieved_titles = [\n        retrieved_result.metadata[\"title\"] for retrieved_result in retrieved_results\n    ]\n    return retrieved_titles\n\n\ndef _calcu_recall(sample, retrieved_titles, dataset) -> dict:\n    if dataset in [\"2wikimultihopqa\"]:\n        gold_passages = [item for item in sample[\"supporting_facts\"]]\n        gold_items = set([item[0] for item in gold_passages])\n        retrieved_items = retrieved_titles\n    else:\n        raise NotImplementedError\n\n    recall = dict()\n    for k in k_list:\n        recall[k] = round(\n            sum(1 for t in gold_items if t in retrieved_items[:k]) / len(gold_items), 4\n        )\n    return recall\n\n\ndef _print_recall_line(recall: dict, pre_str=\"\", post_str=\"\\n\"):\n    print(pre_str, end=\"\")\n    for k in k_list:\n        print(f\"R@{k}: {recall[k]:.3f} \", end=\"\")\n    print(post_str, end=\"\")\n\n\ndef evaluate(\n    dataset: str,\n    output_root: str,\n    pre_num: int = 10,\n    skip_load=False,\n    flag: str = \"result\",\n):\n    corpus_file = os.path.join(current_dir, f\"../examples/data/{dataset}_corpus.json\")\n    if not skip_load:\n        # set chunk size to a large number to avoid chunking, because the dataset was chunked already.\n        load_from_local_files(\n            corpus_file, force_new_collection=True, chunk_size=999999, chunk_overlap=0\n        )\n\n    eval_output_subdir = os.path.join(output_root, flag)\n    os.makedirs(eval_output_subdir, exist_ok=True)\n    csv_file_path = os.path.join(eval_output_subdir, \"details.csv\")\n\n    data_with_gt_file_path = os.path.join(current_dir, f\"../examples/data/{dataset}.json\")\n    data_with_gt = json.load(open(data_with_gt_file_path, \"r\"))\n\n    if not pre_num:\n        pre_num = len(data_with_gt)\n\n    pipeline_error_num = 0\n    end_ind = min(pre_num, len(data_with_gt))\n\n    start_ind = 0\n    existing_df = pd.DataFrame()\n    if os.path.exists(csv_file_path):\n        existing_df = pd.read_csv(csv_file_path)\n        start_ind = len(existing_df)\n        print(f\"Loading results from {csv_file_path}, start_index = {start_ind}\")\n\n    for sample_idx, sample in enumerate(data_with_gt[start_ind:end_ind]):\n        global_idx = sample_idx + start_ind\n        question = sample[\"question\"]\n\n        retrieved_titles, fail = _deepsearch_retrieve_titles(question)\n        retrieved_titles_naive = _naive_retrieve_titles(question)\n\n        if fail:\n            pipeline_error_num += 1\n            print(\n                f\"Pipeline error, no retrieved results. Current pipeline_error_num = {pipeline_error_num}\"\n            )\n\n        print(f\"idx: {global_idx}: \")\n        recall = _calcu_recall(sample, retrieved_titles, dataset)\n        recall_naive = _calcu_recall(sample, retrieved_titles_naive, dataset)\n        current_result = [\n            {\n                \"idx\": global_idx,\n                \"question\": question,\n                \"recall\": recall,\n                \"recall_naive\": recall_naive,\n                \"gold_titles\": [item[0] for item in sample[\"supporting_facts\"]],\n                \"retrieved_titles\": retrieved_titles,\n                \"retrieved_titles_naive\": retrieved_titles_naive,\n            }\n        ]\n        current_df = pd.DataFrame(current_result)\n        existing_df = pd.concat([existing_df, current_df], ignore_index=True)\n        existing_df.to_csv(csv_file_path, index=False)\n        average_recall = dict()\n        average_recall_naive = dict()\n        for k in k_list:\n            average_recall[k] = sum(\n                [\n                    ast.literal_eval(d).get(k) if isinstance(d, str) else d.get(k)\n                    for d in existing_df[\"recall\"]\n                ]\n            ) / len(existing_df)\n            average_recall_naive[k] = sum(\n                [\n                    ast.literal_eval(d).get(k) if isinstance(d, str) else d.get(k)\n                    for d in existing_df[\"recall_naive\"]\n                ]\n            ) / len(existing_df)\n        _print_recall_line(average_recall, pre_str=\"Average recall of DeepSearcher: \")\n        _print_recall_line(average_recall_naive, pre_str=\"Average recall of naive RAG   : \")\n        print(\"\")\n    print(\"Finish results to save.\")\n\n\ndef main_eval():\n    parser = argparse.ArgumentParser(prog=\"evaluation\", description=\"Deep Searcher evaluation.\")\n    parser.add_argument(\n        \"--dataset\",\n        type=str,\n        default=\"2wikimultihopqa\",\n        help=\"Dataset name, default is `2wikimultihopqa`. More datasets will be supported in the future.\",\n    )\n    parser.add_argument(\n        \"--config_yaml\",\n        type=str,\n        default=\"./eval_config.yaml\",\n        help=\"Configuration yaml file path, default is `./eval_config.yaml`\",\n    )\n    parser.add_argument(\n        \"--pre_num\",\n        type=int,\n        default=30,\n        help=\"Number of samples to evaluate, default is 30\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"./eval_output\",\n        help=\"Output root directory, default is `./eval_output`\",\n    )\n    parser.add_argument(\n        \"--skip_load\",\n        action=\"store_true\",\n        help=\"Whether to skip loading the dataset. Default it don't skip loading. If you want to skip loading, please set this flag.\",\n    )\n    parser.add_argument(\n        \"--flag\",\n        type=str,\n        default=\"result\",\n        help=\"Flag for evaluation, default is `result`\",\n    )\n\n    args = parser.parse_args()\n\n    config = Configuration(config_path=args.config_yaml)\n    init_config(config=config)\n\n    evaluate(\n        dataset=args.dataset,\n        output_root=args.output_dir,\n        pre_num=args.pre_num,\n        skip_load=args.skip_load,\n        flag=args.flag,\n    )\n\n\nif __name__ == \"__main__\":\n    main_eval()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/evaluation/evaluate.py b/evaluation/evaluate.py
--- a/evaluation/evaluate.py	(revision 7c0b1bfa6f8e7c05f1e1f0595ab71271668683e3)
+++ b/evaluation/evaluate.py	(date 1740475561939)
@@ -171,7 +171,7 @@
 
 
 def main_eval():
-    parser = argparse.ArgumentParser(prog="evaluation", description="Deep Searcher evaluation.")
+    parser = argparse.ArgumentParser(prog="evaluate", description="Deep Searcher evaluation.")
     parser.add_argument(
         "--dataset",
         type=str,
Index: deepsearcher/agent/chain_of_rag.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/deepsearcher/agent/chain_of_rag.py b/deepsearcher/agent/chain_of_rag.py
new file mode 100644
--- /dev/null	(date 1740474578778)
+++ b/deepsearcher/agent/chain_of_rag.py	(date 1740474578778)
@@ -0,0 +1,166 @@
+from typing import List, Tuple
+
+from deepsearcher.embedding.base import BaseEmbedding
+from deepsearcher.llm.base import BaseLLM
+from deepsearcher.vector_db import RetrievalResult
+from deepsearcher.vector_db.base import BaseVectorDB, deduplicate_results
+
+FOLLOWUP_QUERY_PROMPT = """You are using a search tool to answer the main query by iteratively searching the database. Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up questions only as the search tool may not understand complex questions.
+
+## Previous intermediate queries and answers
+{intermediate_context}
+
+## Main query to answer
+{query}
+
+Respond with a simple follow-up question that will help answer the main query, do not explain yourself or output anything else.
+"""
+
+INTERMEDIATE_ANSWER_PROMPT = """Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond “No relevant information found” if the documents do not contain useful information.
+
+## Documents
+{retrieved_documents}
+
+## Query
+{sub_query}
+
+Respond with a concise answer only, do not explain yourself or output anything else.
+"""
+
+FINAL_ANSWER_PROMPT = """Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate.
+
+## Documents
+{retrieved_documents}
+
+## Intermediate queries and answers
+{intermediate_context}
+
+## Main query
+{query}
+
+Respond with an appropriate answer only, do not explain yourself or output anything else.
+"""
+
+REFLECTION_PROMPT = """Given the following intermediate queries and answers, judge whether you have enough information to answer the main query. If you believe you have enough information, respond with “Yes”, otherwise respond with “No”.
+
+## Intermediate queries and answers
+{intermediate_context}
+
+## Main query
+{query}
+
+Respond with “Yes” or “No” only, do not explain yourself or output anything else.
+"""
+
+GET_SUPPORTED_DOCS_PROMPT = """Given the following documents, select the ones that are support the Q-A pair.
+
+## Documents
+{retrieved_documents}
+
+## Q-A Pair
+### Question
+{query}
+### Answer
+{answer}
+
+Respond with a python list of indices of the selected documents.
+"""
+
+
+class ChainOfRAG:  # todo subclass base agent
+    def __init__(
+        self,
+        llm: BaseLLM,
+        embedding_model: BaseEmbedding,
+        vectordb: BaseVectorDB,
+        **kwargs,
+    ):
+        self.llm = llm
+        self.embedding_model = embedding_model
+        self.vectordb = vectordb
+        self.intermediate_context = []
+
+    def retrieve(
+        self, original_query: str, max_iter: int = 4
+    ) -> Tuple[List[RetrievalResult], List[str], int]:
+        all_retrieved_results = []
+        for iter in range(max_iter):
+            chat_response = self.llm.chat(
+                [
+                    {
+                        "role": "user",
+                        "content": FOLLOWUP_QUERY_PROMPT.format(
+                            query=original_query,
+                            intermediate_context="\n".join(self.intermediate_context),
+                        ),
+                    }
+                ]
+            )
+            followup_query = chat_response.content
+
+            query_vector = self.embedding_model.embed_query(followup_query)
+            retrieved_results = self.vectordb.search_data(collection=None, vector=query_vector)
+
+            chat_response = self.llm.chat(
+                [
+                    {
+                        "role": "user",
+                        "content": INTERMEDIATE_ANSWER_PROMPT.format(
+                            retrieved_documents=self._format_retrieved_results(retrieved_results),
+                            sub_query=followup_query,
+                        ),
+                    }
+                ]
+            )
+            intermediate_answer = chat_response.content
+
+            if "No relevant information found" not in intermediate_answer:
+                chat_response = self.llm.chat(
+                    [
+                        {
+                            "role": "user",
+                            "content": GET_SUPPORTED_DOCS_PROMPT.format(
+                                retrieved_documents=self._format_retrieved_results(
+                                    retrieved_results
+                                ),
+                                query=followup_query,
+                                answer=intermediate_answer,
+                            ),
+                        }
+                    ]
+                )
+                supported_doc_indices = self.llm.literal_eval(chat_response.content)
+                # get retrieved_results from retrived_results using supported_doc_indices
+                supported_retrieved_results = [retrieved_results[i] for i in supported_doc_indices]
+                all_retrieved_results.extend(supported_retrieved_results)
+
+            self.intermediate_context.append(
+                f"Intermediate query{len(self.intermediate_context) + 1}: {followup_query}\nIntermediate answer{len(self.intermediate_context) + 1}: {intermediate_answer}"
+            )
+        all_retrieved_results = deduplicate_results(all_retrieved_results)
+        return all_retrieved_results, [], 0  # todo
+
+    def query(
+        self, original_query: str, max_iter: int = 3
+    ) -> Tuple[str, List[RetrievalResult], int]:
+        all_retrieved_results, _, _ = self.retrieve(original_query, max_iter)
+        chat_response = self.llm.chat(
+            [
+                {
+                    "role": "user",
+                    "content": FINAL_ANSWER_PROMPT.format(
+                        retrieved_documents=self._format_retrieved_results(all_retrieved_results),
+                        intermediate_context="\n".join(self.intermediate_context),
+                        query=original_query,
+                    ),
+                }
+            ]
+        )
+        final_answer = chat_response.content
+        return final_answer, all_retrieved_results, 0  # todo
+
+    def _format_retrieved_results(self, retrieved_results: List[RetrievalResult]) -> str:
+        formatted_documents = []
+        for i, result in enumerate(retrieved_results):
+            formatted_documents.append(f"<Document {i}>\n{result.text}\n<\Document {i}>")
+        return "\n".join(formatted_documents)
