import os
import sqlite3
import json
from datetime import datetime, timedelta
from typing import Optional, Any
import sys
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama.llms import OllamaLLM
from thefuck.conf import settings
import traceback

class LLMCache:
    """Local SQLite caching implementation."""
    def __init__(self, ttl_hours: int = 24):
        self.conn = sqlite3.connect(os.path.expanduser('~/.thefuck_llm_cache.db'))
        self._init_db()
        self.ttl = timedelta(hours=ttl_hours)

    def _init_db(self):
        with self.conn:
            self.conn.execute('''CREATE TABLE IF NOT EXISTS cache
                                (hash TEXT PRIMARY KEY,
                                 suggestion TEXT,
                                 created_at TIMESTAMP)''')

    def get(self, prompt_hash: str) -> Optional[str]:
        cursor = self.conn.execute('''SELECT suggestion, created_at FROM cache
                                    WHERE hash=?''', (prompt_hash,))
        row = cursor.fetchone()
        if row:
            suggestion, created_at = row
            if datetime.now() - datetime.fromisoformat(created_at) < self.ttl:
                return suggestion
        return None

    def set(self, prompt_hash: str, suggestion: str):
        with self.conn:
            self.conn.execute('''INSERT OR REPLACE INTO cache
                               VALUES (?, ?, ?)''',
                            (prompt_hash, suggestion, datetime.now().isoformat()))

class LLMFactory:
    _supported_providers = {
        'openai': {
            'model_configs': {
                'model': lambda: os.environ.get('OPENAI_MODEL', settings.openai_model),
                'api_key': lambda: os.environ.get('OPENAI_API_KEY', settings.openai_api_key),
                'base_url': lambda: os.environ.get('OPENAI_BASE_URL', settings.openai_base_url),
                'temperature': lambda: float(os.environ.get('THEFUCK_TEMPERATURE', settings.temperature)),
            }
        },
        'siliconflow': {
            'model_configs': {
                'model': lambda: os.environ.get('SILICONFLOW_MODEL', settings.siliconflow_model),
                'api_key': lambda: os.environ.get('SILICONFLOW_API_KEY', settings.siliconflow_api_key),
                'temperature': lambda: float(os.environ.get('THEFUCK_TEMPERATURE', settings.temperature)),
                'base_url': lambda: "https://api.siliconflow.cn/"
            },
            'openai_compatible': True
        },
        'deepseek': {
            'model_configs': {
                'model': lambda: os.environ.get('DEEPSEEK_MODEL', settings.deepseek_model),
                'api_key': lambda: os.environ.get('DEEPSEEK_API_KEY', settings.deepseek_api_key),
                'temperature': lambda: float(os.environ.get('THEFUCK_TEMPERATURE', settings.temperature)),
            }
        },
        'ollama': {
            'model_configs': {
                'model': lambda: os.environ.get('OLLAMA_MODEL', settings.ollama_model),
                'temperature': lambda: float(os.environ.get('THEFUCK_TEMPERATURE', settings.temperature)),
                'base_url': lambda: os.environ.get('OLLAMA_BASE_URL', settings.ollama_base_url),
            }
        }
    }

    @classmethod
    def create(cls) -> Any:
        provider = os.environ.get('THEFUCK_MODEL_PROVIDER', settings.model_provider)
        provider = provider.lower()
        if provider not in cls._supported_providers:
            raise ValueError(f"Unsupported LLM provider: {provider}")

        provider_config = cls._supported_providers[provider]
        config = {
            key: getter() 
            for key, getter in provider_config['model_configs'].items()
        }

        if provider == "ollama":
            return OllamaLLM(**config)

        if provider_config.get('openai_compatible', False):
            provider = "openai"
        
        return init_chat_model(
            model_provider=provider,
            **config
        )

def generate_llm_response(context: Any, user_prompt: str, system_prompts: list, use_cache: bool = False ,is_fix_mode: bool = False) -> str:
    """
    Generic LLM response generation function, supporting custom prompts and caching
    
    Args:
        context: Context content for response generation
        system_prompts: List of system prompts, default is command correction prompts
        user_prompt: User prompt template, default is command correction prompt
        use_cache: Whether to use cache, default is True
        
    Returns:
        str: Response generated by LLM
    """

    # Cache handling
    if use_cache:
        cache = LLMCache(24)
        # Generate unique cache key, including prompts and context
        cache_data = {
            "context": context,
            "system_prompts": system_prompts,
            "user_prompt": user_prompt
        }
        context_str = json.dumps(cache_data, sort_keys=True)
        prompt_hash = str(hash(context_str))
        
        # Check cache
        if cached := cache.get(prompt_hash):
            return cached
    
    # Build message list
    messages = []
    for prompt in system_prompts:
        messages.append(("system", prompt))
    messages.append(("user", user_prompt))
    
    # Create prompt template
    prompt = ChatPromptTemplate.from_messages(messages)
    
    try:
        llm = LLMFactory.create()
        
        # Create and execute chain
        chain = prompt | llm | StrOutputParser()
        print("Generating...", flush=True, file=sys.stderr)
        response = chain.invoke({"context": context})
        
        # Cache results
        if use_cache:
            cache.set(prompt_hash, response)
            
        return response
        
    except Exception as e:
        traceback.print_exc()
        print(f"LLM call failed: {str(e)}", flush=True, file=sys.stderr)
        if is_fix_mode:
            print("AI-based suggestion is not available. Falling back to non-AI mode.", flush=True, file=sys.stderr)
        return ""
    
def get_fix_suggestion(context: Any) -> str:
    """
    Generate command correction suggestions
    """
    system_prompts = [
        "Please only return the corrected command without any explanation. Ensure the command is safe and compliant with Unix standards.",
        "Only a single command is needed, not multiple commands.",
        "If the error message contains 'command not found', generate an approximate common command.",
        "If the command itself is not an installation command, such as apt, apt-get, or yum, do not generate an installation command.",
        "Ensure that the command output is clean, without any additional information such as markdown formatting, prompts, or comments."
    ]
    user_prompt = "Please help me fix the following command: {context}"

    return generate_llm_response(context, user_prompt, system_prompts)

def generate_command(context: str) -> str:
    """
    Generate command from natural language description
    """
    system_prompts = [
        "You are a Linux command line expert.",
        "Generate the most suitable shell command based on the user's natural language request.",
        "Return only the command itself, without any explanations or other text.",
        "Ensure that the command output is clean, without any additional information such as markdown formatting, prompts, or comments.",
        "Ensure the command is valid and follows best practices."
    ]
    user_prompt = "Generate a command to: {context}"
    
    return generate_llm_response(context, user_prompt, system_prompts)