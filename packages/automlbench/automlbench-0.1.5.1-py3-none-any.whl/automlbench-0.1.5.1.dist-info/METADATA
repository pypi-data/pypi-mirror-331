Metadata-Version: 2.1
Name: automlbench
Version: 0.1.5.1
Summary: A Python package for automated ML model benchmarking and comparison
Home-page: https://github.com/AnnNaserNabil/automlbench
Author: Ann Naser Nabil
Author-email: Ann Naser Nabil <ann.n.nabil@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/AnnNaserNabil/automlbench
Project-URL: Documentation, https://github.com/AnnNaserNabil/automlbench/wiki
Project-URL: Repository, https://github.com/AnnNaserNabil/automlbench
Project-URL: Issues, https://github.com/AnnNaserNabil/automlbench/issues
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: scikit-learn
Requires-Dist: matplotlib
Requires-Dist: seaborn
Requires-Dist: xgboost
Requires-Dist: lightgbm
Requires-Dist: shap
Requires-Dist: lime
Requires-Dist: catboost
Requires-Dist: imbalanced-learn

## **Automated Machine Learning Benchmarking Library**
üöÄ **AutoMLBench** provides a seamless way to compare machine learning models, preprocess data, evaluate performance, and optimize models with **hyperparameter tuning**.

---

## **üìå Installation**
Ensure all dependencies are installed:
```bash
pip install pandas scikit-learn numpy matplotlib xgboost lightgbm catboost imbalanced-learn
```
Install from pypi:
```bash
pip install automlbench
```

For local development:
```bash
git clone https://github.com/AnnNaserNabil/automlbench.git
cd automlbench
pip install -e .
```

## **Model Comparison Without Hyperparameter Tuning**
The simplest way to compare models using **AutoMLBench**.

### **1Ô∏è‚É£ Load Dataset & Preprocess**
```python
import pandas as pd
from automlbench import preprocess_data, get_models, train_models, evaluate_model, plot_performance

# Load dataset
url = "DATAPATH"
df = pd.read_csv(url) 

# Define target column
target_column = "Name OF the Target Column"

# Preprocess data
X_train, X_test, y_train, y_test = preprocess_data(df, target_column)
```

---

### **2Ô∏è‚É£ Train All Default Models**
```python
# Get predefined models
models = get_models()

# Train models without tuning
results = train_models(X_train, X_test, y_train, y_test)

# Print model performance results
print(results)
```

---

### **3Ô∏è‚É£ Evaluate & Compare Model Performance**
```python
# Evaluate all models
for model_name, model in models.items():
    print(f"Evaluating {model_name}...")
    metrics = evaluate_model(model.fit(X_train, y_train), X_test, y_test)
    print(metrics)

# Plot performance comparison
plot_performance(results, metrics=["Accuracy", "Precision", "Recall", "F1-Score", "RMSE"])
```

---

## **üîπ Model Comparison With Hyperparameter Tuning**
For better performance, use **hyperparameter tuning**.

### **1Ô∏è‚É£ Get Hyperparameter Grids**
```python
from automlbench import get_hyperparameter_grids, tune_hyperparameters

# Retrieve hyperparameter grids
hyperparameter_grids = get_hyperparameter_grids()
```

---

### **2Ô∏è‚É£ Tune Models**
```python
best_models = {}

# Tune each model if it has a predefined hyperparameter grid
for model_name, model in models.items():
    if model_name in hyperparameter_grids:
        print(f"Tuning {model_name}...")
        best_model, best_params = tune_hyperparameters(model, hyperparameter_grids[model_name], X_train, y_train)
        best_models[model_name] = best_model
        print(f"Best params for {model_name}: {best_params}")
    else:
        best_models[model_name] = model  # Use default if no tuning grid
```

---

### **3Ô∏è‚É£ Train Tuned Models**
```python
# Train models using the best hyperparameters found
tuned_results = train_models(
    X_train, X_test, y_train, y_test, 
    selected_models=list(best_models.keys()), 
    hyperparams={name: model.get_params() for name, model in best_models.items()}
)

# Display tuned model results
print(tuned_results)
```

---

### **4Ô∏è‚É£ Evaluate & Compare Tuned Models**
```python
# Evaluate all tuned models
for model_name, model in best_models.items():
    print(f"Evaluating {model_name}...")
    metrics = evaluate_model(model.fit(X_train, y_train), X_test, y_test)
    print(metrics)

# Plot comparison of tuned models
plot_performance(tuned_results, metrics=["Accuracy", "Precision", "Recall", "F1-Score", "RMSE"])
```


## **‚ö° Quick Summary**
‚úÖ **Basic Comparison** ‚Äì Train models with default settings.  
‚úÖ **Hyperparameter Tuning** ‚Äì Optimize models for better performance.  
‚úÖ **Evaluation & Visualization** ‚Äì Compare accuracy, precision, recall, F1-score, and RMSE.  
‚úÖ **Automated ML Benchmarking** ‚Äì Quickly assess multiple models with minimal code.  

---

## **üìå Contributing**
Contributions are welcome! To contribute:
1. **Fork the repository**
2. **Create a new branch (`feature-branch`)**
3. **Make changes & test (`pytest tests/`)**
4. **Submit a pull request (PR)**

---

## **üìú License**
AutoMLBench is released under the **MIT License**.

---

This **documentation makes it easy** for users to **compare models before and after tuning** using **AutoMLBench**. üöÄ Let me know if you need modifications! üî•
