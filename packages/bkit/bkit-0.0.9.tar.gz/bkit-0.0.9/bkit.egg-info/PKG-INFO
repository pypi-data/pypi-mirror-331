Metadata-Version: 2.2
Name: bkit
Version: 0.0.9
Summary: A python tool for Bangla text processing
Author-email: banglagov <manage.bangla@gmail.com>
Project-URL: Homepage, https://github.com/giga-tech/bangla-text-processing-kit
Keywords: bangla kit,bangla text processing,banpipeline,ner,pos,shallow parsing,dependency parsing
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: regex
Provides-Extra: lemma
Requires-Dist: bnlp-toolkit; extra == "lemma"
Provides-Extra: all
Requires-Dist: torch<2.6,>=1.13; extra == "all"
Requires-Dist: bnlp-toolkit; extra == "all"
Requires-Dist: transformers[sentencepiece]==4.28.0; extra == "all"
Requires-Dist: onnxruntime<2.0,>=1.20.1; extra == "all"
Requires-Dist: spacy<4.0,>=3.8.4; extra == "all"
Requires-Dist: datasets; extra == "all"
Requires-Dist: gdown; extra == "all"
Requires-Dist: cython; extra == "all"
Requires-Dist: pyhocon; extra == "all"
Requires-Dist: numpy; extra == "all"
Requires-Dist: segtok; extra == "all"

# bangla-text-processing-kit

A python tool kit for processing Bangla texts.

- [bangla-text-processing-kit](#bangla-text-processing-kit)
  - [How to use](#how-to-use)
    - [Installing](#installing)
    - [Checking text](#checking-text)
    - [Transforming text](#transforming-text)
      - [Normalizer](#normalizer)
      - [Character Normalization](#character-normalization)
      - [Punctuation space normalization](#punctuation-space-normalization)
      - [Zero width characters normalization](#zero-width-characters-normalization)
      - [Halant (рж╣рж╕ржирзНржд) normalization](#halant-рж╣рж╕ржирзНржд-normalization)
      - [Kar ambiguity](#kar-ambiguity)
      - [Clean text](#clean-text)
      - [Clean punctuations](#clean-punctuations)
      - [Clean digits](#clean-digits)
      - [Multiple spaces](#multiple-spaces)
      - [URLs](#urls)
      - [Emojis](#emojis)
      - [HTML tags](#html-tags)
      - [Multiple punctuations](#multiple-punctuations)
      - [Special characters](#special-characters)
      - [Non Bangla characters](#non-bangla-characters)
    - [Text Analysis](#text-analysis)
      - [Word count](#word-count)
      - [Sentence Count](#sentence-count)
    - [Lemmatization](#lemmatization)
      - [Lemmatize text](#lemmatize-text)
      - [Lemmatize word](#lemmatize-word)
    - [Stemmer](#stemmer)
      - [Stem word](#stem-word)
      - [Stem sentence](#stem-sentence)
    - [Tokenization](#tokenization)
      - [Word tokenization](#word-tokenization)
      - [Word and Punctuation tokenization](#word-and-punctuation-tokenization)
      - [Sentence tokenization](#sentence-tokenization)
    - [Named Entity Recognition (NER)](#named-entity-recognition-ner)
      - [Named Entity Recognition (NER) Visualization](#named-entity-recognition-ner-visualization)
    - [Parts of Speech (PoS) tagging](#parts-of-speech-pos-tagging)
      - [Parts of Speech (PoS) Visualization](#parts-of-speech-pos-visualization)
    - [Shallow Parsing (Constituency Parsing)](#shallow-parsing-constituency-parsing)
        - [Shallow Parsing Visualization](#shallow-parsing-visualization)
    - [Dependency Parsing](#dependency-parsing)
        - [Dependency Parsing Visualization](#dependency-parsing-visualization)
    - [Coreference Resolution](#coref-resolution)
      - [Coreference Resolution Visualization](#coreference-resolution-visualization)
    

## How to use<a id="how-to-use"></a>

### Installing<a id="installing"></a>

There are three installation options of the bkit package. These are:

1. `bkit`: The most basic version of bkit with the normalization, cleaning and tokenization capabilities.

```bash
pip install bkit
```
2. `bkit[lemma]`: Everything in the basic version plus lemmatization capability.

```bash
pip install bkit[lemma]
```
3. `bkit[all]`: Everything that are available in bkit including normalization, cleaning, tokenization, lemmatization, NER, POS and shallow parsing. 

```bash
pip install bkit[all]
```

### Checking text<a id="checking-text"></a>

- `bkit.utils.is_bangla(text) -> bool`: Checks if text contains only Bangla characters, digits, spaces, punctuations and some symbols. Returns true if so, else return false.
- `bkit.utils.is_digit(text) -> bool`: Checks if text contains only **Bangla digit** characters. Returns true if so, else return false.
- `bkit.utils.contains_digit(text, check_english_digits) -> bool`: Checks if text contains **any digits**. By default checks only Bangla digits. Returns true if so, else return false.
- `bkit.utils.contains_bangla(text) -> bool`: Checks if text contains **any Bangla character**. Returns true if so, else return false.

### Transforming text<a id="transforming-text"></a>

Text transformation includes the normalization and cleaning procedures. To transform text, use the `bkit.transform` module. Supported functionalities are:

#### Normalizer<a id="normalizer"></a>

This module normalize Bangla text using the following steps:

<!-- no toc -->
1. [Character normalization](#character-normalization)
2. [Zero width character normalization](#zero-width-characters-normalization)
3. [Halant normalization](#halant-рж╣рж╕ржирзНржд-normalization)
4. [Vowel-kar normalization](#kar-ambiguity)
5. [Punctuation space normalization](#punctuation-space-normalization)

```python
import bkit

text = 'ржЕрж╛рж╛ржорж╛ржмрж╝ ред '
print(list(text))
# >>> ['ржЕ', 'рж╛', 'рж╛', 'ржо', 'рж╛', 'ржм', 'рж╝', ' ', 'ред', ' ']

normalizer = bkit.transform.Normalizer(
    normalize_characters=True,
    normalize_zw_characters=True,
    normalize_halant=True,
    normalize_vowel_kar=True,
    normalize_punctuation_spaces=True
)

clean_text = normalizer(text)
print(clean_text, list(clean_text))
# >>> ржЖржорж╛рж░ред ['ржЖ', 'ржо', 'рж╛', 'рж░', 'ред']
```

#### Character Normalization<a id="character-normalization"></a>

This module performs character normalization in Bangla text. It performs nukta normalization, Assamese normalization, Kar normalization, legacy character normalization and Punctuation normalization sequentially.

```python
import bkit

text = 'ржЖржорж╛ржмрж╝'
print(list(text))
# >>> ['ржЖ', 'ржо', 'рж╛', 'ржм', 'рж╝']

text = bkit.transform.normalize_characters(text)

print(list(text))
# >>> ['ржЖ', 'ржо', 'рж╛', 'рж░']
```

#### Punctuation space normalization<a id="punctuation-space-normalization"></a>

Normalizes punctuation spaces i.e. adds necessary spaces before or after specific punctuations, also removes if necessary.

```python
import bkit

text = 'рж░рж╣рж┐ржо(рзирзй)ржП ржХржерж╛ ржмрж▓рзЗржи   редрждрж┐ржирж┐ (    рж░рж╣рж┐ржо ) ржЖрж░ржУ ржЬрж╛ржирж╛ржи, рзз,рзирзк,рзйрзл,рзмрзлрзк.рзйрзирзй ржХрзЛржЯрж┐ ржЯрж╛ржХрж╛ ржмрзНржпрж╛рзЯрзЗ...'

clean_text = bkit.transform.normalize_punctuation_spaces(text)
print(clean_text)
# >>> рж░рж╣рж┐ржо (рзирзй) ржП ржХржерж╛ ржмрж▓рзЗржиред рждрж┐ржирж┐ (рж░рж╣рж┐ржо) ржЖрж░ржУ ржЬрж╛ржирж╛ржи, рзз,рзирзк,рзйрзл,рзмрзлрзк.рзйрзирзй ржХрзЛржЯрж┐ ржЯрж╛ржХрж╛ ржмрзНржпрж╛рзЯрзЗ...
```

#### Zero width characters normalization<a id="zero-width-characters-normalization"></a>

There are two zero-width characters. These are Zero Width Joiner (ZWJ) and Zero Width Non Joiner (ZWNJ) characters. Generally ZWNJ is not used with Bangla texts and ZWJ joiner is used with `рж░` only. So, these characters are normalized based on these intuitions.

```python
import bkit

text = 'рж░тАНрзНржптАМрж╛ржХрзЗржЯ'
print(f"text: {text} \t Characters: {list(text)}")
# >>> text: рж░тАНрзНржптАМрж╛ржХрзЗржЯ     Characters: ['рж░', '\u200d', 'рзН', 'ржп', '\u200c', 'рж╛', 'ржХ', 'рзЗ', 'ржЯ']

clean_text = bkit.transform.normalize_zero_width_chars(text)
print(f"text: {clean_text} \t Characters: {list(clean_text)}")
# >>> text: рж░тАНрзНржпрж╛ржХрзЗржЯ     Characters: ['рж░', '\u200d', 'рзН', 'ржп', 'рж╛', 'ржХ', 'рзЗ', 'ржЯ']
```

#### Halant (рж╣рж╕ржирзНржд) normalization<a id="halant-рж╣рж╕ржирзНржд-normalization"></a>

This function normalizes halant (рж╣рж╕ржирзНржд) [`0x09CD`] in Bangla text. While using this function, it is recommended to normalize the zero width characters at first, e.g. using the `bkit.transform.normalize_zero_width_chars()` function.

During the normalization it also handles the `рждрзН -> рзО` conversion. For a valid conjunct letter (ржпрзБржХрзНрждржмрж░рзНржг) where 'ржд' is the former character, can take one of 'ржд', 'рже', 'ржи', 'ржм', 'ржо', 'ржп', and 'рж░' as the next character. The conversion is perform based on this intuition.

During the halant normalization, the following cases are handled.

- Remove any leading and tailing halant of a word and/or text.
- Replace two or more consecutive occurrences of halant by a single halant.
- Remove halant between any characters that do not follow or precede a halant character. Like a halant that follows or precedes a vowel, kar, рзЯ, etc will be removed.
- Remove multiple fola (multiple ref, ro-fola and jo-fola)

```python
import bkit

text = 'ржЖрж╕ржирзНрзНрзНржи ржЖрж╕ржлрж╛ржХрзБрж▓рзНрж▓рж╛рж╣рзНтАМ ржЖрж▓ржмрждрзНтАН ржЖрж▓ржмрждрзН рж░тАНрзНржпрж╛ржм ржЗрзНрж╕рж┐'
print(list(text))
# >>> ['ржЖ', 'рж╕', 'ржи', 'рзН', 'рзН', 'рзН', 'ржи', ' ', 'ржЖ', 'рж╕', 'ржл', 'рж╛', 'ржХ', 'рзБ', 'рж▓', 'рзН', 'рж▓', 'рж╛', 'рж╣', 'рзН', '\u200c', ' ', 'ржЖ', 'рж▓', 'ржм', 'ржд', 'рзН', '\u200d', ' ', 'ржЖ', 'рж▓', 'ржм', 'ржд', 'рзН', ' ', 'рж░', '\u200d', 'рзН', 'ржп', 'рж╛', 'ржм', ' ', 'ржЗ', 'рзН', 'рж╕', 'рж┐']

clean_text = bkit.transform.normalize_zero_width_chars(text)
clean_text = bkit.transform.normalize_halant(clean_text)
print(clean_text, list(clean_text))
# >>> ржЖрж╕ржирзНржи ржЖрж╕ржлрж╛ржХрзБрж▓рзНрж▓рж╛рж╣ ржЖрж▓ржмрзО ржЖрж▓ржмрзО рж░тАНрзНржпрж╛ржм ржЗрж╕рж┐ ['ржЖ', 'рж╕', 'ржи', 'рзН', 'ржи', ' ', 'ржЖ', 'рж╕', 'ржл', 'рж╛', 'ржХ', 'рзБ', 'рж▓', 'рзН', 'рж▓', 'рж╛', 'рж╣', ' ', 'ржЖ', 'рж▓', 'ржм', 'рзО', ' ', 'ржЖ', 'рж▓', 'ржм', 'рзО', ' ', 'рж░', '\u200d', 'рзН', 'ржп', 'рж╛', 'ржм', ' ', 'ржЗ', 'рж╕', 'рж┐']
```

#### Kar ambiguity<a id="kar-ambiguity"></a>

Normalizes kar ambiguity with vowels, ржБ, ржВ, and ржГ. It removes any kar that is preceded by a vowel or consonant diacritics like: `ржЖрж╛` will be normalized to `ржЖ`. In case of consecutive occurrence of kars like: `ржХрж╛рж╛рж╛рзА`, only the first kar will be kept like: `ржХрж╛`.

```python
import bkit

text = 'ржЕржВрж╢ржЗрзЗ ржЕржВрж╢ржЧрзНрж░рж╣ржгржЗрзЗ ржЖрж╛рж╛рж░рзЛ ржПржЦржиржУрзЛ ржЖрж▓ржмрж╛рж░рзНрждрзЛрзЗ рж╕рж╛ржзрзБрзБ ржХрж╛рж╛рж╛рзА'
print(list(text))
# >>> ['ржЕ', 'ржВ', 'рж╢', 'ржЗ', 'рзЗ', ' ', 'ржЕ', 'ржВ', 'рж╢', 'ржЧ', 'рзН', 'рж░', 'рж╣', 'ржг', 'ржЗ', 'рзЗ', ' ', 'ржЖ', 'рж╛', 'рж╛', 'рж░', 'рзЛ', ' ', 'ржП', 'ржЦ', 'ржи', 'ржУ', 'рзЛ', ' ', 'ржЖ', 'рж▓', 'ржм', 'рж╛', 'рж░', 'рзН', 'ржд', 'рзЛ', 'рзЗ', ' ', 'рж╕', 'рж╛', 'ржз', 'рзБ', 'рзБ', ' ', 'ржХ', 'рж╛', 'рж╛', 'рж╛', 'рзА']

clean_text = bkit.transform.normalize_kar_ambiguity(text)
print(clean_text, list(clean_text))
# >>> ржЕржВрж╢ржЗ ржЕржВрж╢ржЧрзНрж░рж╣ржгржЗ ржЖрж░рзЛ ржПржЦржиржУ ржЖрж▓ржмрж╛рж░рзНрждрзЛ рж╕рж╛ржзрзБ ржХрж╛ ['ржЕ', 'ржВ', 'рж╢', 'ржЗ', ' ', 'ржЕ', 'ржВ', 'рж╢', 'ржЧ', 'рзН', 'рж░', 'рж╣', 'ржг', 'ржЗ', ' ', 'ржЖ', 'рж░', 'рзЛ', ' ', 'ржП', 'ржЦ', 'ржи', 'ржУ', ' ', 'ржЖ', 'рж▓', 'ржм', 'рж╛', 'рж░', 'рзН', 'ржд', 'рзЛ', ' ', 'рж╕', 'рж╛', 'ржз', 'рзБ', ' ', 'ржХ', 'рж╛']
```

#### Clean text<a id="clean-text"></a>

Clean text using the following steps sequentially:

<!-- no toc -->
1. [Removes all HTML tags](#html-tags)
2. [Removes all URLs](#urls)
3. [Removes all emojis (optional)](#emojis)
4. [Removes all digits (optional)](#clean-digits)
5. [Removes all punctuations (optional)](#clean-punctuations)
6. [Removes all extra spaces](#multiple-spaces)
7. [Removes all non bangla characters](#non-bangla-characters)

```python
import bkit

text = '<a href=some_URL>ржмрж╛ржВрж▓рж╛ржжрзЗрж╢</a>\nржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░   ржЖрзЯрждржи рзз.рзкрзн рж▓ржХрзНрж╖ ржХрж┐рж▓рзЛржорж┐ржЯрж╛рж░!!!'

clean_text = bkit.transform.clean_text(text)
print(clean_text)
# >>> ржмрж╛ржВрж▓рж╛ржжрзЗрж╢ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ ржЖрзЯрждржи рж▓ржХрзНрж╖ ржХрж┐рж▓рзЛржорж┐ржЯрж╛рж░
```

#### Clean punctuations<a id="clean-punctuations"></a>

Remove punctuations with the given `replace_with` character/string.

```python
import bkit

text = 'ржЖржорж░рж╛ ржорж╛ржарзЗ ржлрзБржЯржмрж▓ ржЦрзЗрж▓рждрзЗ ржкржЫржирзНржж ржХрж░рж┐!'

clean_text = bkit.transform.clean_punctuations(text)
print(clean_text)
# >>> ржЖржорж░рж╛ ржорж╛ржарзЗ ржлрзБржЯржмрж▓ ржЦрзЗрж▓рждрзЗ ржкржЫржирзНржж ржХрж░рж┐

clean_text = bkit.transform.clean_punctuations(text, replace_with=' PUNC ')
print(clean_text)
# >>> ржЖржорж░рж╛ ржорж╛ржарзЗ ржлрзБржЯржмрж▓ ржЦрзЗрж▓рждрзЗ ржкржЫржирзНржж ржХрж░рж┐ PUNC
```

#### Clean digits<a id="clean-digits"></a>

Remove any bangla digit from text by replacing with the given `replace_with` character/string.

```python
import bkit

text = 'рждрж╛рж░ ржмрж╛рж╕рж╛ рзнрзп ржирж╛ржорзНржмрж╛рж░ рж░рзЛржбрзЗред'

clean_text = bkit.transform.clean_digits(text)
print(clean_text)
# >>> рждрж╛рж░ ржмрж╛рж╕рж╛    ржирж╛ржорзНржмрж╛рж░ рж░рзЛржбрзЗред

clean_text = bkit.transform.clean_digits(text, replace_with='#')
print(clean_text)
# >>> рждрж╛рж░ ржмрж╛рж╕рж╛ ## ржирж╛ржорзНржмрж╛рж░ рж░рзЛржбрзЗред
```

#### Multiple spaces<a id="multiple-spaces"></a>

Clean multiple consecutive whitespace characters including space, newlines, tabs, vertical tabs, etc. It also removes leading and trailing whitespace characters.

```python
import bkit

text = 'рждрж╛рж░ ржмрж╛рж╕рж╛ рзнрзп   \t\t ржирж╛ржорзНржмрж╛рж░   рж░рзЛржбрзЗред\nрж╕рзЗ ржЦрзБржм \v ржнрж╛рж▓рзЛ ржЫрзЗрж▓рзЗред'

clean_text = bkit.transform.clean_multiple_spaces(text)
print(clean_text)
# >>> рждрж╛рж░ ржмрж╛рж╕рж╛ рзнрзп ржирж╛ржорзНржмрж╛рж░ рж░рзЛржбрзЗред рж╕рзЗ ржЦрзБржм ржнрж╛рж▓рзЛ ржЫрзЗрж▓рзЗред

clean_text = bkit.transform.clean_multiple_spaces(text, keep_new_line=True)
print(clean_text)
# >>> рждрж╛рж░ ржмрж╛рж╕рж╛ рзнрзп ржирж╛ржорзНржмрж╛рж░ рж░рзЛржбрзЗред\nрж╕рзЗ ржЦрзБржм \n ржнрж╛рж▓рзЛ ржЫрзЗрж▓рзЗред
```

#### URLs<a id="urls"></a>

Clean URLs from text and replace the URLs with any given string.

```python
import bkit

text = 'ржЖржорж┐ https://xyz.abc рж╕рж╛ржЗржЯрзЗ ржмрзНрж▓ржЧ рж▓рж┐ржЦрж┐ред ржПржЗ ftp://10.17.5.23/books рж╕рж╛рж░рзНржнрж╛рж░ ржерзЗржХрзЗ ржЖржорж╛рж░ ржмржЗржЧрзБрж▓рзЛ ржкрж╛ржмрзЗред ржПржЗ https://bn.wikipedia.org/wiki/%E0%A6%A7%E0%A6%BE%E0%A6%A4%E0%A7%81_(%E0%A6%AC%E0%A6%BE%E0%A6%82%E0%A6%B2%E0%A6%BE_%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%BE%E0%A6%95%E0%A6%B0%E0%A6%A3) рж▓рж┐ржЩрзНржХржЯрж┐рждрзЗ ржнрж╛рж▓рзЛ рждржерзНржп ржЖржЫрзЗред'

clean_text = bkit.transform.clean_urls(text)
print(clean_text)
# >>> ржЖржорж┐   рж╕рж╛ржЗржЯрзЗ ржмрзНрж▓ржЧ рж▓рж┐ржЦрж┐ред ржПржЗ   рж╕рж╛рж░рзНржнрж╛рж░ ржерзЗржХрзЗ ржЖржорж╛рж░ ржмржЗржЧрзБрж▓рзЛ ржкрж╛ржмрзЗред ржПржЗ   рж▓рж┐ржЩрзНржХржЯрж┐рждрзЗ ржнрж╛рж▓рзЛ рждржерзНржп ржЖржЫрзЗред

clean_text = bkit.transform.clean_urls(text, replace_with='URL')
print(clean_text)
# >>> ржЖржорж┐ URL рж╕рж╛ржЗржЯрзЗ ржмрзНрж▓ржЧ рж▓рж┐ржЦрж┐ред ржПржЗ URL рж╕рж╛рж░рзНржнрж╛рж░ ржерзЗржХрзЗ ржЖржорж╛рж░ ржмржЗржЧрзБрж▓рзЛ ржкрж╛ржмрзЗред ржПржЗ URL рж▓рж┐ржЩрзНржХржЯрж┐рждрзЗ ржнрж╛рж▓рзЛ рждржерзНржп ржЖржЫрзЗред
```

#### Emojis<a id="emojis"></a>

Clean emoji and emoticons from text and replace those with any given string.

```python
import bkit

text = 'ржХрж┐ржЫрзБ ржЗржорзЛржЬрж┐ рж╣рж▓: ЁЯШАЁЯлЕЁЯП╛ЁЯлЕЁЯП┐ЁЯлГЁЯП╝ЁЯлГЁЯП╜ЁЯлГЁЯП╛ЁЯлГЁЯП┐ЁЯлДЁЯлДЁЯП╗ЁЯлДЁЯП╝ЁЯлДЁЯП╜ЁЯлДЁЯП╛ЁЯлДЁЯП┐ЁЯзМЁЯк╕ЁЯк╖ЁЯк╣ЁЯк║ЁЯлШЁЯлЧЁЯлЩЁЯЫЭЁЯЫЮЁЯЫЯЁЯкмЁЯкйЁЯклЁЯй╝ЁЯй╗ЁЯлзЁЯккЁЯЯ░'

clean_text = bkit.transform.clean_emojis(text, replace_with='<EMOJI>')
print(clean_text)
# >>> ржХрж┐ржЫрзБ ржЗржорзЛржЬрж┐ рж╣рж▓: <EMOJI>
```

#### HTML tags<a id="html-tags"></a>

Clean HTML tags from text and replace those with any given string.

```python
import bkit

text = '<a href=some_URL>ржмрж╛ржВрж▓рж╛ржжрзЗрж╢</a>'

clean_text = bkit.transform.clean_html(text)
print(clean_text)
# >>> ржмрж╛ржВрж▓рж╛ржжрзЗрж╢
```

#### Multiple punctuations<a id="multiple-punctuations"></a>

Remove multiple consecutive punctuations and keep the first punctuation only.

```python
import bkit

text = 'ржХрж┐ ржЖржиржирзНржж!!!!!'

clean_text = bkit.transform.clean_multiple_punctuations(text)
print(clean_text)
# >>> ржХрж┐ ржЖржиржирзНржж!
```

#### Special characters<a id="special-characters"></a>

Remove special characters like `$`, `#`, `@`, etc and replace them with the given string. If no character list is passed, `[$, #,  &, %, @]` are removed by default.

```python
import bkit

text = '#ржмрж╛ржВрж▓рж╛ржжрзЗрж╢$'

clean_text = bkit.transform.clean_special_characters(text, characters=['#', '$'], replace_with='')
print(clean_text)
# >>> ржмрж╛ржВрж▓рж╛ржжрзЗрж╢
```

#### Non Bangla characters<a id="non-bangla-characters"></a>

Non Bangla characters include characters and punctuation not used in Bangla like english or other language's alphabets  and replace them with the given string.

```python
import bkit

text = 'ржПржЗ рж╢рзВржХржХрзАржЯ рж╣рж╛рждрж┐рж╢рзБржБржбрж╝ Heliotropium indicum, ржЕрждрж╕рзА, ржЖржХржирзНржж Calotropis gigantea ржЧрж╛ржЫрзЗрж░ ржкрж╛рждрж╛рж░ рж░рж╕рж╛рж▓рзЛ ржЕржВрж╢ ржЖрж╣рж╛рж░ ржХрж░рзЗред'

clean_text = bkit.transform.clean_non_bangla(text, replace_with='')
print(clean_text)
# >>> ржПржЗ рж╢рзВржХржХрзАржЯ рж╣рж╛рждрж┐рж╢рзБржБржбрж╝  , ржЕрждрж╕рзА, ржЖржХржирзНржж  ржЧрж╛ржЫрзЗрж░ ржкрж╛рждрж╛рж░ рж░рж╕рж╛рж▓рзЛ ржЕржВрж╢ ржЖрж╣рж╛рж░ ржХрж░рзЗ
```

### Text Analysis<a id="text-analysis"></a>

#### Word count<a id="word-count"></a>

The `bkit.analysis.count_words` function can be used to get the word counts. It has the following paramerts:

```python
"""
Args:
  text (Tuple[str, List[str]]): The text to count words from. If a string is provided,
    it will be split into words. If a list of strings is provided, each string will
    be split into words and counted separately.
  clean_punctuation (bool, optional): Whether to clean punctuation from the words count. Defaults to False.
  punct_replacement (str, optional): The replacement for the punctuation. Only applicable if
    clean_punctuation is True. Defaults to "".
  return_dict (bool, optional): Whether to return the word count as a dictionary.
    Defaults to False.
  ordered (bool, optional): Whether to return the word count in descending order. Only
    applicable if return_dict is True. Defaults to False.

Returns:
  Tuple[int, Dict[str, int]]: If return_dict is True, returns a tuple containing the
    total word count and a dictionary where the keys are the words and the values
    are their respective counts. If return_dict is False, returns only the total
    word count as an integer.
"""

# examples

import bkit

text='ржЕржнрж┐рж╖рзЗржХрзЗрж░ ржЖржЧрзЗрж░ ржжрж┐ржи ржЧрждржХрж╛рж▓ рж░рзЛржмржмрж╛рж░ ржУрзЯрж╛рж╢рж┐ржВржЯржирзЗ ржмрж┐рж╢рж╛рж▓ ржПржХ рж╕ржорж╛ржмрзЗрж╢рзЗ рж╣рж╛ржЬрж┐рж░ рж╣ржи ржЯрзНрж░рж╛ржорзНржкред рждрж┐ржирж┐ ржЙржЪрзНржЫрзНржмрж╕рж┐ржд ржнржХрзНржд-рж╕ржорж░рзНржержХржжрзЗрж░ ржЖржорзЗрж░рж┐ржХрж╛рж░ ржкрждржирзЗрж░ ржпржмржирж┐ржХрж╛ ржШржЯрж╛ржирзЛрж░ ржЕржЩрзНржЧрзАржХрж╛рж░ ржХрж░рзЗржиред'
total_words=bkit.analysis.count_words(text)
print(total_words)
# >>> 21

```

#### Sentence Count<a id="sentence-count"></a>

The bkit.analysis.count_sentences function can be used to get the word counts. It has the following paramerts:

```python
"""
Counts the number of sentences in the given text or list of texts.

Args:
  text (Tuple[str, List[str]]): The text or list of texts to count sentences from.
  return_dict (bool, optional): Whether to return the result as a dictionary. Defaults to False.
  ordered (bool, optional): Whether to order the result in descending order.
    Only applicable if return_dict is True. Defaults to False.

Returns:
  int or dict: The count of sentences. If return_dict is True, returns a dictionary with sentences as keys
    and their counts as values. If return_dict is False, returns the total count of sentences.

Raises:
  AssertionError: If ordered is True but return_dict is False.
"""

# examples
import bkit

text = 'рждрзБржорж┐ ржХрзЛржерж╛рзЯ ржерж╛ржХ? ржврж╛ржХрж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░\n рж░рж╛ржЬржзрж╛ржирзАред ржХрж┐ ржЕржмрж╕рзНржерж╛ рждрж╛рж░! рззрзи/рзжрзй/рзирзжрзирзи рждрж╛рж░рж┐ржЦрзЗ рж╕рзЗ рзк/ржХ ржарж┐ржХрж╛ржирж╛рзЯ ржЧрж┐рзЯрзЗ рззрзи,рзйрзкрзл.рзирзй ржЯрж╛ржХрж╛ ржжрж┐рзЯрзЗржЫрж┐рж▓ред'

count = bkit.analysis.count_sentences(text)
print(count)
# >>> 5

count = bkit.analysis.count_sentences(text, return_dict=True, ordered=True)
print(count)
# >>> {'рждрзБржорж┐ ржХрзЛржерж╛рзЯ ржерж╛ржХ?': 1, 'ржврж╛ржХрж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░\n': 1, 'рж░рж╛ржЬржзрж╛ржирзАред': 1, 'ржХрж┐ ржЕржмрж╕рзНржерж╛ рждрж╛рж░!': 1, 'рззрзи/рзжрзй/рзирзжрзирзи рждрж╛рж░рж┐ржЦрзЗ рж╕рзЗ рзк/ржХ ржарж┐ржХрж╛ржирж╛рзЯ ржЧрж┐рзЯрзЗ рззрзи,рзйрзкрзл.рзирзй ржЯрж╛ржХрж╛ ржжрж┐рзЯрзЗржЫрж┐рж▓ред': 1}
```

### Lemmatization<a id="lemmatization"></a>
Lemmatization is implemented based on our this paper **BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer**  

[For more details](https://arxiv.org/pdf/2311.03078)
#### Lemmatize text<a id="lemmatize-text"></a>

Lemmatize a given text. Generally expects the text to be a sentence.

```python
import bkit

text = 'ржкрзГржерж┐ржмрзАрж░ ржЬржирж╕ржВржЦрзНржпрж╛ рзо ржмрж┐рж▓рж┐рзЯржирзЗрж░ ржХрж┐ржЫрзБ ржХржо'

lemmatized = bkit.lemmatizer.lemmatize(text)

print(lemmatized)
# >>> ржкрзГржерж┐ржмрзА ржЬржирж╕ржВржЦрзНржпрж╛ рзо ржмрж┐рж▓рж┐рзЯржи ржХрж┐ржЫрзБ ржХржо
```

#### Lemmatize word<a id="lemmatize-word"></a>

Lemmatize a word given the PoS information.

```python
import bkit

text = 'ржкрзГржерж┐ржмрзАрж░'

lemmatized = bkit.lemmatizer.lemmatize_word(text, 'noun')

print(lemmatized)
# >>> ржкрзГржерж┐ржмрзА
```

### Stemmer<a id="stemmer"></a>

Stemming is the process of reducing words to their base or root form. Our implementation achieves this by conditionally stripping away predefined prefixes and suffixes from each word.

#### Stem word<a id="stem-word"></a>
```python
import bkit

stemmer = bkit.stemmer.SimpleStemmer()
stemmer.word_stemer('ржиржЧрж░ржмрж╛рж╕рзА')
# >>> ржиржЧрж░
```
#### Stem Sentence<a id="stem-sentence"></a>
```python
import bkit

stemmer = bkit.stemmer.SimpleStemmer()
stemmer.sentence_stemer('ржмрж┐ржХрзЗрж▓рзЗ рж░рзЛржж ржХрж┐ржЫрзБржЯрж╛ ржХржорзЗржЫрзЗред')
# >>> ржмрж┐ржХрзЗрж▓ рж░рзЛржж ржХрж┐ржЫрзБ ржХржо
```


### Tokenization<a id="tokenization"></a>

Tokenize a given text. The `bkit.tokenizer` module is used to tokenizer text into tokens. It supports three types of tokenization.

#### Word tokenization<a id="word-tokenization"></a>

Tokenize text into words. Also separates some punctuations including comma, danda (ред), question mark, etc.

```python
import bkit

text = 'рждрзБржорж┐ ржХрзЛржерж╛рзЯ ржерж╛ржХ? ржврж╛ржХрж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж░рж╛ржЬржзрж╛ржирзАред ржХрж┐ ржЕржмрж╕рзНржерж╛ рждрж╛рж░! рззрзи/рзжрзй/рзирзжрзирзи рждрж╛рж░рж┐ржЦрзЗ рж╕рзЗ рзк/ржХ ржарж┐ржХрж╛ржирж╛рзЯ ржЧрж┐рзЯрзЗ рззрзи,рзйрзкрзл ржЯрж╛ржХрж╛ ржжрж┐рзЯрзЗржЫрж┐рж▓ред'

tokens = bkit.tokenizer.tokenize(text)

print(tokens)
# >>> ['рждрзБржорж┐', 'ржХрзЛржерж╛рзЯ', 'ржерж╛ржХ', '?', 'ржврж╛ржХрж╛', 'ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░', 'рж░рж╛ржЬржзрж╛ржирзА', 'ред', 'ржХрж┐', 'ржЕржмрж╕рзНржерж╛', 'рждрж╛рж░', '!', 'рззрзи/рзжрзй/рзирзжрзирзи', 'рждрж╛рж░рж┐ржЦрзЗ', 'рж╕рзЗ', 'рзк/ржХ', 'ржарж┐ржХрж╛ржирж╛рзЯ', 'ржЧрж┐рзЯрзЗ', 'рззрзи,рзйрзкрзл', 'ржЯрж╛ржХрж╛', 'ржжрж┐рзЯрзЗржЫрж┐рж▓', 'ред']
```

#### Word and Punctuation tokenization<a id="word-and-punctuation-tokenization"></a>

Tokenize text into words and any punctuation.

```python
import bkit

text = 'рждрзБржорж┐ ржХрзЛржерж╛рзЯ ржерж╛ржХ? ржврж╛ржХрж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж░рж╛ржЬржзрж╛ржирзАред ржХрж┐ ржЕржмрж╕рзНржерж╛ рждрж╛рж░! рззрзи/рзжрзй/рзирзжрзирзи рждрж╛рж░рж┐ржЦрзЗ рж╕рзЗ рзк/ржХ ржарж┐ржХрж╛ржирж╛рзЯ ржЧрж┐рзЯрзЗ рззрзи,рзйрзкрзл ржЯрж╛ржХрж╛ ржжрж┐рзЯрзЗржЫрж┐рж▓ред'

tokens = bkit.tokenizer.tokenize_word_punctuation(text)

print(tokens)
# >>> ['рждрзБржорж┐', 'ржХрзЛржерж╛рзЯ', 'ржерж╛ржХ', '?', 'ржврж╛ржХрж╛', 'ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░', 'рж░рж╛ржЬржзрж╛ржирзА', 'ред', 'ржХрж┐', 'ржЕржмрж╕рзНржерж╛', 'рждрж╛рж░', '!', 'рззрзи', '/', 'рзжрзй', '/', 'рзирзжрзирзи', 'рждрж╛рж░рж┐ржЦрзЗ', 'рж╕рзЗ', 'рзк', '/', 'ржХ', 'ржарж┐ржХрж╛ржирж╛рзЯ', 'ржЧрж┐рзЯрзЗ', 'рззрзи', ',', 'рзйрзкрзл', 'ржЯрж╛ржХрж╛', 'ржжрж┐рзЯрзЗржЫрж┐рж▓', 'ред']
```

#### Sentence tokenization<a id="sentence-tokenization"></a>

Tokenize text into sentences.

```python
import bkit

text = 'рждрзБржорж┐ ржХрзЛржерж╛рзЯ ржерж╛ржХ? ржврж╛ржХрж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж░рж╛ржЬржзрж╛ржирзАред ржХрж┐ ржЕржмрж╕рзНржерж╛ рждрж╛рж░! рззрзи/рзжрзй/рзирзжрзирзи рждрж╛рж░рж┐ржЦрзЗ рж╕рзЗ рзк/ржХ ржарж┐ржХрж╛ржирж╛рзЯ ржЧрж┐рзЯрзЗ рззрзи,рзйрзкрзл ржЯрж╛ржХрж╛ ржжрж┐рзЯрзЗржЫрж┐рж▓ред'

tokens = bkit.tokenizer.tokenize_sentence(text)

print(tokens)
# >>> ['рждрзБржорж┐ ржХрзЛржерж╛рзЯ ржерж╛ржХ?', 'ржврж╛ржХрж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж░рж╛ржЬржзрж╛ржирзАред', 'ржХрж┐ ржЕржмрж╕рзНржерж╛ рждрж╛рж░!', 'рззрзи/рзжрзй/рзирзжрзирзи рждрж╛рж░рж┐ржЦрзЗ рж╕рзЗ рзк/ржХ ржарж┐ржХрж╛ржирж╛рзЯ ржЧрж┐рзЯрзЗ рззрзи,рзйрзкрзл ржЯрж╛ржХрж╛ ржжрж┐рзЯрзЗржЫрж┐рж▓ред']
```

### Named Entity Recognition (NER)<a id="named-entity-recognition-ner"></a>

Predicts the tags of the Named Entities of a given text.

```python
import bkit

text = 'рждрзБржорж┐ ржХрзЛржерж╛рзЯ ржерж╛ржХ? ржврж╛ржХрж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж░рж╛ржЬржзрж╛ржирзАред ржХрж┐ ржЕржмрж╕рзНржерж╛ рждрж╛рж░! рззрзи/рзжрзй/рзирзжрзирзи рждрж╛рж░рж┐ржЦрзЗ рж╕рзЗ рзк/ржХ ржарж┐ржХрж╛ржирж╛рзЯ ржЧрж┐рзЯрзЗ рззрзи,рзйрзкрзл.рзирзй ржЯрж╛ржХрж╛ ржжрж┐рзЯрзЗржЫрж┐рж▓ред'

ner = bkit.ner.Infer('ner-noisy-label')
predictions = ner(text)

print(predictions)
# >>> [('рждрзБржорж┐', 'O', 0.9998692), ('ржХрзЛржерж╛рзЯ', 'O', 0.99988306), ('ржерж╛ржХ?', 'O', 0.99983954), ('ржврж╛ржХрж╛', 'B-GPE', 0.99891424), ('ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░', 'B-GPE', 0.99710876), ('рж░рж╛ржЬржзрж╛ржирзАред', 'O', 0.9995414), ('ржХрж┐', 'O', 0.99989176), ('ржЕржмрж╕рзНржерж╛', 'O', 0.99980336), ('рждрж╛рж░!', 'O', 0.99983263), ('рззрзи/рзжрзй/рзирзжрзирзи', 'B-D&T', 0.97921854), ('рждрж╛рж░рж┐ржЦрзЗ', 'O', 0.9271435), ('рж╕рзЗ', 'O', 0.99934834), ('рзк/ржХ', 'B-NUM', 0.8297553), ('ржарж┐ржХрж╛ржирж╛рзЯ', 'O', 0.99728775), ('ржЧрж┐рзЯрзЗ', 'O', 0.9994825), ('рззрзи,рзйрзкрзл.рзирзй', 'B-NUM', 0.99740463), ('ржЯрж╛ржХрж╛', 'B-UNIT', 0.99914896), ('ржжрж┐рзЯрзЗржЫрж┐рж▓ред', 'O', 0.9998908)]
```

#### Named Entity Recognition (NER) Visualization<a id="named-entity-recognition-ner-visualization"></a>
It takes the model's output and visualizes the NER tag for every word in the text.

```python
import bkit

text = 'рждрзБржорж┐ ржХрзЛржерж╛рзЯ ржерж╛ржХ? ржврж╛ржХрж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж░рж╛ржЬржзрж╛ржирзАред ржХрж┐ ржЕржмрж╕рзНржерж╛ рждрж╛рж░! рззрзи/рзжрзй/рзирзжрзирзи рждрж╛рж░рж┐ржЦрзЗ рж╕рзЗ рзк/ржХ ржарж┐ржХрж╛ржирж╛рзЯ ржЧрж┐рзЯрзЗ рззрзи,рзйрзкрзл.рзирзй ржЯрж╛ржХрж╛ ржжрж┐рзЯрзЗржЫрж┐рж▓ред'
ner = bkit.ner.Infer('ner-noisy-label')
predictions = ner(text)
bkit.ner.visualize(predictions)
```
[![NER.png](https://i.postimg.cc/Zq2RrF0z/NER.png)](https://postimg.cc/SXLk49Wg)


### Parts of Speech (PoS) tagging<a id="parts-of-speech-pos-tagging"></a>

Predicts the tags of the parts of speech of a given text.

```python
import bkit

text = 'ржЧржд ржХрж┐ржЫрзБржжрж┐ржи ржзрж░рзЗржЗ ржЬрзНржмрж╛рж▓рж╛ржирж┐рж╣рзАржи ржЕржмрж╕рзНржерж╛рзЯ ржПржХржЯрж┐ ржЫрзЛржЯ ржорж╛ржЫ ржзрж░рж╛рж░ ржирзМржХрж╛рзЯ рззрзлрзж ржЬржи рж░рзЛрж╣рж┐ржЩрзНржЧрж╛ ржЖржирзНржжрж╛ржорж╛ржи рж╕рж╛ржЧрж░рзЗ ржнрж╛рж╕ржорж╛ржи ржЕржмрж╕рзНржерж╛рзЯ рж░рзЯрзЗржЫрзЗ ред'
pos = bkit.pos.Infer('pos-noisy-label')
predictions = pos(text)

print(predictions)
# >>> [('ржЧржд', 'ADJ', 0.98674506), ('ржХрж┐ржЫрзБржжрж┐ржи', 'NNC', 0.97954935), ('ржзрж░рзЗржЗ', 'PP', 0.96124), ('ржЬрзНржмрж╛рж▓рж╛ржирж┐рж╣рзАржи', 'ADJ', 0.93195957), ('ржЕржмрж╕рзНржерж╛рзЯ', 'NNC', 0.9960413), ('ржПржХржЯрж┐', 'QF', 0.9912915), ('ржЫрзЛржЯ', 'ADJ', 0.9810739), ('ржорж╛ржЫ', 'NNC', 0.97365385), ('ржзрж░рж╛рж░', 'NNC', 0.96641904), ('ржирзМржХрж╛рзЯ', 'NNC', 0.99680626), ('рззрзлрзж', 'QF', 0.996005), ('ржЬржи', 'NNC', 0.99434316), ('рж░рзЛрж╣рж┐ржЩрзНржЧрж╛', 'NNP', 0.9141038), ('ржЖржирзНржжрж╛ржорж╛ржи', 'NNP', 0.9856694), ('рж╕рж╛ржЧрж░рзЗ', 'NNP', 0.7122378), ('ржнрж╛рж╕ржорж╛ржи', 'ADJ', 0.93841994), ('ржЕржмрж╕рзНржерж╛рзЯ', 'NNC', 0.9965629), ('рж░рзЯрзЗржЫрзЗ', 'VF', 0.99680847), ('ред', 'PUNCT', 0.9963098)]

```
#### Parts of Speech (PoS) Visualization<a id="parts-of-speech-pos-visualization"></a>
"It takes the model's output and visualizes the Part-of-Speech tag for every word in the text.

```python
import bkit

text = 'ржЧржд ржХрж┐ржЫрзБржжрж┐ржи ржзрж░рзЗржЗ ржЬрзНржмрж╛рж▓рж╛ржирж┐рж╣рзАржи ржЕржмрж╕рзНржерж╛рзЯ ржПржХржЯрж┐ ржЫрзЛржЯ ржорж╛ржЫ ржзрж░рж╛рж░ ржирзМржХрж╛рзЯ рззрзлрзж ржЬржи рж░рзЛрж╣рж┐ржЩрзНржЧрж╛ ржЖржирзНржжрж╛ржорж╛ржи рж╕рж╛ржЧрж░рзЗ ржнрж╛рж╕ржорж╛ржи ржЕржмрж╕рзНржерж╛рзЯ рж░рзЯрзЗржЫрзЗ ред'
pos = bkit.pos.Infer('pos-noisy-label')
predictions = pos(text)
bkit.pos.visualize(predictions)
```
[![pos.png](https://i.postimg.cc/8CjcJnpb/pos.png)](https://postimg.cc/3yQYz1Dy)


### Shallow Parsing (Constituency Parsing)<a id="shallow-parsing-constituency-parsing"></a>

Predicts the shallow parsing tags of a given text.

```python
import bkit

text = 'рждрзБржорж┐ ржХрзЛржерж╛рзЯ ржерж╛ржХ? ржврж╛ржХрж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж░рж╛ржЬржзрж╛ржирзАред ржХрж┐ ржЕржмрж╕рзНржерж╛ рждрж╛рж░! рззрзи/рзжрзй/рзирзжрзирзи рждрж╛рж░рж┐ржЦрзЗ рж╕рзЗ рзк/ржХ ржарж┐ржХрж╛ржирж╛рзЯ ржЧрж┐рзЯрзЗ рззрзи,рзйрзкрзл.рзирзй ржЯрж╛ржХрж╛ ржжрж┐рзЯрзЗржЫрж┐рж▓ред'
shallow = bkit.shallow.Infer(pos_model='pos-noisy-label')
predictions = shallow(text)
print(predictions)
# >>> (S (VP (NP (PRO рждрзБржорж┐)) (VP (ADVP (ADV ржХрзЛржерж╛рзЯ)) (VF ржерж╛ржХ))) (NP (NNP ?) (NNP ржврж╛ржХрж╛) (NNC ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░)) (ADVP (ADV рж░рж╛ржЬржзрж╛ржирзА)) (NP (NP (NP (NNC ред)) (NP (PRO ржХрж┐))) (NP (QF ржЕржмрж╕рзНржерж╛) (NNC рждрж╛рж░)) (NP (PRO !))) (NP (NP (QF рззрзи/рзжрзй/рзирзжрзирзи) (NNC рждрж╛рж░рж┐ржЦрзЗ)) (VNF рж╕рзЗ) (NP (QF рзк/ржХ) (NNC ржарж┐ржХрж╛ржирж╛рзЯ))) (VF ржЧрж┐рзЯрзЗ))
```

#### Shallow Parsing Visualization<a id="shallow-parsing-visualization"></a>
It converts model predictions into an interactive shallow parsing Tree for clear and intuitive analysis

```python
from bkit import shallow
text = "ржХрж╛рждрж╛рж░ ржмрж┐рж╢рзНржмржХрж╛ржкрзЗ ржЖрж░рзНржЬрзЗржирзНржЯрж┐ржирж╛рж░ ржмрж┐рж╢рзНржмржХрж╛ржк ржЬрзЯрзЗ ржорж╛рж░рзНрждрж┐ржирзЗржЬрзЗрж░ ржЕржмржжрж╛ржи ржЕржирзЗржХред"
shallow = shallow.Infer(pos_model='pos-noisy-label')
predictions = shallow(text)
shallow.visualize(predictions)
```

[![shallow.png](https://i.postimg.cc/sgfvygbq/shallow.png)](https://postimg.cc/vcjQtbYt)


### Dependency Parsing<a id="dependency-parsing"></a>

Predicts the dependency parsing tags of a given text.

```python
from bkit import dependency

text = "ржХрж╛рждрж╛рж░ ржмрж┐рж╢рзНржмржХрж╛ржкрзЗ ржЖрж░рзНржЬрзЗржирзНржЯрж┐ржирж╛рж░ ржмрж┐рж╢рзНржмржХрж╛ржк ржЬрзЯрзЗ ржорж╛рж░рзНрждрж┐ржирзЗржЬрзЗрж░ ржЕржмржжрж╛ржи ржЕржирзЗржХред"
dep =dependency.Infer('dependency-parsing')
predictions = dep(text)
print(predictions)
# >>>[{'text': 'ржХрж╛рждрж╛рж░ ржмрж┐рж╢рзНржмржХрж╛ржкрзЗ ржЖрж░рзНржЬрзЗржирзНржЯрж┐ржирж╛рж░ ржмрж┐рж╢рзНржмржХрж╛ржк ржЬрзЯрзЗ ржорж╛рж░рзНрждрж┐ржирзЗржЬрзЗрж░ ржЕржмржжрж╛ржи ржЕржирзЗржХ ред', 'predictions': [{'token_start': 1, 'token_end': 0, 'label': 'compound'}, {'token_start': 7, 'token_end': 1, 'label': 'obl'}, {'token_start': 4, 'token_end': 2, 'label': 'nmod'}, {'token_start': 4, 'token_end': 3, 'label': 'nmod'}, {'token_start': 7, 'token_end': 4, 'label': 'obl'}, {'token_start': 6, 'token_end': 5, 'label': 'nmod'}, {'token_start': 7, 'token_end': 6, 'label': 'nsubj'}, {'token_start': 7, 'token_end': 7, 'label': 'root'}, {'token_start': 7, 'token_end': 8, 'label': 'punct'}]}]
```

#### Dependency Parsing Visualization<a id="dependency-parsing-visualization"></a>
It converts model predictions into an interactive dependency graph for clear and intuitive analysis

```python
from bkit import dependency
text = "ржХрж╛рждрж╛рж░ ржмрж┐рж╢рзНржмржХрж╛ржкрзЗ ржЖрж░рзНржЬрзЗржирзНржЯрж┐ржирж╛рж░ ржмрж┐рж╢рзНржмржХрж╛ржк ржЬрзЯрзЗ ржорж╛рж░рзНрждрж┐ржирзЗржЬрзЗрж░ ржЕржмржжрж╛ржи ржЕржирзЗржХред"
dep = dependency.Infer('dependency-parsing')
predictions = dep(text)
dependency.visualize(predictions)
```

[![dependency-visu.png](https://i.postimg.cc/MpsXG8ST/dependency-visu.png)](https://postimg.cc/K1Mm9w4S)

### Coreference Resolution<a id="coref-resolution"></a>

Predicts the coreferent clusters of a given text.

```python
import bkit

text = "рждрж╛рж░рж╛рж╕рзБржирзНржжрж░рзА ( рззрзорзнрзо - рззрзпрзкрзо ) ржЕржнрж┐ржирзЗрждрзНрж░рзА ред рззрзорзорзк рж╕рж╛рж▓рзЗ ржмрж┐ржирзЛржжрж┐ржирзАрж░ рж╕рж╣рж╛ржпрж╝рждрж╛ржпрж╝ рж╕рзНржЯрж╛рж░ ржерж┐ржпрж╝рзЗржЯрж╛рж░рзЗ ржпрзЛржЧржжрж╛ржирзЗрж░ ржорж╛ржзрзНржпржорзЗ рждрж┐ржирж┐ ржЕржнрж┐ржиржпрж╝ рж╢рзБрж░рзБ ржХрж░рзЗржи ред ржкрзНрж░ржержорзЗ рждрж┐ржирж┐ ржЧрж┐рж░рж┐рж╢ржЪржирзНржжрзНрж░ ржШрзЛрж╖рзЗрж░ ржЪрзИрждржирзНржпрж▓рзАрж▓рж╛ ржирж╛ржЯржХрзЗ ржПржХ ржмрж╛рж▓ржХ ржУ рж╕рж░рж▓рж╛ ржирж╛ржЯржХрзЗ ржЧрзЛржкрж╛рж▓ ржЪрж░рж┐рждрзНрж░рзЗ ржЕржнрж┐ржиржпрж╝ ржХрж░рзЗржи ред"
coref = bkit.coref.Infer('coref')
predictions = coref(text)
print(predictions)
# >>> {'text': ['рждрж╛рж░рж╛рж╕рзБржирзНржжрж░рзА', '(', 'рззрзорзнрзо', '-', 'рззрзпрзкрзо', ')', 'ржЕржнрж┐ржирзЗрждрзНрж░рзА', 'ред', 'рззрзорзорзк', 'рж╕рж╛рж▓рзЗ', 'ржмрж┐ржирзЛржжрж┐ржирзАрж░', 'рж╕рж╣рж╛ржпрж╝рждрж╛ржпрж╝', 'рж╕рзНржЯрж╛рж░', 'ржерж┐ржпрж╝рзЗржЯрж╛рж░рзЗ', 'ржпрзЛржЧржжрж╛ржирзЗрж░', 'ржорж╛ржзрзНржпржорзЗ', 'рждрж┐ржирж┐', 'ржЕржнрж┐ржиржпрж╝', 'рж╢рзБрж░рзБ', 'ржХрж░рзЗржи', 'ред', 'ржкрзНрж░ржержорзЗ', 'рждрж┐ржирж┐', 'ржЧрж┐рж░рж┐рж╢ржЪржирзНржжрзНрж░', 'ржШрзЛрж╖рзЗрж░', 'ржЪрзИрждржирзНржпрж▓рзАрж▓рж╛', 'ржирж╛ржЯржХрзЗ', 'ржПржХ', 'ржмрж╛рж▓ржХ', 'ржУ', 'рж╕рж░рж▓рж╛', 'ржирж╛ржЯржХрзЗ', 'ржЧрзЛржкрж╛рж▓', 'ржЪрж░рж┐рждрзНрж░рзЗ', 'ржЕржнрж┐ржиржпрж╝', 'ржХрж░рзЗржи', 'ред'], 'mention_indices': {0: [{'start_token': 0, 'end_token': 0}, {'start_token': 6, 'end_token': 6}, {'start_token': 10, 'end_token': 10}, {'start_token': 16, 'end_token': 16}, {'start_token': 22, 'end_token': 22}]}}
```

#### Coreference Resolution Visualization<a id="coreference-resolution-visualization"></a>
It takes the model's output and creates an interactive visualization to clearly depict coreference resolution, highlighting the relationships between entities in the text

```python
from bkit import coref

text = "рждрж╛рж░рж╛рж╕рзБржирзНржжрж░рзА ( рззрзорзнрзо - рззрзпрзкрзо ) ржЕржнрж┐ржирзЗрждрзНрж░рзА ред рззрзорзорзк рж╕рж╛рж▓рзЗ ржмрж┐ржирзЛржжрж┐ржирзАрж░ рж╕рж╣рж╛ржпрж╝рждрж╛ржпрж╝ рж╕рзНржЯрж╛рж░ ржерж┐ржпрж╝рзЗржЯрж╛рж░рзЗ ржпрзЛржЧржжрж╛ржирзЗрж░ ржорж╛ржзрзНржпржорзЗ рждрж┐ржирж┐ ржЕржнрж┐ржиржпрж╝ рж╢рзБрж░рзБ ржХрж░рзЗржи ред ржкрзНрж░ржержорзЗ рждрж┐ржирж┐ ржЧрж┐рж░рж┐рж╢ржЪржирзНржжрзНрж░ ржШрзЛрж╖рзЗрж░ ржЪрзИрждржирзНржпрж▓рзАрж▓рж╛ ржирж╛ржЯржХрзЗ ржПржХ ржмрж╛рж▓ржХ ржУ рж╕рж░рж▓рж╛ ржирж╛ржЯржХрзЗ ржЧрзЛржкрж╛рж▓ ржЪрж░рж┐рждрзНрж░рзЗ ржЕржнрж┐ржиржпрж╝ ржХрж░рзЗржи ред"
coref = coref.Infer('coref')
predictions = coref(text)
coref.visualize(predictions)
```

[![coref.png](https://i.postimg.cc/26W5TbBb/coref.png)](https://postimg.cc/cgsZLJh0)




