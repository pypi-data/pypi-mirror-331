{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9785cb1-f65d-42b6-95e5-d09fe0eff8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogen\n",
      "  Downloading autogen-0.7.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pyautogen==0.7.5 (from autogen)\n",
      "  Downloading pyautogen-0.7.5-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting asyncer==0.0.8 (from pyautogen==0.7.5->autogen)\n",
      "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting diskcache (from pyautogen==0.7.5->autogen)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting docker (from pyautogen==0.7.5->autogen)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fast-depends<3,>=2.4.12 (from pyautogen==0.7.5->autogen)\n",
      "  Downloading fast_depends-2.4.12-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.7.5->autogen) (1.24.4)\n",
      "Collecting openai>=1.58 (from pyautogen==0.7.5->autogen)\n",
      "  Downloading openai-1.64.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.7.5->autogen) (23.2)\n",
      "Collecting pydantic<3,>=2.6.1 (from pyautogen==0.7.5->autogen)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting python-dotenv (from pyautogen==0.7.5->autogen)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting termcolor (from pyautogen==0.7.5->autogen)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tiktoken (from pyautogen==0.7.5->autogen)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.7 kB)\n",
      "Collecting websockets<15,>=14 (from pyautogen==0.7.5->autogen)\n",
      "  Downloading websockets-14.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.4.0 in /opt/conda/lib/python3.11/site-packages (from asyncer==0.0.8->pyautogen==0.7.5->autogen) (4.0.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.58->pyautogen==0.7.5->autogen)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.58->pyautogen==0.7.5->autogen)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.58->pyautogen==0.7.5->autogen)\n",
      "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai>=1.58->pyautogen==0.7.5->autogen) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai>=1.58->pyautogen==0.7.5->autogen) (4.66.1)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai>=1.58->pyautogen==0.7.5->autogen)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6.1->pyautogen==0.7.5->autogen)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6.1->pyautogen==0.7.5->autogen)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from docker->pyautogen==0.7.5->autogen) (2.31.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from docker->pyautogen==0.7.5->autogen) (2.0.7)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->pyautogen==0.7.5->autogen)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5.0,>=3.4.0->asyncer==0.0.8->pyautogen==0.7.5->autogen) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.5->autogen) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.5->autogen)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.5->autogen)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->docker->pyautogen==0.7.5->autogen) (3.3.0)\n",
      "Downloading autogen-0.7.5-py3-none-any.whl (12 kB)\n",
      "Downloading pyautogen-0.7.5-py3-none-any.whl (606 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
      "Downloading fast_depends-2.4.12-py3-none-any.whl (17 kB)\n",
      "Downloading openai-1.64.0-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-14.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (335 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.5/335.5 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.1/792.1 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: websockets, typing-extensions, termcolor, regex, python-dotenv, jiter, h11, distro, diskcache, annotated-types, tiktoken, pydantic-core, httpcore, docker, asyncer, pydantic, httpx, openai, fast-depends, pyautogen, autogen\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed annotated-types-0.7.0 asyncer-0.0.8 autogen-0.7.5 diskcache-5.6.3 distro-1.9.0 docker-7.1.0 fast-depends-2.4.12 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.64.0 pyautogen-0.7.5 pydantic-2.10.6 pydantic-core-2.27.2 python-dotenv-1.0.1 regex-2024.11.6 termcolor-2.5.0 tiktoken-0.9.0 typing-extensions-4.12.2 websockets-14.2\n"
     ]
    }
   ],
   "source": [
    "! pip install autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e6cbef-8ab5-4a4b-90e7-120adbf9f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "\n",
    "class DataOnboardingAgent:\n",
    "    data_onboarding_system_message = \"you are a helpful data processing agent\"\n",
    "\n",
    "    schema_inference_system_message = \"\"\"please generate DDL based on input data\n",
    "    here are the rules to follow\n",
    "    * the DDL grammar follows ClickHouse style\n",
    "    * the Table keyword MUST be replaced with Stream\n",
    "    * all datatypes MUST be in lowercase, such uint32\n",
    "    * all keywords MUST be in lowercase, such as nullable\n",
    "    * all field names MUST keep same as in the json\n",
    "    * composite types such as array, tuple, map cannot be nullable\n",
    "    * should use composite types like array, map or tuple to represent complex structure in the json\n",
    "    * if the data value is null, field type MUST be set as 'unknown'\n",
    "    * return the result as a markdown sql code\n",
    "\n",
    "\n",
    "    here is a sample of output DDL:\n",
    "    ```sql\n",
    "    CREATE STREAM car_live_data\n",
    "    (\n",
    "      `cid` string,\n",
    "      `gas_percent` float64,\n",
    "      `in_use` bool,\n",
    "      `latitude` float64,\n",
    "      `longitude` float64,\n",
    "      `locked` bool,\n",
    "      `speed_kmh` float64,\n",
    "      `time` string,\n",
    "      `total_km` float64\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    schema_to_table_system_message = \"\"\"based on generated DDL, please convert it into a json objec\n",
    "    Rules:\n",
    "    * for type string, it MUST be a single line for string\n",
    "    \n",
    "    for example, if the input DDL is:\n",
    "    CREATE STREAM car_live_data\n",
    "    (\n",
    "      `cid` string,\n",
    "      `gas_percent` float64,\n",
    "      `in_use` bool,\n",
    "      `composite` tuple(\n",
    "          'x' int\n",
    "          ),\n",
    "    )\n",
    "    \n",
    "    the output of the json description of the DDL should be:\n",
    "    ```json\n",
    "    \n",
    "    [\n",
    "        {\n",
    "            \"name\" : \"cid\", \"type\" : \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\" : \"gas_percent\", \"type\" : \"float64\"\n",
    "        },\n",
    "        {\n",
    "            \"name\" : \"in_use\", \"type\" : \"bool\"\n",
    "        },\n",
    "        {\n",
    "            \"name\" : \"composite\", \"type\" : \"tuple('x' int)\"\n",
    "        }\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    field_summary_system_message = \"\"\" please generate a report to explain each fields of the schema,\n",
    "    turn the hierachy into flatten when generating this report for each field,\n",
    "    use '.' to connect the parents and children names\n",
    "    output the result into a json object\n",
    "    here is a sample of the output:\n",
    "    [\n",
    "        {\n",
    "            \"name\": \"eventversion\",\n",
    "            \"type\": \"uint32\",\n",
    "            \"description\": \"The version of the current event.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"open_24h\",\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The price of the asset at the beginning of the last 24-hour period.\"\n",
    "        }\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    analysis_recommendations_system_message = \"\"\" please propose what kind an analysis we can do based on input data and schema\n",
    "    output into a json object which is an array\n",
    "    \"\"\"\n",
    "\n",
    "    analysis_sql_generation_system_message = \"\"\" please generate 10 analysis SQL based on input schema and analysis recommendations\n",
    "    output into a json object which is an array\n",
    "    note, you need escape newlines if the output contains multiple lines string\n",
    "\n",
    "    The generate SQL should follow these rules\n",
    "    * the SQL follows the ClickHouse grammar\n",
    "    * all method name MUST be in lower cases, following snake cases, for example : array_sum\n",
    "    * no CROSS JOIN is supported\n",
    "\n",
    "    As timeplus is a streaming processing platform, there are three different types of query regarding how to scan the data\n",
    "    please randomly select one of these three patterns to generate SQL\n",
    "\n",
    "    1 temperal window based analysis tumble window with 5 second window size\n",
    "    following query return analysis result in a continously streaming query for every 5 second window\n",
    "    select window_start, window_end, count(*) as count, max(c1) as max_c1\n",
    "    from tumble(my_stream, 5s) group by window_start, window_end\n",
    "\n",
    "    2 global aggregration which Global aggregation will start the aggregation for all incoming events since the query is submitted, and never ends.\n",
    "    select count(*) as count, id as id\n",
    "    from my_stream group by id\n",
    "\n",
    "    3 historical aggreation, using table function, the query will just run traditional SQL that scan all historical data and return after query end\n",
    "    select count(*) as count, id as id\n",
    "    from table(my_stream) group by id\n",
    "\n",
    "\n",
    "    #########\n",
    "    here is a sample output:\n",
    "    [\n",
    "      {\n",
    "        \"sql\": \"select eventVersion, sum(videoSourceBandwidthBytesPerEvent + videoFecBandwidthBytesPerEvent + audioSourceBandwidthBytesPerEvent + audioFecBandwidthBytesPerEvent) as total_bandwidth_bytes from xray_stream group by eventVersion\",\n",
    "        \"description\": \"Calculate the total bandwidth used per event version by summing up video, audio, and FEC bandwidths.\",\n",
    "        \"name\" : \"Bandwidth Utilization Analysis\"\n",
    "      }\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\"model\": \"gpt-4o-mini\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}\n",
    "            ],\n",
    "            \"temperature\": 0,\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        self._llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "                    \"model\": \"gemma2:9b\",\n",
    "                    \"base_url\": \"http://localhost:11434/v1\",\n",
    "                    \"api_key\": \"ollama\",\n",
    "                }\n",
    "            ]}\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_onboarding_agent = ConversableAgent(\n",
    "            \"data_onboarding_agent\",\n",
    "            system_message=self.data_onboarding_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "        self.schema_inference_agent = ConversableAgent(\n",
    "            \"schema_inference_agent\",\n",
    "            system_message=self.schema_inference_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "        self.schema_to_table_agent = ConversableAgent(\n",
    "            \"schema_to_table_agent\",\n",
    "            system_message=self.schema_to_table_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "        self.field_summary_agent = ConversableAgent(\n",
    "            \"field_summary_agent\",\n",
    "            system_message=self.field_summary_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "        self.analysis_recommendations_agent = ConversableAgent(\n",
    "            \"analysis_recommendations_agent\",\n",
    "            system_message=self.analysis_recommendations_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "        self.analysis_sql_generation_agent = ConversableAgent(\n",
    "            \"analysis_sql_generation_agent\",\n",
    "            system_message=self.analysis_sql_generation_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "    def process(self):\n",
    "        message = (\n",
    "            f\"based on input data : {self.data}, and stream name : {self.stream_name}\"\n",
    "        )\n",
    "        self.data_onboarding_agent.initiate_chats(\n",
    "            [\n",
    "                {\n",
    "                    \"recipient\": self.schema_inference_agent,\n",
    "                    \"message\": message,\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "                {\n",
    "                    \"recipient\": self.field_summary_agent,\n",
    "                    \"message\": \"based on input DDL, add field summary\",\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "                {\n",
    "                    \"recipient\": self.analysis_recommendations_agent,\n",
    "                    \"message\": f\"based on input data : {self.data}, and field summary\",\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "                {\n",
    "                    \"recipient\": self.analysis_sql_generation_agent,\n",
    "                    \"message\": \"based on input schema and recommandations, generate analysis SQL, those metric seems like some video network package quality related data.\",\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def inference(self, data, stream_name):\n",
    "        message = f\"based on input data : {data}, and stream name : {stream_name}\"\n",
    "        self.data_onboarding_agent.initiate_chats(\n",
    "            [\n",
    "                {\n",
    "                    \"recipient\": self.schema_inference_agent,\n",
    "                    \"message\": message,\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "                {\n",
    "                    \"recipient\": self.schema_to_table_agent,\n",
    "                    \"message\": \"please generate json expression for the schema\",\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return self.schema_inference_agent.last_message()[\"content\"], self.schema_to_table_agent.last_message()[\"content\"]\n",
    "\n",
    "    def summary(self, data, columns):\n",
    "        message = f\"based on input data : {data}, and columns : {columns}\"\n",
    "        self.data_onboarding_agent.initiate_chats(\n",
    "            [\n",
    "                {\n",
    "                    \"recipient\": self.field_summary_agent,\n",
    "                    \"message\": message,\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return self.field_summary_agent.last_message()[\"content\"]\n",
    "\n",
    "    def recommendations(self, data, columns, stream_name):\n",
    "        message = f\"based on input data : {data}, and columns : {columns} and stream name : {stream_name}\"\n",
    "        self.data_onboarding_agent.initiate_chats(\n",
    "            [\n",
    "                {\n",
    "                    \"recipient\": self.analysis_sql_generation_agent,\n",
    "                    \"message\": message,\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return self.analysis_sql_generation_agent.last_message()[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c37c1bd-3faa-4f5f-96c9-6906983bbbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mdata_onboarding_agent\u001b[0m (to schema_inference_agent):\n",
      "\n",
      "based on input data : \n",
      "{\n",
      "    \"customer_id\": 56,\n",
      "    \"name\": \"Emily Taylor\",\n",
      "    \"email\": \"brianpeters@example.org\",\n",
      "    \"phone\": \"522.227.0958x8414\",\n",
      "    \"address\": \"231 Brown Orchard\n",
      "North Kristafurt, NC 05498\",\n",
      "    \"home\": null\n",
      "}\n",
      ", and stream name : customer\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mschema_inference_agent\u001b[0m (to data_onboarding_agent):\n",
      "\n",
      "```sql\n",
      "CREATE STREAM customer\n",
      "(\n",
      "  `customer_id` uint32,\n",
      "  `name` string,\n",
      "  `email` string,\n",
      "  `phone` string,\n",
      "  `address` string,\n",
      "  `home` unknown\n",
      ")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mdata_onboarding_agent\u001b[0m (to schema_to_table_agent):\n",
      "\n",
      "please generate json expression for the schema\n",
      "Context: \n",
      "```sql\n",
      "CREATE STREAM customer\n",
      "(\n",
      "  `customer_id` uint32,\n",
      "  `name` string,\n",
      "  `email` string,\n",
      "  `phone` string,\n",
      "  `address` string,\n",
      "  `home` unknown\n",
      ")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mschema_to_table_agent\u001b[0m (to data_onboarding_agent):\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"customer_id\",\n",
      "        \"type\": \"uint32\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"name\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"email\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"phone\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"address\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"home\",\n",
      "        \"type\": \"unknown\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "```sql\n",
      "CREATE STREAM customer\n",
      "(\n",
      "  `customer_id` uint32,\n",
      "  `name` string,\n",
      "  `email` string,\n",
      "  `phone` string,\n",
      "  `address` string,\n",
      "  `home` unknown\n",
      ")\n",
      "```\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"customer_id\",\n",
      "        \"type\": \"uint32\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"name\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"email\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"phone\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"address\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"home\",\n",
      "        \"type\": \"unknown\"\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "data_customer = '''\n",
    "{\n",
    "    \"customer_id\": 56,\n",
    "    \"name\": \"Emily Taylor\",\n",
    "    \"email\": \"brianpeters@example.org\",\n",
    "    \"phone\": \"522.227.0958x8414\",\n",
    "    \"address\": \"231 Brown Orchard\\nNorth Kristafurt, NC 05498\",\n",
    "    \"home\": null\n",
    "}\n",
    "'''\n",
    "agent = DataOnboardingAgent()\n",
    "\n",
    "ddl, ddl_json = agent.inference(data_customer, 'customer')\n",
    "\n",
    "print(ddl)\n",
    "\n",
    "print(ddl_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900df0f7-83bf-4a00-b41b-a5601f50ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mdata_onboarding_agent\u001b[0m (to schema_inference_agent):\n",
      "\n",
      "based on input data : \n",
      "{\n",
      "    \"history_id\": 49,\n",
      "    \"customer_id\": 49,\n",
      "    \"bank_name\": \"Montgomery Inc\",\n",
      "    \"credit_score\": 485,\n",
      "    \"outstanding_debt\": 26706.08,\n",
      "    \"last_updated\": 19674\n",
      "}\n",
      ", and stream name : credit_history\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mschema_inference_agent\u001b[0m (to data_onboarding_agent):\n",
      "\n",
      "```sql\n",
      "CREATE STREAM credit_history\n",
      "(\n",
      "  `history_id` uint32,\n",
      "  `customer_id` uint32,\n",
      "  `bank_name` string,\n",
      "  `credit_score` uint32,\n",
      "  `outstanding_debt` float64,\n",
      "  `last_updated` uint32\n",
      ")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```sql\\nCREATE STREAM credit_history\\n(\\n  `history_id` uint32,\\n  `customer_id` uint32,\\n  `bank_name` string,\\n  `credit_score` uint32,\\n  `outstanding_debt` float64,\\n  `last_updated` uint32\\n)\\n```'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_credit_history = '''\n",
    "{\n",
    "    \"history_id\": 49,\n",
    "    \"customer_id\": 49,\n",
    "    \"bank_name\": \"Montgomery Inc\",\n",
    "    \"credit_score\": 485,\n",
    "    \"outstanding_debt\": 26706.08,\n",
    "    \"last_updated\": 19674\n",
    "}\n",
    "'''\n",
    "agent = DataOnboardingAgent()\n",
    "\n",
    "agent.inference(data_credit_history, 'credit_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3424e020-0f84-44f9-8e13-9c8c0bf45234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mdata_onboarding_agent\u001b[0m (to schema_inference_agent):\n",
      "\n",
      "based on input data : \n",
      "{\n",
      "\t\"_id\": {\n",
      "\t\t\"$oid\": \"67bc078e18939cdcb051732a\"\n",
      "\t},\n",
      "\t\"customer_id\": 49,\n",
      "\t\"raw_data\": {\n",
      "\t\t\"transaction_history\": [\n",
      "\t\t\t{\n",
      "\t\t\t\t\"date\": {\n",
      "\t\t\t\t\t\"$date\": 1738368000000\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"amount\": 391.05\n",
      "\t\t\t},\n",
      "\t\t\t{\n",
      "\t\t\t\t\"date\": {\n",
      "\t\t\t\t\t\"$date\": 1738540800000\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"amount\": 923.33\n",
      "\t\t\t},\n",
      "\t\t\t{\n",
      "\t\t\t\t\"date\": {\n",
      "\t\t\t\t\t\"$date\": 1738972800000\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"amount\": 541.95\n",
      "\t\t\t},\n",
      "\t\t\t{\n",
      "\t\t\t\t\"date\": {\n",
      "\t\t\t\t\t\"$date\": 1736208000000\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"amount\": 116.17\n",
      "\t\t\t},\n",
      "\t\t\t{\n",
      "\t\t\t\t\"date\": {\n",
      "\t\t\t\t\t\"$date\": 1736899200000\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"amount\": 399.86\n",
      "\t\t\t},\n",
      "\t\t\t{\n",
      "\t\t\t\t\"date\": {\n",
      "\t\t\t\t\t\"$date\": 1739577600000\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"amount\": 625.11\n",
      "\t\t\t},\n",
      "\t\t\t{\n",
      "\t\t\t\t\"date\": {\n",
      "\t\t\t\t\t\"$date\": 1738195200000\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"amount\": 668.96\n",
      "\t\t\t},\n",
      "\t\t\t{\n",
      "\t\t\t\t\"date\": {\n",
      "\t\t\t\t\t\"$date\": 1736294400000\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"amount\": 472.46\n",
      "\t\t\t},\n",
      "\t\t\t{\n",
      "\t\t\t\t\"date\": {\n",
      "\t\t\t\t\t\"$date\": 1737072000000\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"amount\": 17.49\n",
      "\t\t\t}\n",
      "\t\t],\n",
      "\t\t\"social_media_activity\": {\n",
      "\t\t\t\"platform\": \"Twitter\",\n",
      "\t\t\t\"activity_score\": 34\n",
      "\t\t},\n",
      "\t\t\"miscellaneous\": {\n",
      "\t\t\t\"notes\": \"Policy wish success begin candidate raise state.\",\n",
      "\t\t\t\"risk_flags\": \"Medium\"\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      ", and stream name : unstructure\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mschema_inference_agent\u001b[0m (to data_onboarding_agent):\n",
      "\n",
      "```sql\n",
      "CREATE STREAM unstructure\n",
      "(\n",
      "  `_id` string,\n",
      "  `customer_id` uint32,\n",
      "  `raw_data` tuple(\n",
      "    `transaction_history` array(tuple(\n",
      "      `date` uint64,\n",
      "      `amount` float64\n",
      "    )),\n",
      "    `social_media_activity` tuple(\n",
      "      `platform` string,\n",
      "      `activity_score` uint32\n",
      "    ),\n",
      "    `miscellaneous` tuple(\n",
      "      `notes` string,\n",
      "      `risk_flags` string\n",
      "    )\n",
      "  )\n",
      ")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mdata_onboarding_agent\u001b[0m (to schema_to_table_agent):\n",
      "\n",
      "please generate json expression for the schema\n",
      "Context: \n",
      "```sql\n",
      "CREATE STREAM unstructure\n",
      "(\n",
      "  `_id` string,\n",
      "  `customer_id` uint32,\n",
      "  `raw_data` tuple(\n",
      "    `transaction_history` array(tuple(\n",
      "      `date` uint64,\n",
      "      `amount` float64\n",
      "    )),\n",
      "    `social_media_activity` tuple(\n",
      "      `platform` string,\n",
      "      `activity_score` uint32\n",
      "    ),\n",
      "    `miscellaneous` tuple(\n",
      "      `notes` string,\n",
      "      `risk_flags` string\n",
      "    )\n",
      "  )\n",
      ")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mschema_to_table_agent\u001b[0m (to data_onboarding_agent):\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"_id\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"customer_id\",\n",
      "        \"type\": \"uint32\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"raw_data\",\n",
      "        \"type\": \"tuple(`transaction_history` array(tuple(`date` uint64, `amount` float64)), `social_media_activity` tuple(`platform` string, `activity_score` uint32), `miscellaneous` tuple(`notes` string, `risk_flags` string))\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "```sql\n",
      "CREATE STREAM unstructure\n",
      "(\n",
      "  `_id` string,\n",
      "  `customer_id` uint32,\n",
      "  `raw_data` tuple(\n",
      "    `transaction_history` array(tuple(\n",
      "      `date` uint64,\n",
      "      `amount` float64\n",
      "    )),\n",
      "    `social_media_activity` tuple(\n",
      "      `platform` string,\n",
      "      `activity_score` uint32\n",
      "    ),\n",
      "    `miscellaneous` tuple(\n",
      "      `notes` string,\n",
      "      `risk_flags` string\n",
      "    )\n",
      "  )\n",
      ")\n",
      "```\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"_id\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"customer_id\",\n",
      "        \"type\": \"uint32\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"raw_data\",\n",
      "        \"type\": \"tuple(`transaction_history` array(tuple(`date` uint64, `amount` float64)), `social_media_activity` tuple(`platform` string, `activity_score` uint32), `miscellaneous` tuple(`notes` string, `risk_flags` string))\"\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "data_unstructure = '''\n",
    "{\n",
    "\t\"_id\": {\n",
    "\t\t\"$oid\": \"67bc078e18939cdcb051732a\"\n",
    "\t},\n",
    "\t\"customer_id\": 49,\n",
    "\t\"raw_data\": {\n",
    "\t\t\"transaction_history\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"date\": {\n",
    "\t\t\t\t\t\"$date\": 1738368000000\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"amount\": 391.05\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"date\": {\n",
    "\t\t\t\t\t\"$date\": 1738540800000\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"amount\": 923.33\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"date\": {\n",
    "\t\t\t\t\t\"$date\": 1738972800000\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"amount\": 541.95\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"date\": {\n",
    "\t\t\t\t\t\"$date\": 1736208000000\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"amount\": 116.17\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"date\": {\n",
    "\t\t\t\t\t\"$date\": 1736899200000\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"amount\": 399.86\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"date\": {\n",
    "\t\t\t\t\t\"$date\": 1739577600000\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"amount\": 625.11\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"date\": {\n",
    "\t\t\t\t\t\"$date\": 1738195200000\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"amount\": 668.96\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"date\": {\n",
    "\t\t\t\t\t\"$date\": 1736294400000\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"amount\": 472.46\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"date\": {\n",
    "\t\t\t\t\t\"$date\": 1737072000000\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"amount\": 17.49\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t\t\"social_media_activity\": {\n",
    "\t\t\t\"platform\": \"Twitter\",\n",
    "\t\t\t\"activity_score\": 34\n",
    "\t\t},\n",
    "\t\t\"miscellaneous\": {\n",
    "\t\t\t\"notes\": \"Policy wish success begin candidate raise state.\",\n",
    "\t\t\t\"risk_flags\": \"Medium\"\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "'''\n",
    "agent = DataOnboardingAgent()\n",
    "\n",
    "ddl, ddl_json = agent.inference(data_unstructure, 'unstructure')\n",
    "\n",
    "print(ddl)\n",
    "\n",
    "print(ddl_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
