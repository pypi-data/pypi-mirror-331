{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd3e649-a34b-46e1-87f2-2815532a8e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogen\n",
      "  Downloading autogen-0.7.6-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pyautogen==0.7.6 (from autogen)\n",
      "  Downloading pyautogen-0.7.6-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting asyncer==0.0.8 (from pyautogen==0.7.6->autogen)\n",
      "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting diskcache (from pyautogen==0.7.6->autogen)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting docker (from pyautogen==0.7.6->autogen)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fast-depends<3,>=2.4.12 (from pyautogen==0.7.6->autogen)\n",
      "  Downloading fast_depends-2.4.12-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting httpx<1,>=0.28.1 (from pyautogen==0.7.6->autogen)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.7.6->autogen) (23.2)\n",
      "Collecting pydantic<3,>=2.6.1 (from pyautogen==0.7.6->autogen)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting python-dotenv (from pyautogen==0.7.6->autogen)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting termcolor (from pyautogen==0.7.6->autogen)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tiktoken (from pyautogen==0.7.6->autogen)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.4.0 in /opt/conda/lib/python3.11/site-packages (from asyncer==0.0.8->pyautogen==0.7.6->autogen) (4.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.28.1->pyautogen==0.7.6->autogen) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.28.1->pyautogen==0.7.6->autogen)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.28.1->pyautogen==0.7.6->autogen) (3.4)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.28.1->pyautogen==0.7.6->autogen)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6.1->pyautogen==0.7.6->autogen)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6.1->pyautogen==0.7.6->autogen)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic<3,>=2.6.1->pyautogen==0.7.6->autogen)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from docker->pyautogen==0.7.6->autogen) (2.31.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from docker->pyautogen==0.7.6->autogen) (2.0.7)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->pyautogen==0.7.6->autogen)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5.0,>=3.4.0->asyncer==0.0.8->pyautogen==0.7.6->autogen) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->docker->pyautogen==0.7.6->autogen) (3.3.0)\n",
      "Downloading autogen-0.7.6-py3-none-any.whl (12 kB)\n",
      "Downloading pyautogen-0.7.6-py3-none-any.whl (630 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.4/630.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
      "Downloading fast_depends-2.4.12-py3-none-any.whl (17 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.1/792.1 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, termcolor, regex, python-dotenv, h11, diskcache, annotated-types, tiktoken, pydantic-core, httpcore, docker, asyncer, pydantic, httpx, fast-depends, pyautogen, autogen\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed annotated-types-0.7.0 asyncer-0.0.8 autogen-0.7.6 diskcache-5.6.3 docker-7.1.0 fast-depends-2.4.12 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 pyautogen-0.7.6 pydantic-2.10.6 pydantic-core-2.27.2 python-dotenv-1.0.1 regex-2024.11.6 termcolor-2.5.0 tiktoken-0.9.0 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "! pip install autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bdb890b-df78-4de4-ba61-488c6d0c6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694dc33f-6a50-4853-91b1-23ad5e9a94d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import json\n",
    "import os\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "\n",
    "class DataExtractionAgent:\n",
    "\n",
    "    data_extraction_system_message = \"you are a helpful data processing agent, help to extract useful information to unstrcuture data\"\n",
    "\n",
    "    payload_extraction_system_message = \"\"\"\n",
    "    the input data is a debezium CDC data payload, our target is to extract the payload in after or payload:after into a new stream\n",
    "    the source stream has just one string field with name raw\n",
    "\n",
    "    here is are sample queries to extrac the after payload based on different types of debezium payload\n",
    "    case1. when the after payload is in root layer\n",
    "    select raw:after from source_stream_name where _tp_time > earliest_ts()\n",
    "    case2. when the after payload is in field of payload\n",
    "    select raw:payload:after from source_stream_name where _tp_time > earliest_ts()\n",
    "\n",
    "    return which extract query should be used in markdown code with sql\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    target_schema_inference_system_message = \"\"\"please generate DDL based on debezium payload\n",
    "    case1. when the after payload is in root layer, using json object in the after field as input\n",
    "    case2. when the after payload is in field of payload,\n",
    "     using the json string in the after field and only in the after field of payload as input\n",
    "     No other fields should be considered, such as source, or schema etc\n",
    "\n",
    "    here are the rules to follow\n",
    "    * the DDL grammar follows ClickHouse style\n",
    "    * the Table keyword MUST be replaced with Stream\n",
    "    * all datatypes MUST be in lowercase, such uint32\n",
    "    * all keywords MUST be in lowercase, such as nullable\n",
    "    * all field names MUST keep same as in the json\n",
    "    * composite types such as array, tuple, map cannot be nullable\n",
    "    * should use composite types like array, map or tuple to represent complex structure in the json\n",
    "    * output should be put into markdown of sql\n",
    "    * bool type is supported\n",
    "    * available composite types are\n",
    "        * array\n",
    "        * tuple\n",
    "        * map\n",
    "    * for composite type, using tuple over map, as tulpe is more generic\n",
    "\n",
    "    here is a sample of output DDL:\n",
    "    ```sql\n",
    "    CREATE STREAM target_stream\n",
    "    (\n",
    "      `cid` string,\n",
    "      `gas_percent` float64,\n",
    "      `in_use` bool,\n",
    "      `latitude` float64,\n",
    "      `longitude` float64,\n",
    "      `locked` bool,\n",
    "      `speed_kmh` float64,\n",
    "      `time` string,\n",
    "      `total_km` float64\n",
    "    )\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    target_mutable_stream_schema_inference_system_message = \"\"\"please generate DDL based on debezium payload\n",
    "    case1. when the after payload is in root layer, using json object in the after field as input\n",
    "    case2. when the after payload is in field of payload,\n",
    "    using the json string in the after field and only in the after field of payload as input\n",
    "    No other fields should be considered, such as source, or schema etc\n",
    "    \n",
    "    the GRAMMAR is\n",
    "    CREATE MUTABLE STREAM [IF NOT EXISTS] stream_name (\n",
    "        <col1> <col_type>,\n",
    "        <col2> <col_type>,\n",
    "        <col3> <col_type>,\n",
    "        <col4> <col_type>\n",
    "        INDEX <index1> (col3)\n",
    "        FAMILY <family1> (col3,col4)\n",
    "    )\n",
    "    PRIMARY KEY (col1, col2) \n",
    "    \n",
    "\n",
    "    here are the rules to follow\n",
    "    * the DDL grammar follows ClickHouse style\n",
    "    * all datatypes MUST be in lowercase, such uint32\n",
    "    * all keywords MUST be in lowercase, such as nullable\n",
    "    * all field names MUST keep same as in the json\n",
    "    * composite types such as array, tuple, map cannot be nullable\n",
    "    * should use composite types like array, map or tuple to represent complex structure in the json\n",
    "    * output should be put into markdown of sql\n",
    "    * bool type is supported\n",
    "    * available composite types are\n",
    "        * array\n",
    "        * tuple\n",
    "    * for composite type, using tuple over map, as tulpe is more generic\n",
    "\n",
    "    here is a sample of output DDL:\n",
    "    ```sql\n",
    "    CREATE MUTABLE STREAM target_stream\n",
    "    (\n",
    "      `cid` string,\n",
    "      `gas_percent` float64,\n",
    "      `in_use` bool,\n",
    "      `latitude` float64,\n",
    "      `longitude` float64,\n",
    "    )\n",
    "    PRIMARY KEY (cid) \n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    mv_extraction_system_message = \"\"\"please create a materialized view to extraction information from source stream into target stream\n",
    "    the source stream has just one string field with name raw\n",
    "    here are the rules to following\n",
    "    * the grammar follows ClickHouse style\n",
    "    * all function name follows snake case, such as json_extract_array\n",
    "    * all keywords MUST be in lowercase, such as nullable\n",
    "    * using tuple for hierarchy case which is generic\n",
    "    * please CHECK the structure of the source payload, make sure the extraction map the structure excatly\n",
    "      especially when there is tuple of tuple, make sure each layer of tuple clearly casted using tuple_cast\n",
    "\n",
    "\n",
    "    here is the grammar of materialized view\n",
    "    CREATE MATERIALIZED VIEW [IF NOT EXISTS] <view_name>\n",
    "    INTO <target_stream> AS <SELECT ...>\n",
    "\n",
    "    NOTE, to extrat json with hierarchy,\n",
    "    this one is WRONG : json_extract_uint(raw, 'after.customer_id') AS customer_id\n",
    "    extract target field does not support hierarchy\n",
    "    SHOULD BE : json_extract_uint(raw:after, 'customer_id') AS customer_id,\n",
    "\n",
    "    this one is WRONG : tuple_cast(json_extract_string(raw:payload:after, '_id.$oid')) AS _id,\n",
    "    SHOULD BE : tuple_cast(json_extract_string(raw:payload:after:_id, '$oid')) AS _id,\n",
    "\n",
    "    to construct or convert tuple type , call tuple_cast, for example:\n",
    "    tuple_cast(a, b) AS tuple_field,\n",
    "    there is no tuple() function, NEVER call tuple() function\n",
    "\n",
    "    In case the payload contains complex composition and hierarchy, you should provide the conversion layer by layer, do not miss any middle layer\n",
    "    here is a sample that one of the target field is a array of tuple, using array_map function to help\n",
    "    array_map(\n",
    "        x -> (\n",
    "            tuple_cast(\n",
    "                json_extract_string(x, 'field_name_1') as field_name_1, \n",
    "                json_extract_float(x, 'field_name_2') as field_name_2\n",
    "            )\n",
    "        ),\n",
    "        json_extract_array(after:raw_data, 'field_name_3')\n",
    "    ) as field\n",
    "\n",
    "    please only use following available json extraction functions if required:\n",
    "    * json_extract_int\n",
    "    * json_extract_uint\n",
    "    * json_extract_float\n",
    "    * json_extract_bool\n",
    "    * json_extract_string\n",
    "    * json_extract_array\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}\n",
    "            ],\n",
    "            \"temperature\": 0,\n",
    "        }\n",
    "\n",
    "        self.data_extraction_agent = ConversableAgent(\n",
    "            \"data_extraction_agent\",\n",
    "            system_message=self.data_extraction_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "        self.payload_extraction_agent = ConversableAgent(\n",
    "            \"payload_extraction_agent\",\n",
    "            system_message=self.payload_extraction_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "        self.target_schema_inference_agent = ConversableAgent(\n",
    "            \"target_schema_inference_agent\",\n",
    "            system_message=self.target_schema_inference_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "        \n",
    "        self.target_mutable_stream_schema_inference_agent = ConversableAgent(\n",
    "            \"target_mutable_stream_schema_inference_agent\",\n",
    "            system_message=self.target_mutable_stream_schema_inference_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "        self.mv_extraction_agent = ConversableAgent(\n",
    "            \"mv_extraction_agent\",\n",
    "            system_message=self.mv_extraction_system_message,\n",
    "            llm_config=self._llm_config,\n",
    "            code_execution_config=False,\n",
    "            max_consecutive_auto_reply=1,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "\n",
    "    def pipeline(self, data, source_stream_name, target_stream_name):\n",
    "        message = (\n",
    "            f\"based on input data : {data} and source stream name {source_stream_name}\"\n",
    "        )\n",
    "        self.data_extraction_agent.initiate_chats(\n",
    "            [\n",
    "                {\n",
    "                    \"recipient\": self.payload_extraction_agent,\n",
    "                    \"message\": message,\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "                {\n",
    "                    \"recipient\": self.target_schema_inference_agent,\n",
    "                    \"message\": f\"based on input data : {data} and target stream name {target_stream_name}\",\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "                {\n",
    "                    \"recipient\": self.mv_extraction_agent,\n",
    "                    \"message\": \"please create materialized view to extrat information from source stream to target stream\",\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self.payload_extraction_agent.last_message()[\"content\"],\n",
    "            self.target_schema_inference_agent.last_message()[\"content\"],\n",
    "            self.mv_extraction_agent.last_message()[\"content\"],\n",
    "        )\n",
    "        \n",
    "    def pipeline_with_mutable_stream(self, data, source_stream_name, target_stream_name, ids):\n",
    "        message = (\n",
    "            f\"based on input data : {data} and source stream name {source_stream_name}\"\n",
    "        )\n",
    "        self.data_extraction_agent.initiate_chats(\n",
    "            [\n",
    "                {\n",
    "                    \"recipient\": self.payload_extraction_agent,\n",
    "                    \"message\": message,\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "                {\n",
    "                    \"recipient\": self.target_mutable_stream_schema_inference_agent,\n",
    "                    \"message\": f\"based on input data : {data} and target stream name {target_stream_name}, , and id fields {','.join(ids)}\",\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "                {\n",
    "                    \"recipient\": self.mv_extraction_agent,\n",
    "                    \"message\": \"please create materialized view to extrat information from source stream to target stream\",\n",
    "                    \"max_turns\": 1,\n",
    "                    \"summary_method\": \"last_msg\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self.payload_extraction_agent.last_message()[\"content\"],\n",
    "            self.target_mutable_stream_schema_inference_agent.last_message()[\"content\"],\n",
    "            self.mv_extraction_agent.last_message()[\"content\"],\n",
    "        )\n",
    "\n",
    "\n",
    "def debezium_payload_extraction(data, source_stream, target_stream):\n",
    "    results = []\n",
    "    for (data, source_stream, target_stream) in zip(data, source_stream, target_stream):\n",
    "        try:\n",
    "            agent = DataExtractionAgent()\n",
    "            payload_extraction_sql, target_stream_ddl, extraction_mv_ddl = agent.pipeline(data, source_stream, target_stream)\n",
    "            result = {\n",
    "                \"target_stream_ddl\" : target_stream_ddl,\n",
    "                \"extraction_mv_ddl\" : extraction_mv_ddl\n",
    "            }\n",
    "            results.append(json.dumps(result))\n",
    "        except Exception as e:\n",
    "            trace = traceback.format_exc()\n",
    "            results.append(trace)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443cf7a8-acb0-422a-b2ae-c77d6776ce33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 ns ± 6.86 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit debezium_payload_extraction([],[],[]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee441be-0301-458a-b4f4-60e74cbd8583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
