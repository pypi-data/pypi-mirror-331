{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of sWeights computation\n",
    "\n",
    "This notebook presents the **sPlot** method available in `hepstats`. This method helps to unfold the contributions of different sources, populating a data sample, for a given variable. The method is described in this [paper](https://arxiv.org/pdf/physics/0402083.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"ZFIT_DISABLE_TF_WARNINGS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import zfit\n",
    "from utils import plotfitresult, pltdist\n",
    "from zfit.loss import ExtendedUnbinnedNLL\n",
    "from zfit.minimize import Minuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (9, 8)\n",
    "plt.rcParams[\"font.size\"] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the method, a fake data sample of mass and momenta distributions for different species, here signal and background, are generated and plotted in the following figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = (0.0, 3.0)\n",
    "nbkg = 10000\n",
    "nsig = 5000\n",
    "\n",
    "# Data and signal\n",
    "\n",
    "np.random.seed(0)\n",
    "tau = -2.0\n",
    "beta = -1 / tau\n",
    "bkg = np.random.exponential(beta, nbkg)\n",
    "peak = np.random.normal(1.2, 0.2, nsig)\n",
    "mass = np.concatenate((bkg, peak))\n",
    "\n",
    "sig_p = np.random.normal(5, 1, size=nsig)\n",
    "bck_p = np.random.normal(3, 1, size=nbkg)\n",
    "p = np.concatenate([bck_p, sig_p])\n",
    "\n",
    "sel = (mass > bounds[0]) & (mass < bounds[1])\n",
    "\n",
    "mass = mass[sel]\n",
    "p = p[sel]\n",
    "\n",
    "sorter = np.argsort(mass)\n",
    "mass = mass[sorter]\n",
    "p = p[sorter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "ax[0].hist(peak, bins=20, density=True, range=bounds)\n",
    "ax[0].hist(bkg, bins=20, density=True, range=bounds, alpha=0.5)\n",
    "ax[0].set_xlabel(\"mass\")\n",
    "\n",
    "ax[1].hist(sig_p, bins=20, density=True, range=(0, 10))\n",
    "ax[1].hist(bck_p, bins=20, density=True, range=(0, 10), alpha=0.5)\n",
    "ax[1].set_xlabel(\"p\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data sample the label of the species, signal and background, are not known. The data is a mixture of the two species are can be seen the next figure. In order to identify the species, a maximum likelihood (extended) fit has to be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltdist(mass, 80, bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the model to fit is a sum of a Gaussian probability density function (pdf) and an exponential pdf. The model definiton and the likelihood are defined using the `zfit` library, the minimisation of the log-likelihood function is performed with `iminuit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = zfit.Space(\"x\", limits=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = zfit.Parameter(\"mean\", 1.2, 0.5, 2.0)\n",
    "sigma = zfit.Parameter(\"sigma\", 0.3, 0.02, 0.2)\n",
    "lambda_ = zfit.Parameter(\"lambda\", -2.0, -4.0, -1.0)\n",
    "Nsig = zfit.Parameter(\"Nsig\", nsig, 0.0, len(mass))\n",
    "Nbkg = zfit.Parameter(\"Nbkg\", nbkg, 0.0, len(mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = zfit.pdf.Gauss(obs=obs, mu=mean, sigma=sigma).create_extended(Nsig)\n",
    "background = zfit.pdf.Exponential(obs=obs, lambda_=lambda_).create_extended(Nbkg)\n",
    "tot_model = zfit.pdf.SumPDF([signal, background])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the negative log likelihood\n",
    "data_ = zfit.data.Data.from_numpy(obs=obs, array=mass)\n",
    "nll = ExtendedUnbinnedNLL(model=tot_model, data=data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a minuit minimizer\n",
    "minimizer = Minuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimisation of the loss function\n",
    "minimum = minimizer.minimize(loss=nll)\n",
    "minimum.hesse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the fit, the signal and background components of the data samples are plotted in the next figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 80\n",
    "pltdist(mass, nbins, bounds)\n",
    "plotfitresult(tot_model, bounds, nbins, label=\"total model\", color=\"crimson\")\n",
    "plotfitresult(background, bounds, nbins, label=\"background\", color=\"forestgreen\")\n",
    "plotfitresult(signal, bounds, nbins, label=\"signal\", color=\"orange\")\n",
    "plt.xlabel(\"m [GeV/c$^2$]\")\n",
    "plt.ylabel(\"number of events\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the [paper](https://arxiv.org/pdf/physics/0402083.pdf), after a model is fit **sWeights** can be computed from the model and the data. The `compute_sweights` function from `hepstats` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepstats.splot import compute_sweights\n",
    "\n",
    "weights = compute_sweights(tot_model, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[Nsig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mass, weights[Nsig], label=\"signal weights\")\n",
    "plt.plot(mass, weights[Nbkg], label=\"background weights\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights can be used, to unfold the signal and background components on another variable that should be stastically independent of the splotted variable, for instance here the momentum distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "hist_conf = {\"bins\": 30, \"alpha\": 0.4, \"range\": [0, 10]}\n",
    "ax[0].hist(sig_p, label=\"original sig p\", **hist_conf)\n",
    "ax[0].hist(p, weights=weights[Nsig], label=\"reconstructed sig p\", **hist_conf)\n",
    "ax[0].set_xlabel(\"p\")\n",
    "ax[0].legend(fontsize=12)\n",
    "\n",
    "ax[1].hist(bck_p, label=\"original bck p\", **hist_conf)\n",
    "ax[1].hist(p, weights=weights[Nbkg], label=\"reconstructed bck p\", **hist_conf)\n",
    "ax[1].set_xlabel(\"p\")\n",
    "ax[1].legend(fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of the importance of unfolding a variable distribution that is non correlated with the splotted distribution, here the sWeights are applied to the mass distribution (which is obviously correlated with the mass). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_conf = {\"bins\": 30, \"alpha\": 0.5, \"range\": bounds}\n",
    "plt.hist(peak, label=\"original sig mass\", **hist_conf)\n",
    "plt.hist(mass, weights=weights[Nsig], label=\"reconstructed sig mass\", **hist_conf)\n",
    "plt.xlabel(\"mass\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
