{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Benchmarking different methods\n",
    "\n",
    "The library provides various evaluation methods for benchmarking methods. Here we show you how to use the DistanceEvaluator and then we show how to do a quick comparison of a range of different recourse methods on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Depending on whether you want example or custom dataset, import relevant module\n",
    "from rocelib.datasets.ExampleDatasets import get_example_dataset\n",
    "from rocelib.datasets.custom_datasets.CsvDatasetLoader import CsvDatasetLoader\n",
    "\n",
    "# Import your model\n",
    "from rocelib.models.pytorch_models.TrainablePyTorchModel import TrainablePyTorchModel\n",
    "\n",
    "# Import the ClassificationTask module required for recourse generation\n",
    "from rocelib.tasks.ClassificationTask import ClassificationTask\n",
    "\n",
    "# Import the recourse method you wish to use\n",
    "from rocelib.recourse_methods.BinaryLinearSearch import BinaryLinearSearch\n",
    "\n",
    "# Import evaluator you wish to use\n",
    "from rocelib.evaluations.DistanceEvaluator import DistanceEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can import your own custom dataset. These datasets must be preprocessed to work with the provided recourse methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age  Gender  EducationLevel  ExperienceYears  PreviousCompanies  \\\n",
      "0 -0.989083       1               2        -1.658237          -0.001418   \n",
      "1  0.416376       1               4         0.928044          -0.001418   \n",
      "2  1.389387       0               2        -1.011667          -0.710538   \n",
      "3 -0.124185       1               2        -0.580620          -0.710538   \n",
      "4 -0.556634       0               1        -0.365097          -1.419657   \n",
      "\n",
      "   DistanceFromCompany  InterviewScore  SkillScore  PersonalityScore  \\\n",
      "0             0.087792       -0.089598    0.916174          1.418126   \n",
      "1             0.024537       -0.543879    0.575386          1.043255   \n",
      "2            -1.070200       -1.068049    0.541307         -1.240051   \n",
      "3            -1.311444       -0.508934   -0.821844          0.702463   \n",
      "4             1.208598       -0.963215    0.030126          1.213651   \n",
      "\n",
      "   RecruitmentStrategy  HiringDecision  \n",
      "0                    1               1  \n",
      "1                    2               1  \n",
      "2                    2               0  \n",
      "3                    3               0  \n",
      "4                    2               0  \n"
     ]
    }
   ],
   "source": [
    "# Custom dataset\n",
    "dl = CsvDatasetLoader(\"../tests/assets/standardized_recruitment_data.csv\", target_column=\"HiringDecision\")\n",
    "\n",
    "print(dl.data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Alternatively, you can also use an example dataset, remember to preprocess these too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0          1          0    0.99539   -0.05889    0.85243    0.02306   \n",
      "1          1          0    1.00000   -0.18829    0.93035   -0.36156   \n",
      "2          1          0    1.00000   -0.03365    1.00000    0.00485   \n",
      "3          1          0    1.00000   -0.45161    1.00000    1.00000   \n",
      "4          1          0    1.00000   -0.02401    0.94140    0.06531   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_25  feature_26  \\\n",
      "0    0.83398   -0.37708    1.00000    0.03760  ...    -0.51171     0.41078   \n",
      "1   -0.10868   -0.93597    1.00000   -0.04549  ...    -0.26569    -0.20468   \n",
      "2    1.00000   -0.12062    0.88965    0.01198  ...    -0.40220     0.58984   \n",
      "3    0.71216   -1.00000    0.00000    0.00000  ...     0.90695     0.51613   \n",
      "4    0.92106   -0.23255    0.77152   -0.16399  ...    -0.65158     0.13290   \n",
      "\n",
      "   feature_27  feature_28  feature_29  feature_30  feature_31  feature_32  \\\n",
      "0    -0.46168     0.21266    -0.34090     0.42267    -0.54487     0.18641   \n",
      "1    -0.18401    -0.19040    -0.11593    -0.16626    -0.06288    -0.13738   \n",
      "2    -0.22145     0.43100    -0.17365     0.60436    -0.24180     0.56045   \n",
      "3     1.00000     1.00000    -0.20099     0.25682     1.00000    -0.32382   \n",
      "4    -0.53206     0.02431    -0.62197    -0.05707    -0.59573    -0.04608   \n",
      "\n",
      "   feature_33  target  \n",
      "0    -0.45300       g  \n",
      "1    -0.02447       b  \n",
      "2    -0.38238       g  \n",
      "3     1.00000       b  \n",
      "4    -0.65697       g  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0   0.348433        0.0   0.712372  -0.234257   0.484208  -0.201735   \n",
      "1   0.348433        0.0   0.721648  -0.527811   0.634308  -1.037587   \n",
      "2   0.348433        0.0   0.721648  -0.176998   0.768477  -0.241309   \n",
      "3   0.348433        0.0   0.721648  -1.125172   0.768477   1.921340   \n",
      "4   0.348433        0.0   0.721648  -0.155129   0.655594  -0.109918   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_25  feature_26  \\\n",
      "0   0.577059  -0.954679   0.964074  -0.297510  ...   -0.867565   -0.253868   \n",
      "1  -1.339106  -2.029452   0.964074  -0.469482  ...   -0.383054   -1.447849   \n",
      "2   0.914531  -0.461494   0.746139  -0.350536  ...   -0.651896    0.093506   \n",
      "3   0.329433  -2.152585  -1.010873  -0.375331  ...    1.926340   -0.049490   \n",
      "4   0.754068  -0.676741   0.512838  -0.714742  ...   -1.143025   -0.792950   \n",
      "\n",
      "   feature_27  feature_28  feature_29  feature_30  feature_31  feature_32  \\\n",
      "0   -0.713971   -0.288290   -0.617039    0.122937   -1.055054   -0.312221   \n",
      "1   -0.208419   -0.989185   -0.173530   -0.909063   -0.115213   -0.932605   \n",
      "2   -0.276586    0.091389   -0.287320    0.441318   -0.464092    0.404443   \n",
      "3    1.947300    1.080843   -0.341218   -0.167687    1.957315   -1.289826   \n",
      "4   -0.842112   -0.615818   -1.171144   -0.717726   -1.154227   -0.757673   \n",
      "\n",
      "   feature_33  target  \n",
      "0   -0.999595       1  \n",
      "1   -0.083286       0  \n",
      "2   -0.848591       1  \n",
      "3    2.107299       0  \n",
      "4   -1.435736       1  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset, here we are using an example one, you can import a custom CSV too\n",
    "dl = get_example_dataset(\"ionosphere\")\n",
    "\n",
    "print(dl.data.head())\n",
    "\n",
    "dl.default_preprocess()\n",
    "print(dl.data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now import your model, in this case we are using a Neural Network model. Be aware of the fact that certain recourse methods may not work with some models, e.g., MCE and other MILP based methods are exclusive to Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing Pytorch NN\n",
    "model = TrainablePyTorchModel(34, [10], 1)\n",
    "\n",
    "model.train(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a ClassificationTask using the model and dataset, and then call the train method to train the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = ClassificationTask(model, dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Instantiate your recourse generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember, the recourse generator takes in the task during instantiation\n",
    "generator = BinaryLinearSearch(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "1   0.348433        0.0  -0.069541  -0.162185   0.135079  -0.163443   \n",
      "3   0.348433        0.0   0.068668   0.002060   0.325054  -0.481468   \n",
      "5   0.348433        0.0  -0.956306   0.012449  -1.092651  -0.248061   \n",
      "7  -2.090528        0.0  -0.840764  -0.057784  -0.725479  -0.166317   \n",
      "9   0.348433        0.0  -0.760374  -0.051525  -0.715251   0.045802   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_26  feature_27  \\\n",
      "1  -0.043267  -0.314285  -0.013367  -0.298152  ...    0.060099    0.140262   \n",
      "3   0.454282  -0.156960   0.293343  -0.469047  ...    0.382602    0.020988   \n",
      "5  -0.986537  -0.155239  -0.718533   0.050303  ...   -1.117684    0.008783   \n",
      "7   0.863834  -1.572191  -0.608564  -0.198904  ...    0.507534    1.832147   \n",
      "9  -0.764558   0.133289  -0.759292  -0.565139  ...   -1.030834    0.018153   \n",
      "\n",
      "   feature_28  feature_29  feature_30  feature_31  feature_32  feature_33  \\\n",
      "1    0.361689    0.048333    0.415856   -0.045484    0.291859    0.114012   \n",
      "3    0.503378    0.099649    0.896556    0.080987    0.733288   -0.014818   \n",
      "5   -0.699143   -0.196983   -0.467603   -0.204696   -0.445997    0.038152   \n",
      "7   -0.617265    0.394189    0.721824    1.849038   -0.688052    0.328997   \n",
      "9   -0.830197   -0.084653   -0.557137    0.135550   -0.852030    0.281308   \n",
      "\n",
      "   target      loss  \n",
      "1       1  6.296402  \n",
      "3       1  8.169010  \n",
      "5       1  1.169596  \n",
      "7       1  2.199860  \n",
      "9       1  1.773566  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the RecourseGenerator notebook to see the other generation functions you can use!\n",
    "recourses = generator.generate_for_all(neg_value=0)\n",
    "\n",
    "print(recourses.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, you can use the evaluator you used to evaluate that particular aspect of the recourses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.414238632011822\n",
      "Average distance: 5.414238632011822\n"
     ]
    }
   ],
   "source": [
    "# The evaluators also take the task as an argument\n",
    "distance_eval = DistanceEvaluator(task)\n",
    "\n",
    "print(recourses[\"loss\"].mean())\n",
    "\n",
    "# Provide the recourses to evaluate\n",
    "print(f\"Average distance: {distance_eval.evaluate(recourses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using QuickTabulate\n",
    "\n",
    "But what if you want to evaluate and benchmark several methods in one go? You can use QuickTabulate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First import the quick_tabulate function and all relevant generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rocelib.lib.QuickTabulate import quick_tabulate\n",
    "\n",
    "from rocelib.recourse_methods.Wachter import Wachter\n",
    "from rocelib.recourse_methods.MCER import MCER\n",
    "from rocelib.recourse_methods.MCE import MCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, create a dictionary consisting of the names of each recourse generation method and the respective class. Notice, we haven't instantiated the class (i.e., we haven't used () after the method name)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The keys are the names you would like to show in the table produced\n",
    "methods = {\"BLS\": BinaryLinearSearch, \"MCE\": MCE, \"MCER\": MCER, \"Wachter\": Wachter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And finally, call the quick_tabulate function!\n",
    "\n",
    "It is important to note that each recourse generator may take different arguments. These arguments have default values in each recourse generator, however to pass a custom value you can simply pass them as a keyword argument into quick_tabulate, as we have done for delta here.\n",
    "\n",
    "Furthermore, if you are using an example dataset, you can set the preprocess parameter to True if you would like to default preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2539031\n",
      "Academic license 2539031 - for non-commercial use only - registered to ap___@ic.ac.uk\n",
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n",
      "| Method   |   Execution Time (s) |   Validity proportion |   Average Distance |   Robustness proportion |\n",
      "+==========+======================+=======================+====================+=========================+\n",
      "| BLS      |             0.970424 |                     1 |            5.03893 |                   0.208 |\n",
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n",
      "| MCE      |             3.57642  |                     1 |            8.28336 |                   0     |\n",
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n",
      "| MCER     |            44.0608   |                     1 |           11.1068  |                   1     |\n",
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n",
      "| Wachter  |             3.68128  |                     1 |            1.58938 |                   0.248 |\n",
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "quick_tabulate(dl, model, methods, neg_value=0, column_name=\"target\", preprocess=False, delta=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If subset is not specified, quick_tabulate assumes you would like to produce recourses for all negative instances. You may specify a subset as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n",
      "| Method   |   Execution Time (s) |   Validity proportion |   Average Distance |   Robustness proportion |\n",
      "+==========+======================+=======================+====================+=========================+\n",
      "| BLS      |             0.603367 |                     1 |            4.73238 |                    0.22 |\n",
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n",
      "| MCE      |             4.31172  |                     1 |            8.37369 |                    0    |\n",
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n",
      "| MCER     |            73.9481   |                     1 |           12.9453  |                    1    |\n",
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n",
      "| Wachter  |             0.520294 |                     1 |            1.56513 |                    0.24 |\n",
      "+----------+----------------------+-----------------------+--------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "subset = dl.get_negative_instances(neg_value=0, column_name=\"target\").sample(n=50)\n",
    "\n",
    "quick_tabulate(dl, model, methods, subset=subset, neg_value=0, column_name=\"target\", preprocess=False, delta=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And there you have it! That's how you can compare various methods on a singular dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
