{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import signalflow as sf\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "from pixasonics.core import App, Mapper\n",
    "from pixasonics.features import *\n",
    "from pixasonics.synths import Theremin, Oscillator, FilteredNoise, SimpleFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create app\n",
    "app = App(image_size=(800, 800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all red-ch images into arrays and concatenate them in the channel dimension\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "img_files = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs = []\n",
    "for img_file in img_files:\n",
    "    img_path = os.path.join(img_folder, img_file)\n",
    "    img = Image.open(img_path)\n",
    "    img = np.array(img)\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the channel dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine red and green channels and all layers\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs = []\n",
    "for img_red, img_green in zip(imgs_red, imgs_green):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img = np.stack([img_red, img_green], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use all images in the folder\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs_blue = [f for f in img_files if f.endswith(\"w3.TIF\")] # only blue channel images\n",
    "imgs = []\n",
    "for img_red, img_green, img_blue in zip(imgs_red, imgs_green, imgs_blue):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_path_blue = os.path.join(img_folder, img_blue)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_blue = Image.open(img_path_blue)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img_blue = np.array(img_blue)\n",
    "    img = np.stack([img_red, img_green, img_blue], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixasonics.core import AppRegistry\n",
    "\n",
    "app_registry = AppRegistry()\n",
    "\n",
    "app_registry._apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app2 = App(image_size=(500, 500))\n",
    "img_path = \"images/test.jpg\"\n",
    "app2.load_image_file(img_path)\n",
    "mean_red2 = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "# attach the feature to the app\n",
    "app2.attach_feature(mean_red2)\n",
    "\n",
    "# create a Theremin, a simple sine wave synth that we will use to sonify the mean pixel value\n",
    "theremin2 = Theremin()\n",
    "# attach the Theremin to the app\n",
    "app2.attach_synth(theremin2)\n",
    "\n",
    "# create a Mapper that will map the mean red pixel value (within the Probe) to the frequency of the Theremin\n",
    "red2freq2 = Mapper(\n",
    "    mean_red2, \n",
    "    theremin2[\"frequency\"], \n",
    "    exponent=2, name=\"Red2Freq\") # cubic mapping curve for a more \"linear\" feel of frequency changes\n",
    "# attach the Mapper to the app\n",
    "app2.attach_mapper(red2freq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.graph.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "graph = sf.AudioGraph.get_shared_graph()\n",
    "print(graph is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "\n",
    "graph = None\n",
    "buf = sf.Buffer(1, 48000)\n",
    "\n",
    "class TestPatch(sf.Patch):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        param = self.add_input(\"param\")\n",
    "\n",
    "        out = param * sf.SineOscillator(440)\n",
    "        self.set_output(out)\n",
    "\n",
    "def create_audio_graph(nrt=False):\n",
    "    graph = sf.AudioGraph.get_shared_graph()\n",
    "    output_device = sf.AudioOut_Dummy(2) if nrt else None\n",
    "    if graph is not None:\n",
    "        graph.destroy()\n",
    "    graph = sf.AudioGraph(\n",
    "        start=True,\n",
    "        output_device=output_device)\n",
    "    my_patch = TestPatch()\n",
    "    my_patch.set_input(\"param\", 0.5)\n",
    "    graph.play(my_patch)\n",
    "    if nrt:\n",
    "        graph.render_to_buffer(buf)\n",
    "    graph.stop(my_patch)\n",
    "    return graph\n",
    "\n",
    "print(graph) # should be None\n",
    "\n",
    "graph = create_audio_graph(nrt=False)\n",
    "\n",
    "print(\"RT\", graph.status)\n",
    "\n",
    "graph = create_audio_graph(nrt=True)\n",
    "\n",
    "print(\"NRT\", graph.status)\n",
    "\n",
    "graph = create_audio_graph(nrt=False)\n",
    "\n",
    "print(\"RT2\", graph.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = App(image_size=(800, 800))\n",
    "app.load_image_file(\"images/cellular_dataset/merged_8bit/Timepoint_001_220518-ST_C03_s1.jpg\")\n",
    "\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "app.attach(mean_red)\n",
    "\n",
    "num_instances = 5\n",
    "\n",
    "for i in range(num_instances):\n",
    "    theremin = Theremin()\n",
    "    app.attach(theremin)\n",
    "\n",
    "    red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=f\"Red2Freq{i}\")\n",
    "    app.attach(red2freq)\n",
    "\n",
    "    red2amp = Mapper(mean_red, theremin[\"amplitude\"], exponent=1, name=f\"Red2Amp{i}\")\n",
    "    app.attach(red2amp)\n",
    "\n",
    "    red2pan = Mapper(mean_red, theremin[\"panning\"], exponent=1, name=f\"Red2Pan{i}\")\n",
    "    app.attach(red2pan)\n",
    "\n",
    "# app.interaction_mode = \"toggle\"\n",
    "# app.audio = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.output_buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.output_buffer_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osc = Oscillator()\n",
    "app.attach(osc)\n",
    "\n",
    "fnoise = FilteredNoise()\n",
    "app.attach(fnoise)\n",
    "\n",
    "fm = SimpleFM()\n",
    "app.attach(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(app.mappers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test audio settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.audio = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_volume = -6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.recording = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.recording_path = \"hey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_envelope.attack = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display_global = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.display_channel_offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.display_layer_offset = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test probe settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_width = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_height = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_x = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_y = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.interaction_mode = \"hold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_follows_idle_mouse = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "graph = sf.AudioGraph.get_shared_graph()\n",
    "if graph is not None:\n",
    "    graph.destroy()\n",
    "graph = sf.AudioGraph(output_device=sf.AudioOut_Dummy(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "graph = sf.AudioGraph()\n",
    "\n",
    "class Synth(sf.Patch):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class TestPatch(Synth):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        param = self.add_input(\"param\")\n",
    "\n",
    "        out = param * sf.SineOscillator(440)\n",
    "        self.set_output(out)\n",
    "\n",
    "patch = TestPatch()\n",
    "patch.set_input(\"param\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.set_input(\"param\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the type of the patch object\n",
    "print(type(patch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(patch, Synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(patch, sf.Patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "config = sf.AudioGraphConfig()\n",
    "config.output_buffer_size = 480\n",
    "graph = sf.AudioGraph(config)\n",
    "\n",
    "class TestPatch(sf.Patch):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        freq = self.add_input(\"freq\", 440)\n",
    "        out = sf.SineOscillator(freq)\n",
    "        self.set_output(out)\n",
    "\n",
    "patch = TestPatch()\n",
    "\n",
    "graph.play(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.destroy()\n",
    "config = sf.AudioGraphConfig()\n",
    "config.output_buffer_size = 1024\n",
    "graph = sf.AudioGraph(config)\n",
    "print(\"About to play\") # still prints\n",
    "#graph.play(patch) # will crash Kernel here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.set_input(\"freq\", 880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = patch.to_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spec.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch2 = sf.Patch(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.play(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.output_buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.stop(patch)\n",
    "graph.clear()\n",
    "graph.destroy()\n",
    "config = sf.AudioGraphConfig()\n",
    "config.output_buffer_size = 1024\n",
    "graph = sf.AudioGraph(config)\n",
    "print(\"About to play\") # still prints\n",
    "graph.play(patch2) # will crash Kernel here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = patch.to_spec()\n",
    "graph.destroy()\n",
    "config = sf.AudioGraphConfig()\n",
    "config.output_buffer_size = 1024\n",
    "graph = sf.AudioGraph(config)\n",
    "patch2 = sf.Patch(spec)\n",
    "graph.play(patch2) # this will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "from pixasonics.synths import Theremin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = sf.AudioGraphConfig()\n",
    "config.output_buffer_size = 480\n",
    "graph = sf.AudioGraph(config)\n",
    "\n",
    "theremin = Theremin()\n",
    "\n",
    "graph.play(theremin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theremin.set_input_buf(\"frequency\", 880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theremin_spec = theremin.to_spec()\n",
    "print(theremin_spec.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theremin.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.output_buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.destroy()\n",
    "config = sf.AudioGraphConfig()\n",
    "config.output_buffer_size = 1024\n",
    "graph = sf.AudioGraph(config)\n",
    "theremin2 = sf.Patch(theremin_spec)\n",
    "graph.play(theremin2) # this will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponent Canvas proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "from pixasonics.utils import scale_array_exp\n",
    "import numpy as np\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentCanvas():\n",
    "    def __init__(self, width=200, height=200, exponent=1):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._exponent = exponent\n",
    "        self.canvas = Canvas(width=width, height=height)\n",
    "        self.draw()\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.canvas\n",
    "    \n",
    "    @property\n",
    "    def exponent(self):\n",
    "        return self._exponent\n",
    "    \n",
    "    @exponent.setter\n",
    "    def exponent(self, value):\n",
    "        self._exponent = value\n",
    "        self.draw()\n",
    "\n",
    "    def draw(self):\n",
    "        with hold_canvas(self.canvas):\n",
    "            self.canvas.clear()\n",
    "            x = np.linspace(0, 1, self.width)\n",
    "            y = scale_array_exp(x, 0, 1, 0, 1, self._exponent)\n",
    "            y = 1 - y\n",
    "            y = y * self.height\n",
    "            self.canvas.fill_style = \"black\"\n",
    "            self.canvas.fill_rects(x * self.width, y, 1, self.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ExponentCanvas(600)\n",
    "display(c())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.exponent = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_slider = widgets.FloatLogSlider(\n",
    "    value=1,\n",
    "    base=10,\n",
    "    min=log10(0.01), # max exponent\n",
    "    max=log10(100), # min exponent\n",
    "    step=0.0001,\n",
    "    description='Exponent:',\n",
    "    continuous_update=True,\n",
    "    readout_format='.4f',\n",
    ")\n",
    "exp_slider.observe(lambda change: setattr(c, \"exponent\", change.new), names=\"value\")\n",
    "display(exp_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentPlot():\n",
    "    def __init__(self, width=1000, height=200, exponent=1):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._exponent = exponent\n",
    "        \n",
    "        self.create_ui()\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.card\n",
    "    \n",
    "    @property\n",
    "    def exponent(self):\n",
    "        return self._exponent\n",
    "    \n",
    "    def create_ui(self):\n",
    "        canvas = ExponentCanvas(self.width, self.height, self.exponent)\n",
    "        exp_slider = widgets.FloatLogSlider(\n",
    "            value=self.exponent,\n",
    "            base=10,\n",
    "            min=log10(0.01),\n",
    "            max=log10(100),\n",
    "            step=0.0001,\n",
    "            description='Exponent:',\n",
    "            continuous_update=True,\n",
    "            readout_format='.4f',\n",
    "        )\n",
    "        exp_slider.observe(lambda change: setattr(canvas, \"exponent\", change.new), names=\"value\")\n",
    "        self.card = widgets.VBox([canvas(), exp_slider])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ExponentPlot(1000, 200, 1)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test headless mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import signalflow as sf\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "from pixasonics.core import App, Mapper\n",
    "from pixasonics.features import *\n",
    "from pixasonics.synths import Theremin, Oscillator, FilteredNoise, SimpleFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = App(headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.load_image_file(\"images/test.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "app.attach(mean_red)\n",
    "app.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theremin = Theremin(name=\"MySine\")\n",
    "app.attach(theremin)\n",
    "app.synths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=\"Red2Freq\")\n",
    "app.attach(red2freq)\n",
    "app.mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_width, app.probe_height, app.probe_x, app.probe_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_x, app.probe_y = 200, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.audio = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.graph.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.unmuted = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_x, app.probe_y = 50, 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.unmuted = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little loop to unmute the probe, then move it around then turn it off\n",
    "import time\n",
    "app.unmuted = True\n",
    "app.probe_x, app.probe_y = 250, 0\n",
    "while app.probe_y < 350:\n",
    "    app.probe_y += 1\n",
    "    time.sleep(0.01)\n",
    "app.unmuted = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.load_image_file(\"images/cellular_dataset/merged_8bit/Timepoint_001_220518-ST_C03_s1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a for loop where for each image in the folder we load a headless app and render a timeline in nrt mode\n",
    "img_folder = \"images/cellular_dataset/merged_8bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "\n",
    "# example: horizontal scan\n",
    "duration = 5\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 1,\n",
    "        \"probe_height\": 500,\n",
    "        \"probe_x\": 0,\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_x\": 499\n",
    "    })\n",
    "]\n",
    "app = App(headless=True, nrt=True) # create a global graph object, necessary for the Theremin\n",
    "app.cleanup() # clean up the app to avoid hanging\n",
    "# only need to create processor objects once and attach them to the apps\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "theremin = Theremin(name=\"MySine\")\n",
    "red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=\"Red2Freq\")\n",
    "# loop over all images in the folder, create a headless app, load the image, attach the processors and render the timeline\n",
    "for img_file in img_files:\n",
    "    print(f\"Processing {img_file}\")\n",
    "    img_path = os.path.join(img_folder, img_file)\n",
    "    with App(headless=True, nrt=True) as app:\n",
    "        app.load_image_file(img_path)\n",
    "        app.attach(mean_red)\n",
    "        app.attach(theremin)\n",
    "        app.attach(red2freq)\n",
    "        target_filename = img_file.replace(\".jpg\", \".wav\")\n",
    "        app.render_timeline_to_file(my_timeline, target_filename)\n",
    "        print(f\"Saved {target_filename}\")\n",
    "    display(Audio(target_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing but render np.arrays instead\n",
    "img_folder = \"images/cellular_dataset/merged_8bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "\n",
    "# example: horizontal scan\n",
    "duration = 5\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 1,\n",
    "        \"probe_height\": 500,\n",
    "        \"probe_x\": 0,\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_x\": 499\n",
    "    })\n",
    "]\n",
    "app = App(headless=True, nrt=True) # create a global graph object, necessary for the Theremin\n",
    "app.cleanup()\n",
    "# only need to create processor objects once and attach them to the apps\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "theremin = Theremin(name=\"MySine\")\n",
    "red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=\"Red2Freq\")\n",
    "# loop over all images in the folder, create a headless app, load the image, attach the processors and render the timeline\n",
    "for img_file in img_files:\n",
    "    print(f\"Processing {img_file}\")\n",
    "    img_path = os.path.join(img_folder, img_file)\n",
    "    buf = None\n",
    "    with App(headless=True, nrt=True) as app:\n",
    "        app.load_image_file(img_path)\n",
    "        app.attach(mean_red)\n",
    "        app.attach(theremin)\n",
    "        app.attach(red2freq)\n",
    "        buf = app.render_timeline_to_array(my_timeline)\n",
    "        print(f\"Saved {target_filename}\")\n",
    "    display(Audio(buf, rate=app.sample_rate, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "nrt = True\n",
    "_output_buffer_size = 480\n",
    "_sample_rate = 48000\n",
    "# Get or create the shared audio graph\n",
    "graph = sf.AudioGraph.get_shared_graph()\n",
    "if graph is not None and nrt:\n",
    "    graph.destroy()\n",
    "    graph = None\n",
    "if graph is None:\n",
    "    output_device = sf.AudioOut_Dummy(2) if nrt else None\n",
    "    config = sf.AudioGraphConfig()\n",
    "    print(f\"Setting AudioGraphConfig output_buffer_size to {_output_buffer_size}\")\n",
    "    config.output_buffer_size = _output_buffer_size\n",
    "    print(f\"Setting AudioGraphConfig sample_rate to {_sample_rate}\")\n",
    "    config.sample_rate = _sample_rate\n",
    "    graph = sf.AudioGraph(config=config, start=True, output_device=output_device)\n",
    "print(f\"Graph sample_rate: {graph.sample_rate}\")\n",
    "print(f\"Graph output_buffer_size: {graph.output_buffer_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "nrt = True\n",
    "_output_buffer_size = 480\n",
    "_sample_rate = 48000\n",
    "# Get or create the shared audio graph\n",
    "graph = sf.AudioGraph.get_shared_graph()\n",
    "if graph is not None and nrt:\n",
    "    graph.destroy()\n",
    "    graph = None\n",
    "if graph is None:\n",
    "    print(f\"Setting AudioOut_Dummy buffer_size to {_output_buffer_size}\")\n",
    "    output_device = sf.AudioOut_Dummy(2, buffer_size=_output_buffer_size) if nrt else None\n",
    "    config = sf.AudioGraphConfig()\n",
    "    # print(f\"Setting output buffer size to {_output_buffer_size}\")\n",
    "    # config.output_buffer_size = _output_buffer_size\n",
    "    print(f\"Setting AudioGraphConfig sample_rate to {_sample_rate}\")\n",
    "    config.sample_rate = _sample_rate\n",
    "    graph = sf.AudioGraph(config=config, start=True, output_device=output_device)\n",
    "print(f\"Graph sample_rate: {graph.sample_rate}\")\n",
    "print(f\"Graph output_buffer_size: {graph.output_buffer_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test custom Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import signalflow as sf\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "from pixasonics.core import App, Mapper\n",
    "from pixasonics.features import Feature\n",
    "from pixasonics.synths import Theremin, Oscillator, FilteredNoise, SimpleFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.load_image_file(\"images/cellular_dataset/merged_8bit/Timepoint_001_220518-ST_C03_s1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine red and green channels and all layers\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs = []\n",
    "for img_red, img_green in zip(imgs_red, imgs_green):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img = np.stack([img_red, img_green], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFeature(Feature):\n",
    "    def __init__(self, name=\"MyFeature\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def process_image(self, mat):\n",
    "        return np.random.rand(*mat.shape)\n",
    "    \n",
    "    def compute(self, mat):\n",
    "        num_features = mat.shape[self.target_dim]\n",
    "        return np.random.rand(num_features)\n",
    "    \n",
    "my_feature = MyFeature()\n",
    "app.attach(my_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.detach(my_feature)\n",
    "my_feature = MyFeature()\n",
    "my_feature.target_dim = 0\n",
    "app.attach(my_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class KMeansFeature(Feature):\n",
    "    def __init__(self, n_clusters=3, name=\"KMeansFeature\"):\n",
    "        super().__init__(name=name)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.kmeans = None\n",
    "        self._original_shape = None\n",
    "\n",
    "    def _reshape_for_kmeans(self, mat):\n",
    "        \"\"\"Helper to reshape 4D matrix to 2D for KMeans\"\"\"\n",
    "        mat_reshaped = np.moveaxis(mat, self.target_dim, 0)\n",
    "        return mat_reshaped.reshape(mat_reshaped.shape[0], -1)\n",
    "\n",
    "    def process_image(self, mat):\n",
    "        self._original_shape = mat.shape\n",
    "        features = self._reshape_for_kmeans(mat)\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters).fit(features.T)\n",
    "\n",
    "        # Get cluster assignments and reshape back to original dimensions\n",
    "        labels = self.kmeans.predict(features.T)\n",
    "        other_dims = [s for i, s in enumerate(mat.shape) if i != self.target_dim]\n",
    "        self.transformed_image = np.expand_dims(\n",
    "            labels.reshape(*other_dims), \n",
    "            axis=self.target_dim\n",
    "        )\n",
    "        return self.transformed_image\n",
    "    \n",
    "    def compute(self, mat):\n",
    "        if self.kmeans is None:\n",
    "            raise ValueError(\"KMeans model has not been fitted. Call process_image first.\")\n",
    "        features = self._reshape_for_kmeans(mat)\n",
    "        labels = self.kmeans.predict(features.T)\n",
    "        # Compute histogram of cluster assignments\n",
    "        hist, _ = np.histogram(labels, bins=range(self.n_clusters + 1))\n",
    "        return hist.astype(float) / hist.sum() # normalize to sum to 1\n",
    "\n",
    "# Example usage\n",
    "kmeans_feature = KMeansFeature(n_clusters=10)\n",
    "app.attach(kmeans_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a multichannel Theremin that has its frequencies in a harmonic series\n",
    "fundamental_freq = 110\n",
    "num_harmonics = kmeans_feature.n_clusters\n",
    "freqs = fundamental_freq * np.arange(1, num_harmonics + 1)\n",
    "print(\"Frequencies:\",freqs)\n",
    "osc = Theremin(frequency=freqs, name=\"KMeansOsc\")\n",
    "app.attach(osc)\n",
    "\n",
    "# create a Mapper that will map the KMeans cluster histogram to the amplitude of the Theremin \n",
    "k2amp = Mapper(kmeans_feature, osc[\"amplitude\"], exponent=1, name=\"K2Amp\")\n",
    "app.attach(k2amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.attach(osc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a colormap with distinct colors for each cluster\n",
    "n_clusters = kmeans_feature.n_clusters\n",
    "colors = sns.color_palette(\"husl\", n_colors=n_clusters)\n",
    "colormap = {i: colors[i] for i in range(n_clusters)}\n",
    "\n",
    "# Get the cluster assignments from transformed_image\n",
    "cluster_image = kmeans_feature.transformed_image[:, :, 0, 0]\n",
    "print(cluster_image.shape)\n",
    "\n",
    "# Create RGB image where each cluster gets a unique color\n",
    "rgb_image = np.zeros((*cluster_image.shape, 3))\n",
    "for cluster_id, color in colormap.items():\n",
    "    mask = cluster_image == cluster_id\n",
    "    rgb_image[mask] = color\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(app.image_displayed)  # Show first layer of original image\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('K-means Clusters')\n",
    "plt.imshow(rgb_image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "synthmaps_pca_mel_json = \"/Volumes/T7RITMO/synthmaps_code/data/pca_mels_mean.json\"\n",
    "synthmaps_pca_mel_json = \"/Volumes/T7RITMO/synthmaps_code/data/pca_perceptual.json\"\n",
    "synthmaps_pca_mel_json = \"/Volumes/T7RITMO/synthmaps_code/data/pca_encodec.json\"\n",
    "synthmaps_pca_mel_json = \"/Volumes/T7RITMO/synthmaps_code/data/pca_clap.json\"\n",
    "with open(synthmaps_pca_mel_json, \"r\") as f:\n",
    "    pca_mel_data = json.load(f)\n",
    "print(pca_mel_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluid_dataset2array(\n",
    "        dataset: dict,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a json dataset to a numpy array.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): The json dataset to convert.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The numpy array.\n",
    "    \"\"\"\n",
    "    num_cols = dataset[\"cols\"]\n",
    "    num_rows = len(dataset[\"data\"])\n",
    "    out_array = np.zeros((num_rows, num_cols))\n",
    "    for i in range(num_rows):\n",
    "        out_array[i] = np.array(dataset[\"data\"][str(i)])\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mel_data_array = fluid_dataset2array(pca_mel_data)\n",
    "print(pca_mel_data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "synthmaps_scaler = MinMaxScaler()\n",
    "pca_mel_data_scaled = synthmaps_scaler.fit_transform(pca_mel_data_array)\n",
    "print(pca_mel_data_scaled.shape)\n",
    "print(pca_mel_data_scaled.min(), pca_mel_data_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_params_json = \"/Volumes/T7RITMO/synthmaps_code/data/fm_params.json\"\n",
    "with open(fm_params_json, \"r\") as f:\n",
    "    fm_params_data = json.load(f)\n",
    "print(fm_params_data.keys())\n",
    "fm_params_data_array = fluid_dataset2array(fm_params_data)\n",
    "print(fm_params_data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "class PCA2FMTimbreSpace(Feature):\n",
    "    def __init__(self, name=\"PCA2FMTimbreSpace\"):\n",
    "        super().__init__(name=name)\n",
    "        self.pca = None\n",
    "        self.pca_scaler = None\n",
    "        self.kdtree = KDTree(pca_mel_data_scaled)\n",
    "        self._original_shape = None\n",
    "        self._transformed_points = None\n",
    "\n",
    "    def _reshape_for_pca(self, mat):\n",
    "        \"\"\"Helper to reshape 4D matrix (H, W, Ch, L) to 2D by concatenating the Channel and Layer dimensions\"\"\"\n",
    "        mat_reshaped = mat.reshape(mat.shape[0], mat.shape[1], -1)\n",
    "        return mat_reshaped.reshape(-1, mat_reshaped.shape[-1])\n",
    "\n",
    "    def process_image(self, mat):\n",
    "        self._original_shape = mat.shape\n",
    "        features = self._reshape_for_pca(mat)\n",
    "        print(features.shape)\n",
    "        self.pca = IncrementalPCA(n_components=2)\n",
    "        self.pca.fit(features)\n",
    "        self.pca_scaler = MinMaxScaler(feature_range=(0.1, 0.9))\n",
    "        self._transformed_points = self.pca.transform(features)\n",
    "        self.pca_scaler.fit(self._transformed_points)\n",
    "        return mat\n",
    "    \n",
    "    def compute(self, mat):\n",
    "        if self.pca is None:\n",
    "            raise ValueError(\"PCA model has not been fitted. Call process_image first.\")\n",
    "        features = self._reshape_for_pca(mat)\n",
    "        projected = self.pca.transform(features)\n",
    "        projected_scaled = self.pca_scaler.transform(projected)\n",
    "        projected_scaled_mean = projected_scaled.mean(axis=0, keepdims=True)\n",
    "        nearest_idx = self.kdtree.query(projected_scaled_mean, return_distance=False)[0][0]\n",
    "        fm_params = fm_params_data_array[nearest_idx]\n",
    "        return fm_params\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pca2FM_feature = PCA2FMTimbreSpace()\n",
    "app.attach(pca2FM_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plot the pca space\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "points = pca2FM_feature._transformed_points\n",
    "plt.scatter(points[:, 0], points[:, 1], alpha=0.5)\n",
    "plt.title('PCA Space')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "class TSNE2FMTimbreSpace(Feature):\n",
    "    def __init__(self, name=\"TSNE2FMTimbreSpace\"):\n",
    "        super().__init__(name=name)\n",
    "        self.tsne = None\n",
    "        self.tsne_scaler = None\n",
    "        self.kdtree = KDTree(pca_mel_data_scaled)\n",
    "        self._original_shape = None\n",
    "        self._transformed_points = None\n",
    "\n",
    "    def _reshape_for_tsne(self, mat):\n",
    "        \"\"\"Helper to reshape 4D matrix (H, W, Ch, L) to 2D by concatenating the Channel and Layer dimensions\"\"\"\n",
    "        mat_reshaped = mat.reshape(mat.shape[0], mat.shape[1], -1)\n",
    "        return mat_reshaped.reshape(-1, mat_reshaped.shape[-1])\n",
    "\n",
    "    def process_image(self, mat):\n",
    "        self._original_shape = mat.shape\n",
    "        features = self._reshape_for_tsne(mat)\n",
    "        print(features.shape)\n",
    "        self.tsne = TSNE(n_components=2)\n",
    "        self._transformed_points = self.tsne.fit_transform(features)\n",
    "        self.tsne_scaler = MinMaxScaler(feature_range=(0.1, 0.9))\n",
    "        self.tsne_scaler.fit(self._transformed_points)\n",
    "        return mat\n",
    "    \n",
    "    def compute(self, mat):\n",
    "        if self.tsne is None:\n",
    "            raise ValueError(\"TSNE model has not been fitted. Call process_image first.\")\n",
    "        features = self._reshape_for_tsne(mat)\n",
    "        projected = self.tsne.transform(features)\n",
    "        projected_scaled = self.tsne_scaler.transform(projected)\n",
    "        projected_scaled_mean = projected_scaled.mean(axis=0, keepdims=True)\n",
    "        nearest_idx = self.kdtree.query(projected_scaled_mean, return_distance=False)[0][0]\n",
    "        fm_params = fm_params_data_array[nearest_idx]\n",
    "        return fm_params\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tsne2FM_feature = TSNE2FMTimbreSpace()\n",
    "app.attach(tsne2FM_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plot the tsne space\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "points = tsne2FM_feature._transformed_points\n",
    "plt.scatter(points[:, 0], points[:, 1], alpha=0.5)\n",
    "plt.title('TSNE Space')\n",
    "plt.xlabel('TSNE 1')\n",
    "plt.ylabel('TSNE 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = SimpleFM()\n",
    "app.attach(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMParamSetter(Mapper):\n",
    "    def __init__(self, feature, synth, name=\"FMParamSetter\"):\n",
    "        super().__init__(feature, synth, name=name)\n",
    "\n",
    "    def map(self, frame=None):\n",
    "        fm_params = self.buf_in.data\n",
    "        if fm_params.shape[0] == 3:\n",
    "            self.obj_out_owner.set_input_buf(\"carrier_freq\", fm_params[0], from_slider=False)\n",
    "            self.obj_out_owner.set_input_buf(\"harm_ratio\", fm_params[1], from_slider=False)\n",
    "            self.obj_out_owner.set_input_buf(\"mod_index\", fm_params[2], from_slider=False)\n",
    "\n",
    "fm_param_setter = FMParamSetter(pca2FM_feature, fm[\"carrier_freq\"])\n",
    "app.attach(fm_param_setter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm2 = SimpleFM(name=\"FM2\")\n",
    "app.attach(fm2)\n",
    "fm2_param_setter = FMParamSetter(tsne2FM_feature, fm2[\"carrier_freq\"])\n",
    "app.attach(fm2_param_setter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_height, app.probe_width = 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.detach(pca2FM_feature)\n",
    "app.detach(fm_param_setter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.detach(pca2FM_feature)\n",
    "app.detach(fm_param_setter)\n",
    "pca2FM_feature = PCA2FMTimbreSpace()\n",
    "pca2FM_feature.filter_channels = None # try also 0\n",
    "app.attach(pca2FM_feature)\n",
    "fm_param_setter = FMParamSetter(pca2FM_feature, fm[\"carrier_freq\"])\n",
    "app.attach(fm_param_setter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixasonics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
